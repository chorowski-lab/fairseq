{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-QZDVJVnfs1"
   },
   "source": [
    "### This notebook is optionally accelerated with a GPU runtime.\n",
    "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# vgg-nets\n",
    "\n",
    "*Author: Pytorch Team*\n",
    "\n",
    "**Award winning ConvNets from 2014 Imagenet ILSVRC challenge**\n",
    "\n",
    "<img src=\"https://pytorch.org/assets/images/vgg.png\" alt=\"alt\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAq2VY_knfs7"
   },
   "source": [
    "All pre-trained models expect input images normalized in the same way,\n",
    "i.e. mini-batches of 3-channel RGB images of shape `(3 x H x W)`, where `H` and `W` are expected to be at least `224`.\n",
    "The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\n",
    "and `std = [0.229, 0.224, 0.225]`.\n",
    "\n",
    "Here's a sample execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APUCl9km1Tah"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "SPGMNCmpLxgQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GFxPrB8tE7kq"
   },
   "outputs": [],
   "source": [
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=False)\n",
    "# or any of these variants\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11_bn', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg13', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg13_bn', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16_bn', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19_bn', pretrained=True)\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3SvcSw131eON"
   },
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv,\n",
    "        output_C=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = conv\n",
    "        self.output_C = output_C\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        if self.output_C is None:\n",
    "            self.output_C = C\n",
    "        x = x.reshape(B, -1, self.output_C)\n",
    "        x = x.prod(-1).sum(-1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lpLifQs091Yv"
   },
   "outputs": [],
   "source": [
    "class Smartpool(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        factor,\n",
    "        search_perc,\n",
    "        mlp2=False\n",
    "    ):\n",
    "        \"\"\"Smart pooling algorithm\n",
    "\n",
    "        Args:\n",
    "            factor: factor by which the sequence's length will be reduced\n",
    "            search_perc: percentage of length of sequence after smartpooling to search for border. Ideally the border is located somewhere in +-search_perc\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.search_perc = search_perc\n",
    "        self.factor = factor\n",
    "        self.register_buffer(\"filters\", torch.FloatTensor([[[[-1,1],[1,-1]]]]), persistent=False)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256,512),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256,1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        if mlp2 == True:\n",
    "            self.mlp2 = nn.Sequential(\n",
    "                nn.Linear(2, 256),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(256,512),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(512,256),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(256,1))\n",
    "        else:\n",
    "            self.mlp2 = None\n",
    "\n",
    "    def warp(self, X, new_lens):\n",
    "        new_lens_cs = new_lens.cumsum(1)\n",
    "        # This really searches for the low boundary of each new pixel\n",
    "        pixel_contributions = new_lens_cs.view(1, -1, 1) - torch.arange(torch.round(new_lens_cs[0, -1]).item(), device=X.device).view(1, 1, -1)\n",
    "        pixel_contributions = pixel_contributions.view(X.size(0), X.size(1), pixel_contributions.size(2))\n",
    "        # Zero out the negative contributions, i.e. pixels which come before each row                              \n",
    "        pixel_contributions = torch.max(torch.tensor(0.0, device=X.device), pixel_contributions)       \n",
    "        \n",
    "        # # This contains the cumulated pixel lengths for all pixels in each \n",
    "        # pixel_contributions\n",
    "    \n",
    "        pixel_contributions = pixel_contributions.unsqueeze(1)\n",
    "        interp_weights = F.conv2d(pixel_contributions, self.filters, padding=1)\n",
    "        interp_weights = interp_weights[:,:,:-1,1:] # Removing padding\n",
    "        interp_weights = interp_weights.squeeze(1)\n",
    "\n",
    "        # # Each column corresponds to a new element. Its values are the \n",
    "        # # weights associated with the original data.\n",
    "        # interp_weights\n",
    "\n",
    "        interp_weights = interp_weights.transpose(1, 2)\n",
    "        Xnew = interp_weights @ X\n",
    "        return Xnew, interp_weights\n",
    "\n",
    "    def nonzero_interval_length(self, x, dim):\n",
    "        nonz = (x > 0)\n",
    "        _, low = ((nonz.cumsum(dim) == 1) & nonz).max(dim, keepdim=True)\n",
    "        rev_cumsum = nonz.long().flip(dim).cumsum(dim).flip(dim)\n",
    "        _, high = ((rev_cumsum == 1) & nonz).max(dim, keepdim=True)\n",
    "        \n",
    "        return high - low + 1\n",
    "\n",
    "    def forward(self, features):\n",
    "        B,T,C = features.size()\n",
    "\n",
    "        padding_mask = torch.zeros(B,T, dtype=torch.bool, device=features.device)\n",
    "        padding_per_batch = (padding_mask > 0).sum(1)\n",
    "        total_T = padding_mask.numel() - padding_per_batch.sum()\n",
    "\n",
    "        # MLP test\n",
    "        new_lens = self.mlp(features.view(B*T,C)).view(1,-1)\n",
    "        new_lens = new_lens / new_lens.sum(1, keepdim=True) * (total_T / self.factor) # Reducing the original length T by some factor\n",
    "       \n",
    "        features, interp_weights = self.warp(features, new_lens)\n",
    "        \n",
    "        if self.mlp2 is not None:\n",
    "            features = self.mlp2(features)\n",
    "\n",
    "        return features\n",
    "        \n",
    "\n",
    "class DoXTimes(nn.Module):\n",
    "    def __init__(self, model):\n",
    "         super().__init__()\n",
    "         self.model = model\n",
    "\n",
    "    def forward(self, x): \n",
    "        return torch.cat([self.model(x[i].unsqueeze(0)) for i in range(x.shape[0])]).prod(-1).sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybPAVStr1HuV"
   },
   "source": [
    "# Task code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SSUEYv2oBYZG"
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size, seq_len, divider):\n",
    "    assert seq_len % divider == 0\n",
    "    data = torch.empty(batch_size, seq_len, 2)\n",
    "    data[:,:,0] = torch.zeros_like(data[:,:,0])\n",
    "    batch_id = torch.arange(batch_size).view(-1,1)\n",
    "    data[batch_id, torch.multinomial(torch.ones_like(data[:,:,0]), seq_len//divider), 0] = 1\n",
    "    data[:,:,1].uniform_(0,1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QEN8XMlE9CDm"
   },
   "outputs": [],
   "source": [
    "def train(model, epoch, optimizer, scheduler, dataset_len, batch_size, seq_len, divider):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, i in enumerate(range(0, dataset_len, batch_size)):\n",
    "        data = get_batch(batch_size, seq_len, divider).to(device)\n",
    "        targets = data.prod(-1).sum(-1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = (1 - output / targets).abs().mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            \"\"\"\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, dataset_len // batch_size, scheduler.get_last_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            \"\"\"\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} |'.format(\n",
    "                    epoch, batch, dataset_len // batch_size, scheduler.get_last_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AEyFhNHW9EJA"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataset_len, batch_size, seq_len, divider):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, dataset_len, batch_size):\n",
    "            data = get_batch(batch_size, seq_len, divider).to(device)\n",
    "            targets = data.prod(-1).sum(-1)\n",
    "            output = model(data)\n",
    "            total_loss += (1 - output / targets).abs().mean().item()\n",
    "    return total_loss / (dataset_len / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "-TTTvnM0D9Ru"
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model, epoch, optimizer, scheduler, dataset_len, batch_size, seq_len, divider)\n",
    "        val_loss = evaluate(model, dataset_len, eval_batch_size, seq_len, divider)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                        val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    test_loss = evaluate(best_model, dataset_len, eval_batch_size, seq_len, divider)\n",
    "    print('=' * 89)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "        test_loss, math.exp(test_loss)))\n",
    "    print('=' * 89)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zk_zQPnYFqX"
   },
   "source": [
    "# To 2 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtQivzJ5xy5d"
   },
   "source": [
    "## Pooling T/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxS_DruSDYKz"
   },
   "source": [
    "### Average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tdL9jwtc8cC",
    "outputId": "0f2cee26-3644-4ca2-e2df-9f5089e0ce68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/archive/v0.6.0.zip\" to /home/i273233/.cache/torch/hub/v0.6.0.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): AvgPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1))\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): AvgPool2d(kernel_size=2, stride=(2, 1), padding=0)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (linear): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=False)\n",
    "model = model.features[0:9]\n",
    "model[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model[2] = nn.AvgPool2d(kernel_size=2, stride=(2,1), padding=(0,1))\n",
    "model[5] = nn.AvgPool2d(kernel_size=2, stride=(2,1), padding=0)\n",
    "model[8] = nn.Conv2d(256, 256, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model.add_module(\"linear\", nn.Conv2d(256, 1, kernel_size=1, stride=1, padding=0))\n",
    "model = Conv(model)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUhhG0Tr3VJs",
    "outputId": "51cf775a-61a3-47ad-a4ad-1cc292c11e87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 5000 batches | lr 1.00 | ms/batch 37.01 | loss  0.95 |\n",
      "| epoch   1 |   400/ 5000 batches | lr 1.00 | ms/batch 34.65 | loss  0.93 |\n",
      "| epoch   1 |   600/ 5000 batches | lr 1.00 | ms/batch 34.81 | loss  0.93 |\n",
      "| epoch   1 |   800/ 5000 batches | lr 1.00 | ms/batch 34.85 | loss  0.93 |\n",
      "| epoch   1 |  1000/ 5000 batches | lr 1.00 | ms/batch 34.90 | loss  0.93 |\n",
      "| epoch   1 |  1200/ 5000 batches | lr 1.00 | ms/batch 34.92 | loss  0.93 |\n",
      "| epoch   1 |  1400/ 5000 batches | lr 1.00 | ms/batch 34.96 | loss  0.93 |\n",
      "| epoch   1 |  1600/ 5000 batches | lr 1.00 | ms/batch 35.12 | loss  0.93 |\n",
      "| epoch   1 |  1800/ 5000 batches | lr 1.00 | ms/batch 35.17 | loss  0.93 |\n",
      "| epoch   1 |  2000/ 5000 batches | lr 1.00 | ms/batch 35.18 | loss  0.93 |\n",
      "| epoch   1 |  2200/ 5000 batches | lr 1.00 | ms/batch 35.25 | loss  0.93 |\n",
      "| epoch   1 |  2400/ 5000 batches | lr 1.00 | ms/batch 35.25 | loss  0.93 |\n",
      "| epoch   1 |  2600/ 5000 batches | lr 1.00 | ms/batch 35.28 | loss  0.93 |\n",
      "| epoch   1 |  2800/ 5000 batches | lr 1.00 | ms/batch 35.23 | loss  0.93 |\n",
      "| epoch   1 |  3000/ 5000 batches | lr 1.00 | ms/batch 35.27 | loss  0.92 |\n",
      "| epoch   1 |  3200/ 5000 batches | lr 1.00 | ms/batch 35.34 | loss  0.93 |\n",
      "| epoch   1 |  3400/ 5000 batches | lr 1.00 | ms/batch 35.33 | loss  0.93 |\n",
      "| epoch   1 |  3600/ 5000 batches | lr 1.00 | ms/batch 35.31 | loss  0.93 |\n",
      "| epoch   1 |  3800/ 5000 batches | lr 1.00 | ms/batch 35.34 | loss  0.93 |\n",
      "| epoch   1 |  4000/ 5000 batches | lr 1.00 | ms/batch 35.35 | loss  0.93 |\n",
      "| epoch   1 |  4200/ 5000 batches | lr 1.00 | ms/batch 35.36 | loss  0.93 |\n",
      "| epoch   1 |  4400/ 5000 batches | lr 1.00 | ms/batch 35.38 | loss  0.93 |\n",
      "| epoch   1 |  4600/ 5000 batches | lr 1.00 | ms/batch 35.35 | loss  0.93 |\n",
      "| epoch   1 |  4800/ 5000 batches | lr 1.00 | ms/batch 35.47 | loss  0.93 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 254.30s | valid loss  1.77 | valid ppl     5.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5000 batches | lr 0.95 | ms/batch 36.88 | loss  0.57 |\n",
      "| epoch   2 |   400/ 5000 batches | lr 0.95 | ms/batch 36.91 | loss  0.56 |\n",
      "| epoch   2 |   600/ 5000 batches | lr 0.95 | ms/batch 36.92 | loss  0.56 |\n",
      "| epoch   2 |   800/ 5000 batches | lr 0.95 | ms/batch 36.55 | loss  0.56 |\n",
      "| epoch   2 |  1000/ 5000 batches | lr 0.95 | ms/batch 36.62 | loss  0.56 |\n",
      "| epoch   2 |  1200/ 5000 batches | lr 0.95 | ms/batch 36.95 | loss  0.56 |\n",
      "| epoch   2 |  1400/ 5000 batches | lr 0.95 | ms/batch 36.94 | loss  0.56 |\n",
      "| epoch   2 |  1600/ 5000 batches | lr 0.95 | ms/batch 36.95 | loss  0.56 |\n",
      "| epoch   2 |  1800/ 5000 batches | lr 0.95 | ms/batch 36.94 | loss  0.56 |\n",
      "| epoch   2 |  2000/ 5000 batches | lr 0.95 | ms/batch 37.40 | loss  0.56 |\n",
      "| epoch   2 |  2200/ 5000 batches | lr 0.95 | ms/batch 37.31 | loss  0.56 |\n",
      "| epoch   2 |  2400/ 5000 batches | lr 0.95 | ms/batch 37.31 | loss  0.56 |\n",
      "| epoch   2 |  2600/ 5000 batches | lr 0.95 | ms/batch 37.28 | loss  0.56 |\n",
      "| epoch   2 |  2800/ 5000 batches | lr 0.95 | ms/batch 37.29 | loss  0.56 |\n",
      "| epoch   2 |  3000/ 5000 batches | lr 0.95 | ms/batch 37.30 | loss  0.56 |\n",
      "| epoch   2 |  3200/ 5000 batches | lr 0.95 | ms/batch 37.29 | loss  0.56 |\n",
      "| epoch   2 |  3400/ 5000 batches | lr 0.95 | ms/batch 37.28 | loss  0.56 |\n",
      "| epoch   2 |  3600/ 5000 batches | lr 0.95 | ms/batch 37.31 | loss  0.56 |\n",
      "| epoch   2 |  3800/ 5000 batches | lr 0.95 | ms/batch 37.75 | loss  0.56 |\n",
      "| epoch   2 |  4000/ 5000 batches | lr 0.95 | ms/batch 37.71 | loss  0.56 |\n",
      "| epoch   2 |  4200/ 5000 batches | lr 0.95 | ms/batch 37.72 | loss  0.56 |\n",
      "| epoch   2 |  4400/ 5000 batches | lr 0.95 | ms/batch 37.73 | loss  0.56 |\n",
      "| epoch   2 |  4600/ 5000 batches | lr 0.95 | ms/batch 37.69 | loss  0.56 |\n",
      "| epoch   2 |  4800/ 5000 batches | lr 0.95 | ms/batch 37.67 | loss  0.56 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 267.48s | valid loss  0.75 | valid ppl     2.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5000 batches | lr 0.90 | ms/batch 37.92 | loss  0.53 |\n",
      "| epoch   3 |   400/ 5000 batches | lr 0.90 | ms/batch 37.17 | loss  0.52 |\n",
      "| epoch   3 |   600/ 5000 batches | lr 0.90 | ms/batch 37.05 | loss  0.52 |\n",
      "| epoch   3 |   800/ 5000 batches | lr 0.90 | ms/batch 37.06 | loss  0.52 |\n",
      "| epoch   3 |  1000/ 5000 batches | lr 0.90 | ms/batch 37.06 | loss  0.52 |\n",
      "| epoch   3 |  1200/ 5000 batches | lr 0.90 | ms/batch 37.52 | loss  0.52 |\n",
      "| epoch   3 |  1400/ 5000 batches | lr 0.90 | ms/batch 38.02 | loss  0.52 |\n",
      "| epoch   3 |  1600/ 5000 batches | lr 0.90 | ms/batch 37.03 | loss  0.52 |\n",
      "| epoch   3 |  1800/ 5000 batches | lr 0.90 | ms/batch 37.78 | loss  0.52 |\n",
      "| epoch   3 |  2000/ 5000 batches | lr 0.90 | ms/batch 37.83 | loss  0.52 |\n",
      "| epoch   3 |  2200/ 5000 batches | lr 0.90 | ms/batch 38.30 | loss  0.52 |\n",
      "| epoch   3 |  2400/ 5000 batches | lr 0.90 | ms/batch 38.42 | loss  0.52 |\n",
      "| epoch   3 |  2600/ 5000 batches | lr 0.90 | ms/batch 38.47 | loss  0.52 |\n",
      "| epoch   3 |  2800/ 5000 batches | lr 0.90 | ms/batch 38.43 | loss  0.52 |\n",
      "| epoch   3 |  3000/ 5000 batches | lr 0.90 | ms/batch 38.48 | loss  0.52 |\n",
      "| epoch   3 |  3200/ 5000 batches | lr 0.90 | ms/batch 38.40 | loss  0.52 |\n",
      "| epoch   3 |  3400/ 5000 batches | lr 0.90 | ms/batch 38.48 | loss  0.52 |\n",
      "| epoch   3 |  3600/ 5000 batches | lr 0.90 | ms/batch 38.49 | loss  0.52 |\n",
      "| epoch   3 |  3800/ 5000 batches | lr 0.90 | ms/batch 38.45 | loss  0.52 |\n",
      "| epoch   3 |  4000/ 5000 batches | lr 0.90 | ms/batch 38.48 | loss  0.52 |\n",
      "| epoch   3 |  4200/ 5000 batches | lr 0.90 | ms/batch 38.45 | loss  0.52 |\n",
      "| epoch   3 |  4400/ 5000 batches | lr 0.90 | ms/batch 38.48 | loss  0.52 |\n",
      "| epoch   3 |  4600/ 5000 batches | lr 0.90 | ms/batch 38.35 | loss  0.52 |\n",
      "| epoch   3 |  4800/ 5000 batches | lr 0.90 | ms/batch 37.94 | loss  0.52 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 273.10s | valid loss  0.75 | valid ppl     2.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5000 batches | lr 0.86 | ms/batch 38.24 | loss  0.49 |\n",
      "| epoch   4 |   400/ 5000 batches | lr 0.86 | ms/batch 38.04 | loss  0.49 |\n",
      "| epoch   4 |   600/ 5000 batches | lr 0.86 | ms/batch 38.07 | loss  0.49 |\n",
      "| epoch   4 |   800/ 5000 batches | lr 0.86 | ms/batch 38.05 | loss  0.49 |\n",
      "| epoch   4 |  1000/ 5000 batches | lr 0.86 | ms/batch 38.05 | loss  0.49 |\n",
      "| epoch   4 |  1200/ 5000 batches | lr 0.86 | ms/batch 38.05 | loss  0.49 |\n",
      "| epoch   4 |  1400/ 5000 batches | lr 0.86 | ms/batch 38.07 | loss  0.49 |\n",
      "| epoch   4 |  1600/ 5000 batches | lr 0.86 | ms/batch 38.05 | loss  0.49 |\n",
      "| epoch   4 |  1800/ 5000 batches | lr 0.86 | ms/batch 38.02 | loss  0.49 |\n",
      "| epoch   4 |  2000/ 5000 batches | lr 0.86 | ms/batch 38.03 | loss  0.49 |\n",
      "| epoch   4 |  2200/ 5000 batches | lr 0.86 | ms/batch 38.03 | loss  0.49 |\n",
      "| epoch   4 |  2400/ 5000 batches | lr 0.86 | ms/batch 38.03 | loss  0.49 |\n",
      "| epoch   4 |  2600/ 5000 batches | lr 0.86 | ms/batch 38.01 | loss  0.49 |\n",
      "| epoch   4 |  2800/ 5000 batches | lr 0.86 | ms/batch 38.04 | loss  0.49 |\n",
      "| epoch   4 |  3000/ 5000 batches | lr 0.86 | ms/batch 38.02 | loss  0.49 |\n",
      "| epoch   4 |  3200/ 5000 batches | lr 0.86 | ms/batch 38.01 | loss  0.49 |\n",
      "| epoch   4 |  3400/ 5000 batches | lr 0.86 | ms/batch 38.01 | loss  0.49 |\n",
      "| epoch   4 |  3600/ 5000 batches | lr 0.86 | ms/batch 38.02 | loss  0.49 |\n",
      "| epoch   4 |  3800/ 5000 batches | lr 0.86 | ms/batch 38.02 | loss  0.49 |\n",
      "| epoch   4 |  4000/ 5000 batches | lr 0.86 | ms/batch 38.02 | loss  0.49 |\n",
      "| epoch   4 |  4200/ 5000 batches | lr 0.86 | ms/batch 38.03 | loss  0.49 |\n",
      "| epoch   4 |  4400/ 5000 batches | lr 0.86 | ms/batch 38.05 | loss  0.49 |\n",
      "| epoch   4 |  4600/ 5000 batches | lr 0.86 | ms/batch 38.10 | loss  0.49 |\n",
      "| epoch   4 |  4800/ 5000 batches | lr 0.86 | ms/batch 38.06 | loss  0.49 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 273.44s | valid loss  0.75 | valid ppl     2.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5000 batches | lr 0.81 | ms/batch 38.24 | loss  0.46 |\n",
      "| epoch   5 |   400/ 5000 batches | lr 0.81 | ms/batch 38.08 | loss  0.45 |\n",
      "| epoch   5 |   600/ 5000 batches | lr 0.81 | ms/batch 38.08 | loss  0.45 |\n",
      "| epoch   5 |   800/ 5000 batches | lr 0.81 | ms/batch 38.08 | loss  0.45 |\n",
      "| epoch   5 |  1000/ 5000 batches | lr 0.81 | ms/batch 38.09 | loss  0.45 |\n",
      "| epoch   5 |  1200/ 5000 batches | lr 0.81 | ms/batch 38.11 | loss  0.45 |\n",
      "| epoch   5 |  1400/ 5000 batches | lr 0.81 | ms/batch 38.10 | loss  0.45 |\n",
      "| epoch   5 |  1600/ 5000 batches | lr 0.81 | ms/batch 38.10 | loss  0.45 |\n",
      "| epoch   5 |  1800/ 5000 batches | lr 0.81 | ms/batch 38.08 | loss  0.45 |\n",
      "| epoch   5 |  2000/ 5000 batches | lr 0.81 | ms/batch 38.08 | loss  0.45 |\n",
      "| epoch   5 |  2200/ 5000 batches | lr 0.81 | ms/batch 38.08 | loss  0.45 |\n",
      "| epoch   5 |  2400/ 5000 batches | lr 0.81 | ms/batch 38.05 | loss  0.45 |\n",
      "| epoch   5 |  2600/ 5000 batches | lr 0.81 | ms/batch 38.07 | loss  0.45 |\n",
      "| epoch   5 |  2800/ 5000 batches | lr 0.81 | ms/batch 38.06 | loss  0.45 |\n",
      "| epoch   5 |  3000/ 5000 batches | lr 0.81 | ms/batch 38.10 | loss  0.45 |\n",
      "| epoch   5 |  3200/ 5000 batches | lr 0.81 | ms/batch 38.10 | loss  0.45 |\n",
      "| epoch   5 |  3400/ 5000 batches | lr 0.81 | ms/batch 38.10 | loss  0.46 |\n",
      "| epoch   5 |  3600/ 5000 batches | lr 0.81 | ms/batch 38.10 | loss  0.45 |\n",
      "| epoch   5 |  3800/ 5000 batches | lr 0.81 | ms/batch 38.08 | loss  0.45 |\n",
      "| epoch   5 |  4000/ 5000 batches | lr 0.81 | ms/batch 38.09 | loss  0.45 |\n",
      "| epoch   5 |  4200/ 5000 batches | lr 0.81 | ms/batch 38.08 | loss  0.45 |\n",
      "| epoch   5 |  4400/ 5000 batches | lr 0.81 | ms/batch 38.09 | loss  0.45 |\n",
      "| epoch   5 |  4600/ 5000 batches | lr 0.81 | ms/batch 38.08 | loss  0.45 |\n",
      "| epoch   5 |  4800/ 5000 batches | lr 0.81 | ms/batch 38.06 | loss  0.45 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 273.66s | valid loss  0.75 | valid ppl     2.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.75 | test ppl     2.12\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "seq_len = 1000\n",
    "divider = 4\n",
    "dataset_len = 100000\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "epochs = 5 # The number of epochs\n",
    "\n",
    "model = train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wj4jJx_R9VpV",
    "outputId": "c96d6bc6-2433-463a-c4ae-2c0278091dee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: tensor([[0.0000e+00, 1.9316e-01],\n",
      "        [0.0000e+00, 1.2288e-01],\n",
      "        [1.0000e+00, 8.2184e-01],\n",
      "        [0.0000e+00, 2.9088e-01],\n",
      "        [0.0000e+00, 8.8348e-01],\n",
      "        [0.0000e+00, 4.2186e-01],\n",
      "        [1.0000e+00, 9.7892e-01],\n",
      "        [0.0000e+00, 3.2873e-01],\n",
      "        [0.0000e+00, 9.8628e-04],\n",
      "        [0.0000e+00, 3.1622e-01],\n",
      "        [0.0000e+00, 2.9373e-01],\n",
      "        [0.0000e+00, 7.7972e-01],\n",
      "        [0.0000e+00, 4.7035e-01],\n",
      "        [0.0000e+00, 1.2425e-01],\n",
      "        [0.0000e+00, 3.3145e-01],\n",
      "        [0.0000e+00, 5.1976e-02],\n",
      "        [0.0000e+00, 5.1003e-01],\n",
      "        [0.0000e+00, 1.6871e-01],\n",
      "        [1.0000e+00, 7.8025e-01],\n",
      "        [0.0000e+00, 9.3069e-01],\n",
      "        [1.0000e+00, 8.0811e-01],\n",
      "        [0.0000e+00, 3.9422e-01],\n",
      "        [1.0000e+00, 9.8349e-01],\n",
      "        [1.0000e+00, 7.3919e-01],\n",
      "        [0.0000e+00, 3.6395e-01],\n",
      "        [1.0000e+00, 1.2754e-01],\n",
      "        [0.0000e+00, 7.2988e-01],\n",
      "        [0.0000e+00, 9.3250e-01],\n",
      "        [0.0000e+00, 9.5595e-02],\n",
      "        [0.0000e+00, 1.9342e-01],\n",
      "        [0.0000e+00, 4.4071e-01],\n",
      "        [0.0000e+00, 8.9550e-01],\n",
      "        [0.0000e+00, 2.9614e-01],\n",
      "        [0.0000e+00, 7.6576e-01],\n",
      "        [1.0000e+00, 1.7572e-01],\n",
      "        [0.0000e+00, 8.1042e-01],\n",
      "        [0.0000e+00, 8.7943e-01],\n",
      "        [0.0000e+00, 7.7275e-01],\n",
      "        [0.0000e+00, 9.4483e-01],\n",
      "        [0.0000e+00, 6.4526e-01],\n",
      "        [0.0000e+00, 7.1774e-02],\n",
      "        [0.0000e+00, 4.8297e-02],\n",
      "        [1.0000e+00, 3.1190e-01],\n",
      "        [1.0000e+00, 9.0207e-01],\n",
      "        [1.0000e+00, 2.5033e-01],\n",
      "        [0.0000e+00, 5.9913e-01],\n",
      "        [0.0000e+00, 5.2550e-01],\n",
      "        [0.0000e+00, 9.3465e-01],\n",
      "        [0.0000e+00, 9.0823e-01],\n",
      "        [1.0000e+00, 1.9213e-01]], device='cuda:0')\n",
      "output: tensor([31.0412, 31.0412, 31.0412, 31.0412, 31.0412, 31.0412, 31.0412, 31.0412,\n",
      "        31.0412, 31.0412], device='cuda:0')\n",
      "targets: tensor([115.4538, 122.5260, 130.9462, 121.8647, 123.4440, 130.8453, 122.2321,\n",
      "        129.7234, 123.2588, 122.7149], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = get_batch(eval_batch_size, seq_len, divider).to(device)\n",
    "    targets = data.prod(-1).sum(-1).squeeze(-1)\n",
    "    output = model(data)\n",
    "    print('data[0]:', data[0,:50,:])\n",
    "\n",
    "    print('output:', output)\n",
    "    print('targets:', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRBAvZu7fMqE"
   },
   "source": [
    "### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ez6hLt4efLjM",
    "outputId": "6df56109-6233-4f6b-c111-2174ff1cb5dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/i273233/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (linear): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=False)\n",
    "model = model.features[0:9]\n",
    "model[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model[2] = nn.MaxPool2d(kernel_size=2, stride=(2,1), padding=(0,1))\n",
    "model[5] = nn.MaxPool2d(kernel_size=2, stride=(2,1), padding=0)\n",
    "model[8] = nn.Conv2d(256, 256, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model.add_module(\"linear\", nn.Conv2d(256, 1, kernel_size=1, stride=1, padding=0))\n",
    "model = Conv(model)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-B_SfUcfW8Y",
    "outputId": "9f4b11e7-a30e-4e56-b5ae-fc37b9dbfd2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 5000 batches | lr 1.00 | ms/batch 37.73 | loss  0.67 |\n",
      "| epoch   1 |   400/ 5000 batches | lr 1.00 | ms/batch 37.67 | loss  0.48 |\n",
      "| epoch   1 |   600/ 5000 batches | lr 1.00 | ms/batch 37.69 | loss  0.48 |\n",
      "| epoch   1 |   800/ 5000 batches | lr 1.00 | ms/batch 37.69 | loss  0.48 |\n",
      "| epoch   1 |  1000/ 5000 batches | lr 1.00 | ms/batch 37.69 | loss  0.48 |\n",
      "| epoch   1 |  1200/ 5000 batches | lr 1.00 | ms/batch 37.68 | loss  0.48 |\n",
      "| epoch   1 |  1400/ 5000 batches | lr 1.00 | ms/batch 37.71 | loss  0.48 |\n",
      "| epoch   1 |  1600/ 5000 batches | lr 1.00 | ms/batch 37.68 | loss  0.48 |\n",
      "| epoch   1 |  1800/ 5000 batches | lr 1.00 | ms/batch 37.67 | loss  0.48 |\n",
      "| epoch   1 |  2000/ 5000 batches | lr 1.00 | ms/batch 37.68 | loss  0.48 |\n",
      "| epoch   1 |  2200/ 5000 batches | lr 1.00 | ms/batch 37.69 | loss  0.48 |\n",
      "| epoch   1 |  2400/ 5000 batches | lr 1.00 | ms/batch 37.69 | loss  0.47 |\n",
      "| epoch   1 |  2600/ 5000 batches | lr 1.00 | ms/batch 37.68 | loss  0.48 |\n",
      "| epoch   1 |  2800/ 5000 batches | lr 1.00 | ms/batch 37.67 | loss  0.48 |\n",
      "| epoch   1 |  3000/ 5000 batches | lr 1.00 | ms/batch 37.68 | loss  0.48 |\n",
      "| epoch   1 |  3200/ 5000 batches | lr 1.00 | ms/batch 37.70 | loss  0.48 |\n",
      "| epoch   1 |  3400/ 5000 batches | lr 1.00 | ms/batch 37.69 | loss  0.48 |\n",
      "| epoch   1 |  3600/ 5000 batches | lr 1.00 | ms/batch 37.67 | loss  0.48 |\n",
      "| epoch   1 |  3800/ 5000 batches | lr 1.00 | ms/batch 37.85 | loss  0.48 |\n",
      "| epoch   1 |  4000/ 5000 batches | lr 1.00 | ms/batch 37.99 | loss  0.48 |\n",
      "| epoch   1 |  4200/ 5000 batches | lr 1.00 | ms/batch 37.98 | loss  0.48 |\n",
      "| epoch   1 |  4400/ 5000 batches | lr 1.00 | ms/batch 37.98 | loss  0.48 |\n",
      "| epoch   1 |  4600/ 5000 batches | lr 1.00 | ms/batch 37.97 | loss  0.48 |\n",
      "| epoch   1 |  4800/ 5000 batches | lr 1.00 | ms/batch 37.98 | loss  0.48 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 271.28s | valid loss  0.05 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5000 batches | lr 0.95 | ms/batch 38.17 | loss  0.46 |\n",
      "| epoch   2 |   400/ 5000 batches | lr 0.95 | ms/batch 37.96 | loss  0.46 |\n",
      "| epoch   2 |   600/ 5000 batches | lr 0.95 | ms/batch 37.99 | loss  0.46 |\n",
      "| epoch   2 |   800/ 5000 batches | lr 0.95 | ms/batch 37.86 | loss  0.46 |\n",
      "| epoch   2 |  1000/ 5000 batches | lr 0.95 | ms/batch 37.88 | loss  0.46 |\n",
      "| epoch   2 |  1200/ 5000 batches | lr 0.95 | ms/batch 37.97 | loss  0.46 |\n",
      "| epoch   2 |  1400/ 5000 batches | lr 0.95 | ms/batch 37.98 | loss  0.46 |\n",
      "| epoch   2 |  1600/ 5000 batches | lr 0.95 | ms/batch 37.98 | loss  0.46 |\n",
      "| epoch   2 |  1800/ 5000 batches | lr 0.95 | ms/batch 38.00 | loss  0.46 |\n",
      "| epoch   2 |  2000/ 5000 batches | lr 0.95 | ms/batch 37.99 | loss  0.46 |\n",
      "| epoch   2 |  2200/ 5000 batches | lr 0.95 | ms/batch 38.01 | loss  0.46 |\n",
      "| epoch   2 |  2400/ 5000 batches | lr 0.95 | ms/batch 38.01 | loss  0.46 |\n",
      "| epoch   2 |  2600/ 5000 batches | lr 0.95 | ms/batch 37.99 | loss  0.46 |\n",
      "| epoch   2 |  2800/ 5000 batches | lr 0.95 | ms/batch 37.99 | loss  0.46 |\n",
      "| epoch   2 |  3000/ 5000 batches | lr 0.95 | ms/batch 38.01 | loss  0.46 |\n",
      "| epoch   2 |  3200/ 5000 batches | lr 0.95 | ms/batch 37.99 | loss  0.46 |\n",
      "| epoch   2 |  3400/ 5000 batches | lr 0.95 | ms/batch 37.99 | loss  0.46 |\n",
      "| epoch   2 |  3600/ 5000 batches | lr 0.95 | ms/batch 37.96 | loss  0.46 |\n",
      "| epoch   2 |  3800/ 5000 batches | lr 0.95 | ms/batch 37.98 | loss  0.46 |\n",
      "| epoch   2 |  4000/ 5000 batches | lr 0.95 | ms/batch 38.02 | loss  0.46 |\n",
      "| epoch   2 |  4200/ 5000 batches | lr 0.95 | ms/batch 38.02 | loss  0.46 |\n",
      "| epoch   2 |  4400/ 5000 batches | lr 0.95 | ms/batch 38.00 | loss  0.46 |\n",
      "| epoch   2 |  4600/ 5000 batches | lr 0.95 | ms/batch 38.00 | loss  0.46 |\n",
      "| epoch   2 |  4800/ 5000 batches | lr 0.95 | ms/batch 38.03 | loss  0.46 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 272.51s | valid loss  0.05 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5000 batches | lr 0.90 | ms/batch 38.19 | loss  0.45 |\n",
      "| epoch   3 |   400/ 5000 batches | lr 0.90 | ms/batch 38.03 | loss  0.45 |\n",
      "| epoch   3 |   600/ 5000 batches | lr 0.90 | ms/batch 38.02 | loss  0.45 |\n",
      "| epoch   3 |   800/ 5000 batches | lr 0.90 | ms/batch 38.03 | loss  0.45 |\n",
      "| epoch   3 |  1000/ 5000 batches | lr 0.90 | ms/batch 38.01 | loss  0.45 |\n",
      "| epoch   3 |  1200/ 5000 batches | lr 0.90 | ms/batch 38.01 | loss  0.45 |\n",
      "| epoch   3 |  1400/ 5000 batches | lr 0.90 | ms/batch 38.06 | loss  0.45 |\n",
      "| epoch   3 |  1600/ 5000 batches | lr 0.90 | ms/batch 37.72 | loss  0.45 |\n",
      "| epoch   3 |  1800/ 5000 batches | lr 0.90 | ms/batch 37.71 | loss  0.45 |\n",
      "| epoch   3 |  2000/ 5000 batches | lr 0.90 | ms/batch 37.71 | loss  0.45 |\n",
      "| epoch   3 |  2200/ 5000 batches | lr 0.90 | ms/batch 37.73 | loss  0.45 |\n",
      "| epoch   3 |  2400/ 5000 batches | lr 0.90 | ms/batch 37.72 | loss  0.45 |\n",
      "| epoch   3 |  2600/ 5000 batches | lr 0.90 | ms/batch 37.73 | loss  0.45 |\n",
      "| epoch   3 |  2800/ 5000 batches | lr 0.90 | ms/batch 37.73 | loss  0.45 |\n",
      "| epoch   3 |  3000/ 5000 batches | lr 0.90 | ms/batch 37.74 | loss  0.45 |\n",
      "| epoch   3 |  3200/ 5000 batches | lr 0.90 | ms/batch 37.73 | loss  0.45 |\n",
      "| epoch   3 |  3400/ 5000 batches | lr 0.90 | ms/batch 37.72 | loss  0.45 |\n",
      "| epoch   3 |  3600/ 5000 batches | lr 0.90 | ms/batch 37.72 | loss  0.45 |\n",
      "| epoch   3 |  3800/ 5000 batches | lr 0.90 | ms/batch 37.71 | loss  0.45 |\n",
      "| epoch   3 |  4000/ 5000 batches | lr 0.90 | ms/batch 37.72 | loss  0.45 |\n",
      "| epoch   3 |  4200/ 5000 batches | lr 0.90 | ms/batch 37.72 | loss  0.45 |\n",
      "| epoch   3 |  4400/ 5000 batches | lr 0.90 | ms/batch 37.70 | loss  0.45 |\n",
      "| epoch   3 |  4600/ 5000 batches | lr 0.90 | ms/batch 37.77 | loss  0.45 |\n",
      "| epoch   3 |  4800/ 5000 batches | lr 0.90 | ms/batch 38.05 | loss  0.45 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 271.76s | valid loss  0.05 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5000 batches | lr 0.86 | ms/batch 38.18 | loss  0.44 |\n",
      "| epoch   4 |   400/ 5000 batches | lr 0.86 | ms/batch 38.04 | loss  0.44 |\n",
      "| epoch   4 |   600/ 5000 batches | lr 0.86 | ms/batch 38.04 | loss  0.44 |\n",
      "| epoch   4 |   800/ 5000 batches | lr 0.86 | ms/batch 38.02 | loss  0.44 |\n",
      "| epoch   4 |  1000/ 5000 batches | lr 0.86 | ms/batch 38.02 | loss  0.44 |\n",
      "| epoch   4 |  1200/ 5000 batches | lr 0.86 | ms/batch 38.16 | loss  0.44 |\n",
      "| epoch   4 |  1400/ 5000 batches | lr 0.86 | ms/batch 38.04 | loss  0.44 |\n",
      "| epoch   4 |  1600/ 5000 batches | lr 0.86 | ms/batch 38.04 | loss  0.44 |\n",
      "| epoch   4 |  1800/ 5000 batches | lr 0.86 | ms/batch 38.06 | loss  0.44 |\n",
      "| epoch   4 |  2000/ 5000 batches | lr 0.86 | ms/batch 38.04 | loss  0.44 |\n",
      "| epoch   4 |  2200/ 5000 batches | lr 0.86 | ms/batch 38.06 | loss  0.44 |\n",
      "| epoch   4 |  2400/ 5000 batches | lr 0.86 | ms/batch 38.03 | loss  0.44 |\n",
      "| epoch   4 |  2600/ 5000 batches | lr 0.86 | ms/batch 38.02 | loss  0.44 |\n",
      "| epoch   4 |  2800/ 5000 batches | lr 0.86 | ms/batch 38.05 | loss  0.44 |\n",
      "| epoch   4 |  3000/ 5000 batches | lr 0.86 | ms/batch 38.03 | loss  0.44 |\n",
      "| epoch   4 |  3200/ 5000 batches | lr 0.86 | ms/batch 38.06 | loss  0.44 |\n",
      "| epoch   4 |  3400/ 5000 batches | lr 0.86 | ms/batch 38.02 | loss  0.44 |\n",
      "| epoch   4 |  3600/ 5000 batches | lr 0.86 | ms/batch 38.02 | loss  0.44 |\n",
      "| epoch   4 |  3800/ 5000 batches | lr 0.86 | ms/batch 38.04 | loss  0.44 |\n",
      "| epoch   4 |  4000/ 5000 batches | lr 0.86 | ms/batch 37.76 | loss  0.44 |\n",
      "| epoch   4 |  4200/ 5000 batches | lr 0.86 | ms/batch 37.74 | loss  0.44 |\n",
      "| epoch   4 |  4400/ 5000 batches | lr 0.86 | ms/batch 37.75 | loss  0.44 |\n",
      "| epoch   4 |  4600/ 5000 batches | lr 0.86 | ms/batch 37.72 | loss  0.44 |\n",
      "| epoch   4 |  4800/ 5000 batches | lr 0.86 | ms/batch 37.73 | loss  0.44 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 272.51s | valid loss  0.05 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5000 batches | lr 0.81 | ms/batch 37.91 | loss  0.43 |\n",
      "| epoch   5 |   400/ 5000 batches | lr 0.81 | ms/batch 37.70 | loss  0.42 |\n",
      "| epoch   5 |   600/ 5000 batches | lr 0.81 | ms/batch 37.71 | loss  0.43 |\n",
      "| epoch   5 |   800/ 5000 batches | lr 0.81 | ms/batch 37.72 | loss  0.43 |\n",
      "| epoch   5 |  1000/ 5000 batches | lr 0.81 | ms/batch 37.71 | loss  0.43 |\n",
      "| epoch   5 |  1200/ 5000 batches | lr 0.81 | ms/batch 37.88 | loss  0.43 |\n",
      "| epoch   5 |  1400/ 5000 batches | lr 0.81 | ms/batch 38.02 | loss  0.43 |\n",
      "| epoch   5 |  1600/ 5000 batches | lr 0.81 | ms/batch 38.01 | loss  0.43 |\n",
      "| epoch   5 |  1800/ 5000 batches | lr 0.81 | ms/batch 37.98 | loss  0.43 |\n",
      "| epoch   5 |  2000/ 5000 batches | lr 0.81 | ms/batch 38.00 | loss  0.43 |\n",
      "| epoch   5 |  2200/ 5000 batches | lr 0.81 | ms/batch 38.00 | loss  0.43 |\n",
      "| epoch   5 |  2400/ 5000 batches | lr 0.81 | ms/batch 38.01 | loss  0.43 |\n",
      "| epoch   5 |  2600/ 5000 batches | lr 0.81 | ms/batch 38.02 | loss  0.43 |\n",
      "| epoch   5 |  2800/ 5000 batches | lr 0.81 | ms/batch 37.98 | loss  0.43 |\n",
      "| epoch   5 |  3000/ 5000 batches | lr 0.81 | ms/batch 38.00 | loss  0.43 |\n",
      "| epoch   5 |  3200/ 5000 batches | lr 0.81 | ms/batch 38.01 | loss  0.43 |\n",
      "| epoch   5 |  3400/ 5000 batches | lr 0.81 | ms/batch 37.99 | loss  0.43 |\n",
      "| epoch   5 |  3600/ 5000 batches | lr 0.81 | ms/batch 37.97 | loss  0.43 |\n",
      "| epoch   5 |  3800/ 5000 batches | lr 0.81 | ms/batch 38.00 | loss  0.43 |\n",
      "| epoch   5 |  4000/ 5000 batches | lr 0.81 | ms/batch 37.97 | loss  0.43 |\n",
      "| epoch   5 |  4200/ 5000 batches | lr 0.81 | ms/batch 38.03 | loss  0.43 |\n",
      "| epoch   5 |  4400/ 5000 batches | lr 0.81 | ms/batch 38.00 | loss  0.43 |\n",
      "| epoch   5 |  4600/ 5000 batches | lr 0.81 | ms/batch 38.00 | loss  0.43 |\n",
      "| epoch   5 |  4800/ 5000 batches | lr 0.81 | ms/batch 37.97 | loss  0.43 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 272.24s | valid loss  0.05 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.05 | test ppl     1.05\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "seq_len = 1000\n",
    "divider = 4\n",
    "dataset_len = 100000\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "epochs = 5 # The number of epochs\n",
    "\n",
    "model = train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cnpf7knfdNW",
    "outputId": "933ea7d7-bfa5-4a12-9cd9-99ebd337c58c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: tensor([[0.0000, 0.5144],\n",
      "        [0.0000, 0.4849],\n",
      "        [0.0000, 0.8429],\n",
      "        [1.0000, 0.5824],\n",
      "        [0.0000, 0.7493],\n",
      "        [0.0000, 0.3822],\n",
      "        [0.0000, 0.3252],\n",
      "        [0.0000, 0.6235],\n",
      "        [1.0000, 0.2795],\n",
      "        [1.0000, 0.5185],\n",
      "        [0.0000, 0.1497],\n",
      "        [1.0000, 0.9238],\n",
      "        [0.0000, 0.4850],\n",
      "        [1.0000, 0.8176],\n",
      "        [1.0000, 0.3368],\n",
      "        [0.0000, 0.5251],\n",
      "        [0.0000, 0.0786],\n",
      "        [0.0000, 0.8424],\n",
      "        [0.0000, 0.7199],\n",
      "        [0.0000, 0.9394],\n",
      "        [0.0000, 0.0350],\n",
      "        [0.0000, 0.5511],\n",
      "        [0.0000, 0.0335],\n",
      "        [0.0000, 0.6633],\n",
      "        [0.0000, 0.2322],\n",
      "        [0.0000, 0.6020],\n",
      "        [1.0000, 0.5486],\n",
      "        [0.0000, 0.9701],\n",
      "        [1.0000, 0.8008],\n",
      "        [0.0000, 0.0352],\n",
      "        [0.0000, 0.8791],\n",
      "        [1.0000, 0.2091],\n",
      "        [0.0000, 0.9137],\n",
      "        [0.0000, 0.7905],\n",
      "        [0.0000, 0.7517],\n",
      "        [1.0000, 0.0346],\n",
      "        [0.0000, 0.0902],\n",
      "        [0.0000, 0.9536],\n",
      "        [0.0000, 0.8477],\n",
      "        [1.0000, 0.4926],\n",
      "        [0.0000, 0.4298],\n",
      "        [0.0000, 0.2160],\n",
      "        [1.0000, 0.7268],\n",
      "        [1.0000, 0.4519],\n",
      "        [1.0000, 0.4687],\n",
      "        [0.0000, 0.4201],\n",
      "        [0.0000, 0.8140],\n",
      "        [1.0000, 0.0196],\n",
      "        [0.0000, 0.3433],\n",
      "        [1.0000, 0.2634]], device='cuda:0')\n",
      "output: tensor([131.0273, 131.0265, 131.0272, 131.0270, 131.0276, 131.0260, 131.0255,\n",
      "        131.0255, 131.0267, 131.0267], device='cuda:0')\n",
      "targets: tensor([125.7444, 129.0693, 127.3049, 121.3991, 134.2188, 118.3687, 126.0899,\n",
      "        122.0352, 126.7802, 131.4394], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = get_batch(eval_batch_size, seq_len, divider).to(device)\n",
    "    targets = data.prod(-1).sum(-1).squeeze(-1)\n",
    "    output = model(data)\n",
    "    print('data[0]:', data[0,:50,:])\n",
    "\n",
    "    print('output:', output)\n",
    "    print('targets:', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vywCegytDUbK"
   },
   "source": [
    "### Smart pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "id": "HQ08b1msmZKw",
    "outputId": "e7a0898e-cb4e-42dd-fbd3-45ccc8a69e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoXTimes(\n",
      "  (model): Smartpool(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (4): Dropout(p=0.1, inplace=False)\n",
      "      (5): GELU()\n",
      "      (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (7): Dropout(p=0.1, inplace=False)\n",
      "      (8): GELU()\n",
      "      (9): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (10): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| epoch   1 |   200/ 5000 batches | lr 1.00 | ms/batch 72.50 | loss  0.04 |\n",
      "| epoch   1 |   400/ 5000 batches | lr 1.00 | ms/batch 73.39 | loss  0.00 |\n",
      "| epoch   1 |   600/ 5000 batches | lr 1.00 | ms/batch 73.41 | loss  0.00 |\n",
      "| epoch   1 |   800/ 5000 batches | lr 1.00 | ms/batch 73.26 | loss  0.00 |\n",
      "| epoch   1 |  1000/ 5000 batches | lr 1.00 | ms/batch 73.10 | loss  0.00 |\n",
      "| epoch   1 |  1200/ 5000 batches | lr 1.00 | ms/batch 73.38 | loss  0.00 |\n",
      "| epoch   1 |  1400/ 5000 batches | lr 1.00 | ms/batch 73.12 | loss  0.00 |\n",
      "| epoch   1 |  1600/ 5000 batches | lr 1.00 | ms/batch 73.06 | loss  0.00 |\n",
      "| epoch   1 |  1800/ 5000 batches | lr 1.00 | ms/batch 73.35 | loss  0.00 |\n",
      "| epoch   1 |  2000/ 5000 batches | lr 1.00 | ms/batch 73.29 | loss  0.00 |\n",
      "| epoch   1 |  2200/ 5000 batches | lr 1.00 | ms/batch 72.92 | loss  0.00 |\n",
      "| epoch   1 |  2400/ 5000 batches | lr 1.00 | ms/batch 73.45 | loss  0.00 |\n",
      "| epoch   1 |  2600/ 5000 batches | lr 1.00 | ms/batch 72.68 | loss  0.00 |\n",
      "| epoch   1 |  2800/ 5000 batches | lr 1.00 | ms/batch 73.20 | loss  0.00 |\n",
      "| epoch   1 |  3000/ 5000 batches | lr 1.00 | ms/batch 73.26 | loss  0.00 |\n",
      "| epoch   1 |  3200/ 5000 batches | lr 1.00 | ms/batch 73.33 | loss  0.00 |\n",
      "| epoch   1 |  3400/ 5000 batches | lr 1.00 | ms/batch 72.86 | loss  0.00 |\n",
      "| epoch   1 |  3600/ 5000 batches | lr 1.00 | ms/batch 73.05 | loss  0.00 |\n",
      "| epoch   1 |  3800/ 5000 batches | lr 1.00 | ms/batch 72.98 | loss  0.00 |\n",
      "| epoch   1 |  4000/ 5000 batches | lr 1.00 | ms/batch 73.47 | loss  0.00 |\n",
      "| epoch   1 |  4200/ 5000 batches | lr 1.00 | ms/batch 73.19 | loss  0.00 |\n",
      "| epoch   1 |  4400/ 5000 batches | lr 1.00 | ms/batch 73.13 | loss  0.00 |\n",
      "| epoch   1 |  4600/ 5000 batches | lr 1.00 | ms/batch 72.83 | loss  0.00 |\n",
      "| epoch   1 |  4800/ 5000 batches | lr 1.00 | ms/batch 73.16 | loss  0.00 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 498.63s | valid loss  0.00 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5000 batches | lr 0.95 | ms/batch 73.47 | loss  0.00 |\n",
      "| epoch   2 |   400/ 5000 batches | lr 0.95 | ms/batch 72.93 | loss  0.00 |\n",
      "| epoch   2 |   600/ 5000 batches | lr 0.95 | ms/batch 72.96 | loss  0.00 |\n",
      "| epoch   2 |   800/ 5000 batches | lr 0.95 | ms/batch 72.94 | loss  0.00 |\n",
      "| epoch   2 |  1000/ 5000 batches | lr 0.95 | ms/batch 72.69 | loss  0.00 |\n",
      "| epoch   2 |  1200/ 5000 batches | lr 0.95 | ms/batch 72.92 | loss  0.00 |\n",
      "| epoch   2 |  1400/ 5000 batches | lr 0.95 | ms/batch 72.87 | loss  0.00 |\n",
      "| epoch   2 |  1600/ 5000 batches | lr 0.95 | ms/batch 72.73 | loss  0.00 |\n",
      "| epoch   2 |  1800/ 5000 batches | lr 0.95 | ms/batch 68.17 | loss  0.00 |\n",
      "| epoch   2 |  2000/ 5000 batches | lr 0.95 | ms/batch 63.07 | loss  0.00 |\n",
      "| epoch   2 |  2200/ 5000 batches | lr 0.95 | ms/batch 63.06 | loss  0.00 |\n",
      "| epoch   2 |  2400/ 5000 batches | lr 0.95 | ms/batch 63.07 | loss  0.00 |\n",
      "| epoch   2 |  2600/ 5000 batches | lr 0.95 | ms/batch 63.05 | loss  0.00 |\n",
      "| epoch   2 |  2800/ 5000 batches | lr 0.95 | ms/batch 64.84 | loss  0.00 |\n",
      "| epoch   2 |  3000/ 5000 batches | lr 0.95 | ms/batch 72.64 | loss  0.00 |\n",
      "| epoch   2 |  3200/ 5000 batches | lr 0.95 | ms/batch 72.81 | loss  0.00 |\n",
      "| epoch   2 |  3400/ 5000 batches | lr 0.95 | ms/batch 73.00 | loss  0.00 |\n",
      "| epoch   2 |  3600/ 5000 batches | lr 0.95 | ms/batch 72.91 | loss  0.00 |\n",
      "| epoch   2 |  3800/ 5000 batches | lr 0.95 | ms/batch 73.11 | loss  0.00 |\n",
      "| epoch   2 |  4000/ 5000 batches | lr 0.95 | ms/batch 64.45 | loss  0.00 |\n",
      "| epoch   2 |  4200/ 5000 batches | lr 0.95 | ms/batch 63.04 | loss  0.00 |\n",
      "| epoch   2 |  4400/ 5000 batches | lr 0.95 | ms/batch 63.06 | loss  0.00 |\n",
      "| epoch   2 |  4600/ 5000 batches | lr 0.95 | ms/batch 63.04 | loss  0.00 |\n",
      "| epoch   2 |  4800/ 5000 batches | lr 0.95 | ms/batch 63.04 | loss  0.00 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 475.58s | valid loss  0.00 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5000 batches | lr 0.90 | ms/batch 63.36 | loss  0.00 |\n",
      "| epoch   3 |   400/ 5000 batches | lr 0.90 | ms/batch 63.04 | loss  0.00 |\n",
      "| epoch   3 |   600/ 5000 batches | lr 0.90 | ms/batch 63.04 | loss  0.00 |\n",
      "| epoch   3 |   800/ 5000 batches | lr 0.90 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   3 |  1000/ 5000 batches | lr 0.90 | ms/batch 63.05 | loss  0.00 |\n",
      "| epoch   3 |  1200/ 5000 batches | lr 0.90 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   3 |  1400/ 5000 batches | lr 0.90 | ms/batch 63.04 | loss  0.00 |\n",
      "| epoch   3 |  1600/ 5000 batches | lr 0.90 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   3 |  1800/ 5000 batches | lr 0.90 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   3 |  2000/ 5000 batches | lr 0.90 | ms/batch 63.06 | loss  0.00 |\n",
      "| epoch   3 |  2200/ 5000 batches | lr 0.90 | ms/batch 63.05 | loss  0.00 |\n",
      "| epoch   3 |  2400/ 5000 batches | lr 0.90 | ms/batch 63.04 | loss  0.00 |\n",
      "| epoch   3 |  2600/ 5000 batches | lr 0.90 | ms/batch 63.05 | loss  0.00 |\n",
      "| epoch   3 |  2800/ 5000 batches | lr 0.90 | ms/batch 63.05 | loss  0.00 |\n",
      "| epoch   3 |  3000/ 5000 batches | lr 0.90 | ms/batch 63.05 | loss  0.00 |\n",
      "| epoch   3 |  3200/ 5000 batches | lr 0.90 | ms/batch 63.04 | loss  0.00 |\n",
      "| epoch   3 |  3400/ 5000 batches | lr 0.90 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   3 |  3600/ 5000 batches | lr 0.90 | ms/batch 63.05 | loss  0.00 |\n",
      "| epoch   3 |  3800/ 5000 batches | lr 0.90 | ms/batch 63.33 | loss  0.00 |\n",
      "| epoch   3 |  4000/ 5000 batches | lr 0.90 | ms/batch 63.04 | loss  0.00 |\n",
      "| epoch   3 |  4200/ 5000 batches | lr 0.90 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   3 |  4400/ 5000 batches | lr 0.90 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   3 |  4600/ 5000 batches | lr 0.90 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   3 |  4800/ 5000 batches | lr 0.90 | ms/batch 63.04 | loss  0.00 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 448.19s | valid loss  0.00 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5000 batches | lr 0.86 | ms/batch 63.33 | loss  0.00 |\n",
      "| epoch   4 |   400/ 5000 batches | lr 0.86 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   4 |   600/ 5000 batches | lr 0.86 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   4 |   800/ 5000 batches | lr 0.86 | ms/batch 63.04 | loss  0.00 |\n",
      "| epoch   4 |  1000/ 5000 batches | lr 0.86 | ms/batch 63.01 | loss  0.00 |\n",
      "| epoch   4 |  1200/ 5000 batches | lr 0.86 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   4 |  1400/ 5000 batches | lr 0.86 | ms/batch 63.04 | loss  0.00 |\n",
      "| epoch   4 |  1600/ 5000 batches | lr 0.86 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   4 |  1800/ 5000 batches | lr 0.86 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   4 |  2000/ 5000 batches | lr 0.86 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   4 |  2200/ 5000 batches | lr 0.86 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   4 |  2400/ 5000 batches | lr 0.86 | ms/batch 63.05 | loss  0.00 |\n",
      "| epoch   4 |  2600/ 5000 batches | lr 0.86 | ms/batch 63.05 | loss  0.00 |\n",
      "| epoch   4 |  2800/ 5000 batches | lr 0.86 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   4 |  3000/ 5000 batches | lr 0.86 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   4 |  3200/ 5000 batches | lr 0.86 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   4 |  3400/ 5000 batches | lr 0.86 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   4 |  3600/ 5000 batches | lr 0.86 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   4 |  3800/ 5000 batches | lr 0.86 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   4 |  4000/ 5000 batches | lr 0.86 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   4 |  4200/ 5000 batches | lr 0.86 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   4 |  4400/ 5000 batches | lr 0.86 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   4 |  4600/ 5000 batches | lr 0.86 | ms/batch 63.01 | loss  0.00 |\n",
      "| epoch   4 |  4800/ 5000 batches | lr 0.86 | ms/batch 63.04 | loss  0.00 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 448.16s | valid loss  0.00 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5000 batches | lr 0.81 | ms/batch 63.34 | loss  0.00 |\n",
      "| epoch   5 |   400/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   5 |   600/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   5 |   800/ 5000 batches | lr 0.81 | ms/batch 63.04 | loss  0.00 |\n",
      "| epoch   5 |  1000/ 5000 batches | lr 0.81 | ms/batch 63.01 | loss  0.00 |\n",
      "| epoch   5 |  1200/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   5 |  1400/ 5000 batches | lr 0.81 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   5 |  1600/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   5 |  1800/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   5 |  2000/ 5000 batches | lr 0.81 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   5 |  2200/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   5 |  2400/ 5000 batches | lr 0.81 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   5 |  2600/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   5 |  2800/ 5000 batches | lr 0.81 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   5 |  3000/ 5000 batches | lr 0.81 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   5 |  3200/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   5 |  3400/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   5 |  3600/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   5 |  3800/ 5000 batches | lr 0.81 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   5 |  4000/ 5000 batches | lr 0.81 | ms/batch 63.04 | loss  0.00 |\n",
      "| epoch   5 |  4200/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   5 |  4400/ 5000 batches | lr 0.81 | ms/batch 63.02 | loss  0.00 |\n",
      "| epoch   5 |  4600/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "| epoch   5 |  4800/ 5000 batches | lr 0.81 | ms/batch 63.03 | loss  0.00 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 448.04s | valid loss  0.00 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.00 | test ppl     1.00\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "seq_len = 1000\n",
    "divider = 4\n",
    "dataset_len = 100000\n",
    "epochs = 5 # The number of epochs\n",
    "\n",
    "model = DoXTimes(Smartpool(divider, 0.3))\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "model = train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vc3RgEAzvtnA",
    "outputId": "24228518-297b-4f34-c008-6ddc05397c40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: tensor([[0.0000, 0.8117],\n",
      "        [0.0000, 0.2613],\n",
      "        [1.0000, 0.9511],\n",
      "        ...,\n",
      "        [1.0000, 0.7560],\n",
      "        [0.0000, 0.4698],\n",
      "        [0.0000, 0.4036]], device='cuda:0')\n",
      "output: tensor([118.1460, 126.8192, 119.2346, 123.9964, 125.6771, 126.9987, 119.8981,\n",
      "        121.9886, 126.0591, 131.1343], device='cuda:0')\n",
      "targets: tensor([118.1460, 126.8192, 119.2346, 123.9963, 125.6771, 126.9987, 119.8981,\n",
      "        121.9886, 126.0591, 131.1343], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = get_batch(eval_batch_size, seq_len, divider).to(device)\n",
    "    targets = data.prod(-1).sum(-1)\n",
    "    output = model(data)\n",
    "    print('data[0]:', data[0,:,:])\n",
    "\n",
    "    print('output:', output)\n",
    "    print('targets:', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5eKkfBrw6Pd"
   },
   "source": [
    "## Pooling T/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCtbVJecx_mc"
   },
   "source": [
    "### Average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "At-q3olSyDSt",
    "outputId": "077a2f8d-d16f-4c2f-8880-2d90a3f45fe2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/i273233/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): AvgPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1))\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): AvgPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1))\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): AvgPool2d(kernel_size=2, stride=(2, 1), padding=0)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): AvgPool2d(kernel_size=2, stride=(2, 1), padding=0)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (linear): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=False)\n",
    "model = model.features[0:19]\n",
    "model[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model[2] = nn.AvgPool2d(kernel_size=2, stride=(2,1), padding=(0,1))\n",
    "model[5] = nn.AvgPool2d(kernel_size=2, stride=(2,1), padding=(0,1))\n",
    "model[10] = nn.AvgPool2d(kernel_size=2, stride=(2,1), padding=0)\n",
    "model[15] = nn.AvgPool2d(kernel_size=2, stride=(2,1), padding=0)\n",
    "model[18] = nn.Conv2d(512, 512, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model.add_module(\"linear\", nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0))\n",
    "model = Conv(model)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JThVMqz30f2h",
    "outputId": "8aa27cdc-e714-4d56-f32b-47a1cf2e1053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 5120 batches | lr 1.00 | ms/batch 115.94 | loss  0.55 |\n",
      "| epoch   1 |   400/ 5120 batches | lr 1.00 | ms/batch 120.74 | loss  0.49 |\n",
      "| epoch   1 |   600/ 5120 batches | lr 1.00 | ms/batch 123.80 | loss  0.49 |\n",
      "| epoch   1 |   800/ 5120 batches | lr 1.00 | ms/batch 123.74 | loss  0.49 |\n",
      "| epoch   1 |  1000/ 5120 batches | lr 1.00 | ms/batch 123.79 | loss  0.49 |\n",
      "| epoch   1 |  1200/ 5120 batches | lr 1.00 | ms/batch 123.74 | loss  0.49 |\n",
      "| epoch   1 |  1400/ 5120 batches | lr 1.00 | ms/batch 123.86 | loss  0.49 |\n",
      "| epoch   1 |  1600/ 5120 batches | lr 1.00 | ms/batch 123.89 | loss  0.49 |\n",
      "| epoch   1 |  1800/ 5120 batches | lr 1.00 | ms/batch 123.79 | loss  0.49 |\n",
      "| epoch   1 |  2000/ 5120 batches | lr 1.00 | ms/batch 123.77 | loss  0.49 |\n",
      "| epoch   1 |  2200/ 5120 batches | lr 1.00 | ms/batch 123.64 | loss  0.49 |\n",
      "| epoch   1 |  2400/ 5120 batches | lr 1.00 | ms/batch 123.64 | loss  0.49 |\n",
      "| epoch   1 |  2600/ 5120 batches | lr 1.00 | ms/batch 123.62 | loss  0.49 |\n",
      "| epoch   1 |  2800/ 5120 batches | lr 1.00 | ms/batch 123.64 | loss  0.49 |\n",
      "| epoch   1 |  3000/ 5120 batches | lr 1.00 | ms/batch 123.61 | loss  0.49 |\n",
      "| epoch   1 |  3200/ 5120 batches | lr 1.00 | ms/batch 123.10 | loss  0.49 |\n",
      "| epoch   1 |  3400/ 5120 batches | lr 1.00 | ms/batch 123.09 | loss  0.50 |\n",
      "| epoch   1 |  3600/ 5120 batches | lr 1.00 | ms/batch 123.14 | loss  0.49 |\n",
      "| epoch   1 |  3800/ 5120 batches | lr 1.00 | ms/batch 123.13 | loss  0.50 |\n",
      "| epoch   1 |  4000/ 5120 batches | lr 1.00 | ms/batch 123.10 | loss  0.49 |\n",
      "| epoch   1 |  4200/ 5120 batches | lr 1.00 | ms/batch 123.03 | loss  0.49 |\n",
      "| epoch   1 |  4400/ 5120 batches | lr 1.00 | ms/batch 122.94 | loss  0.49 |\n",
      "| epoch   1 |  4600/ 5120 batches | lr 1.00 | ms/batch 122.85 | loss  0.49 |\n",
      "| epoch   1 |  4800/ 5120 batches | lr 1.00 | ms/batch 122.92 | loss  0.49 |\n",
      "| epoch   1 |  5000/ 5120 batches | lr 1.00 | ms/batch 122.91 | loss  0.49 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 912.09s | valid loss  0.89 | valid ppl     2.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5120 batches | lr 0.95 | ms/batch 123.72 | loss  0.54 |\n",
      "| epoch   2 |   400/ 5120 batches | lr 0.95 | ms/batch 123.19 | loss  0.54 |\n",
      "| epoch   2 |   600/ 5120 batches | lr 0.95 | ms/batch 123.22 | loss  0.54 |\n",
      "| epoch   2 |   800/ 5120 batches | lr 0.95 | ms/batch 123.21 | loss  0.54 |\n",
      "| epoch   2 |  1000/ 5120 batches | lr 0.95 | ms/batch 123.17 | loss  0.54 |\n",
      "| epoch   2 |  1200/ 5120 batches | lr 0.95 | ms/batch 123.18 | loss  0.54 |\n",
      "| epoch   2 |  1400/ 5120 batches | lr 0.95 | ms/batch 123.11 | loss  0.54 |\n",
      "| epoch   2 |  1600/ 5120 batches | lr 0.95 | ms/batch 123.22 | loss  0.54 |\n",
      "| epoch   2 |  1800/ 5120 batches | lr 0.95 | ms/batch 123.15 | loss  0.54 |\n",
      "| epoch   2 |  2000/ 5120 batches | lr 0.95 | ms/batch 123.26 | loss  0.54 |\n",
      "| epoch   2 |  2200/ 5120 batches | lr 0.95 | ms/batch 123.24 | loss  0.54 |\n",
      "| epoch   2 |  2400/ 5120 batches | lr 0.95 | ms/batch 123.18 | loss  0.54 |\n",
      "| epoch   2 |  2600/ 5120 batches | lr 0.95 | ms/batch 123.07 | loss  0.54 |\n",
      "| epoch   2 |  2800/ 5120 batches | lr 0.95 | ms/batch 123.19 | loss  0.54 |\n",
      "| epoch   2 |  3000/ 5120 batches | lr 0.95 | ms/batch 123.19 | loss  0.54 |\n",
      "| epoch   2 |  3200/ 5120 batches | lr 0.95 | ms/batch 123.18 | loss  0.54 |\n",
      "| epoch   2 |  3400/ 5120 batches | lr 0.95 | ms/batch 123.10 | loss  0.54 |\n",
      "| epoch   2 |  3600/ 5120 batches | lr 0.95 | ms/batch 123.12 | loss  0.54 |\n",
      "| epoch   2 |  3800/ 5120 batches | lr 0.95 | ms/batch 123.06 | loss  0.54 |\n",
      "| epoch   2 |  4000/ 5120 batches | lr 0.95 | ms/batch 122.98 | loss  0.54 |\n",
      "| epoch   2 |  4200/ 5120 batches | lr 0.95 | ms/batch 123.05 | loss  0.54 |\n",
      "| epoch   2 |  4400/ 5120 batches | lr 0.95 | ms/batch 123.00 | loss  0.54 |\n",
      "| epoch   2 |  4600/ 5120 batches | lr 0.95 | ms/batch 123.04 | loss  0.54 |\n",
      "| epoch   2 |  4800/ 5120 batches | lr 0.95 | ms/batch 123.09 | loss  0.54 |\n",
      "| epoch   2 |  5000/ 5120 batches | lr 0.95 | ms/batch 123.00 | loss  0.54 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 912.89s | valid loss  0.79 | valid ppl     2.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5120 batches | lr 0.90 | ms/batch 123.65 | loss  0.51 |\n",
      "| epoch   3 |   400/ 5120 batches | lr 0.90 | ms/batch 123.00 | loss  0.50 |\n",
      "| epoch   3 |   600/ 5120 batches | lr 0.90 | ms/batch 123.08 | loss  0.50 |\n",
      "| epoch   3 |   800/ 5120 batches | lr 0.90 | ms/batch 123.10 | loss  0.50 |\n",
      "| epoch   3 |  1000/ 5120 batches | lr 0.90 | ms/batch 123.05 | loss  0.50 |\n",
      "| epoch   3 |  1200/ 5120 batches | lr 0.90 | ms/batch 123.14 | loss  0.50 |\n",
      "| epoch   3 |  1400/ 5120 batches | lr 0.90 | ms/batch 123.11 | loss  0.50 |\n",
      "| epoch   3 |  1600/ 5120 batches | lr 0.90 | ms/batch 123.07 | loss  0.50 |\n",
      "| epoch   3 |  1800/ 5120 batches | lr 0.90 | ms/batch 123.06 | loss  0.50 |\n",
      "| epoch   3 |  2000/ 5120 batches | lr 0.90 | ms/batch 123.05 | loss  0.50 |\n",
      "| epoch   3 |  2200/ 5120 batches | lr 0.90 | ms/batch 123.18 | loss  0.50 |\n",
      "| epoch   3 |  2400/ 5120 batches | lr 0.90 | ms/batch 123.12 | loss  0.50 |\n",
      "| epoch   3 |  2600/ 5120 batches | lr 0.90 | ms/batch 123.07 | loss  0.50 |\n",
      "| epoch   3 |  2800/ 5120 batches | lr 0.90 | ms/batch 123.17 | loss  0.50 |\n",
      "| epoch   3 |  3000/ 5120 batches | lr 0.90 | ms/batch 123.16 | loss  0.50 |\n",
      "| epoch   3 |  3200/ 5120 batches | lr 0.90 | ms/batch 123.15 | loss  0.50 |\n",
      "| epoch   3 |  3400/ 5120 batches | lr 0.90 | ms/batch 123.15 | loss  0.50 |\n",
      "| epoch   3 |  3600/ 5120 batches | lr 0.90 | ms/batch 123.20 | loss  0.50 |\n",
      "| epoch   3 |  3800/ 5120 batches | lr 0.90 | ms/batch 123.16 | loss  0.50 |\n",
      "| epoch   3 |  4000/ 5120 batches | lr 0.90 | ms/batch 123.10 | loss  0.50 |\n",
      "| epoch   3 |  4200/ 5120 batches | lr 0.90 | ms/batch 123.16 | loss  0.50 |\n",
      "| epoch   3 |  4400/ 5120 batches | lr 0.90 | ms/batch 123.26 | loss  0.50 |\n",
      "| epoch   3 |  4600/ 5120 batches | lr 0.90 | ms/batch 123.21 | loss  0.50 |\n",
      "| epoch   3 |  4800/ 5120 batches | lr 0.90 | ms/batch 123.18 | loss  0.50 |\n",
      "| epoch   3 |  5000/ 5120 batches | lr 0.90 | ms/batch 123.11 | loss  0.50 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 913.27s | valid loss  0.79 | valid ppl     2.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5120 batches | lr 0.86 | ms/batch 123.79 | loss  0.47 |\n",
      "| epoch   4 |   400/ 5120 batches | lr 0.86 | ms/batch 123.22 | loss  0.47 |\n",
      "| epoch   4 |   600/ 5120 batches | lr 0.86 | ms/batch 123.24 | loss  0.47 |\n",
      "| epoch   4 |   800/ 5120 batches | lr 0.86 | ms/batch 123.32 | loss  0.47 |\n",
      "| epoch   4 |  1000/ 5120 batches | lr 0.86 | ms/batch 123.31 | loss  0.47 |\n",
      "| epoch   4 |  1200/ 5120 batches | lr 0.86 | ms/batch 123.23 | loss  0.47 |\n",
      "| epoch   4 |  1400/ 5120 batches | lr 0.86 | ms/batch 123.11 | loss  0.47 |\n",
      "| epoch   4 |  1600/ 5120 batches | lr 0.86 | ms/batch 123.23 | loss  0.47 |\n",
      "| epoch   4 |  1800/ 5120 batches | lr 0.86 | ms/batch 123.31 | loss  0.47 |\n",
      "| epoch   4 |  2000/ 5120 batches | lr 0.86 | ms/batch 123.25 | loss  0.47 |\n",
      "| epoch   4 |  2200/ 5120 batches | lr 0.86 | ms/batch 123.18 | loss  0.46 |\n",
      "| epoch   4 |  2400/ 5120 batches | lr 0.86 | ms/batch 123.13 | loss  0.47 |\n",
      "| epoch   4 |  2600/ 5120 batches | lr 0.86 | ms/batch 123.23 | loss  0.47 |\n",
      "| epoch   4 |  2800/ 5120 batches | lr 0.86 | ms/batch 123.21 | loss  0.47 |\n",
      "| epoch   4 |  3000/ 5120 batches | lr 0.86 | ms/batch 123.20 | loss  0.47 |\n",
      "| epoch   4 |  3200/ 5120 batches | lr 0.86 | ms/batch 123.09 | loss  0.47 |\n",
      "| epoch   4 |  3400/ 5120 batches | lr 0.86 | ms/batch 123.13 | loss  0.46 |\n",
      "| epoch   4 |  3600/ 5120 batches | lr 0.86 | ms/batch 123.07 | loss  0.47 |\n",
      "| epoch   4 |  3800/ 5120 batches | lr 0.86 | ms/batch 123.06 | loss  0.47 |\n",
      "| epoch   4 |  4000/ 5120 batches | lr 0.86 | ms/batch 123.03 | loss  0.47 |\n",
      "| epoch   4 |  4200/ 5120 batches | lr 0.86 | ms/batch 123.02 | loss  0.47 |\n",
      "| epoch   4 |  4400/ 5120 batches | lr 0.86 | ms/batch 123.09 | loss  0.46 |\n",
      "| epoch   4 |  4600/ 5120 batches | lr 0.86 | ms/batch 123.14 | loss  0.47 |\n",
      "| epoch   4 |  4800/ 5120 batches | lr 0.86 | ms/batch 123.19 | loss  0.46 |\n",
      "| epoch   4 |  5000/ 5120 batches | lr 0.86 | ms/batch 123.19 | loss  0.47 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 913.40s | valid loss  0.79 | valid ppl     2.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5120 batches | lr 0.81 | ms/batch 123.63 | loss  0.44 |\n",
      "| epoch   5 |   400/ 5120 batches | lr 0.81 | ms/batch 123.08 | loss  0.44 |\n",
      "| epoch   5 |   600/ 5120 batches | lr 0.81 | ms/batch 123.19 | loss  0.44 |\n",
      "| epoch   5 |   800/ 5120 batches | lr 0.81 | ms/batch 123.11 | loss  0.49 |\n",
      "| epoch   5 |  1000/ 5120 batches | lr 0.81 | ms/batch 123.13 | loss  0.50 |\n",
      "| epoch   5 |  1200/ 5120 batches | lr 0.81 | ms/batch 123.26 | loss  0.50 |\n",
      "| epoch   5 |  1400/ 5120 batches | lr 0.81 | ms/batch 123.33 | loss  0.50 |\n",
      "| epoch   5 |  1600/ 5120 batches | lr 0.81 | ms/batch 123.30 | loss  0.50 |\n",
      "| epoch   5 |  1800/ 5120 batches | lr 0.81 | ms/batch 123.24 | loss  0.50 |\n",
      "| epoch   5 |  2000/ 5120 batches | lr 0.81 | ms/batch 123.13 | loss  0.50 |\n",
      "| epoch   5 |  2200/ 5120 batches | lr 0.81 | ms/batch 123.17 | loss  0.50 |\n",
      "| epoch   5 |  2400/ 5120 batches | lr 0.81 | ms/batch 123.22 | loss  0.50 |\n",
      "| epoch   5 |  2600/ 5120 batches | lr 0.81 | ms/batch 123.20 | loss  0.50 |\n",
      "| epoch   5 |  2800/ 5120 batches | lr 0.81 | ms/batch 123.29 | loss  0.50 |\n",
      "| epoch   5 |  3000/ 5120 batches | lr 0.81 | ms/batch 123.16 | loss  0.50 |\n",
      "| epoch   5 |  3200/ 5120 batches | lr 0.81 | ms/batch 123.26 | loss  0.50 |\n",
      "| epoch   5 |  3400/ 5120 batches | lr 0.81 | ms/batch 123.23 | loss  0.50 |\n",
      "| epoch   5 |  3600/ 5120 batches | lr 0.81 | ms/batch 123.29 | loss  0.50 |\n",
      "| epoch   5 |  3800/ 5120 batches | lr 0.81 | ms/batch 123.33 | loss  0.50 |\n",
      "| epoch   5 |  4000/ 5120 batches | lr 0.81 | ms/batch 123.12 | loss  0.50 |\n",
      "| epoch   5 |  4200/ 5120 batches | lr 0.81 | ms/batch 123.08 | loss  0.50 |\n",
      "| epoch   5 |  4400/ 5120 batches | lr 0.81 | ms/batch 123.09 | loss  0.50 |\n",
      "| epoch   5 |  4600/ 5120 batches | lr 0.81 | ms/batch 123.15 | loss  0.50 |\n",
      "| epoch   5 |  4800/ 5120 batches | lr 0.81 | ms/batch 123.12 | loss  0.50 |\n",
      "| epoch   5 |  5000/ 5120 batches | lr 0.81 | ms/batch 123.15 | loss  0.50 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 913.66s | valid loss  0.67 | valid ppl     1.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.67 | test ppl     1.95\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "seq_len = 1024\n",
    "divider = 16\n",
    "dataset_len = 102400\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "epochs = 5 # The number of epochs\n",
    "\n",
    "model = train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84iEiRyL0kkQ",
    "outputId": "4beb7bd0-09b1-4ecf-b3ad-a12f9a08b39a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: tensor([[0.0000, 0.3159],\n",
      "        [0.0000, 0.3881],\n",
      "        [0.0000, 0.0700],\n",
      "        [0.0000, 0.3819],\n",
      "        [0.0000, 0.0777],\n",
      "        [0.0000, 0.3863],\n",
      "        [0.0000, 0.2058],\n",
      "        [0.0000, 0.5430],\n",
      "        [0.0000, 0.5289],\n",
      "        [0.0000, 0.0553],\n",
      "        [0.0000, 0.2238],\n",
      "        [0.0000, 0.5081],\n",
      "        [0.0000, 0.3758],\n",
      "        [0.0000, 0.6520],\n",
      "        [0.0000, 0.3167],\n",
      "        [0.0000, 0.8875],\n",
      "        [0.0000, 0.4066],\n",
      "        [0.0000, 0.8926],\n",
      "        [0.0000, 0.8963],\n",
      "        [0.0000, 0.0858],\n",
      "        [0.0000, 0.2926],\n",
      "        [0.0000, 0.7110],\n",
      "        [0.0000, 0.6208],\n",
      "        [0.0000, 0.2697],\n",
      "        [0.0000, 0.9850],\n",
      "        [0.0000, 0.8698],\n",
      "        [0.0000, 0.7973],\n",
      "        [0.0000, 0.1435],\n",
      "        [0.0000, 0.7069],\n",
      "        [0.0000, 0.4728],\n",
      "        [0.0000, 0.0657],\n",
      "        [0.0000, 0.6978],\n",
      "        [0.0000, 0.0677],\n",
      "        [0.0000, 0.6686],\n",
      "        [0.0000, 0.6546],\n",
      "        [0.0000, 0.5797],\n",
      "        [0.0000, 0.9028],\n",
      "        [0.0000, 0.5751],\n",
      "        [0.0000, 0.3478],\n",
      "        [0.0000, 0.6721],\n",
      "        [0.0000, 0.9224],\n",
      "        [0.0000, 0.0895],\n",
      "        [0.0000, 0.4444],\n",
      "        [0.0000, 0.5574],\n",
      "        [0.0000, 0.7633],\n",
      "        [0.0000, 0.8915],\n",
      "        [0.0000, 0.9144],\n",
      "        [0.0000, 0.3515],\n",
      "        [0.0000, 0.8710],\n",
      "        [0.0000, 0.6082]], device='cuda:0')\n",
      "output: tensor([10.6232, 10.6232, 10.6232, 10.6232, 10.6232, 10.6232, 10.6232, 10.6232,\n",
      "        10.6232, 10.6232], device='cuda:0')\n",
      "targets: tensor([32.2080, 32.9095, 34.1089, 31.3225, 33.6871, 27.5791, 33.6800, 26.6792,\n",
      "        29.9740, 36.1351], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = get_batch(eval_batch_size, seq_len, divider).to(device)\n",
    "    targets = data.prod(-1).sum(-1).squeeze(-1)\n",
    "    output = model(data)\n",
    "    print('data[0]:', data[0,:50,:])\n",
    "\n",
    "    print('output:', output)\n",
    "    print('targets:', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsrMRkVd1ksb"
   },
   "source": [
    "### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-B9LVmr1hJ7",
    "outputId": "a0c86c4b-85de-400e-ea88-e0e83c52d50f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/i273233/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (linear): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=False)\n",
    "model = model.features[0:19]\n",
    "model[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model[2] = nn.MaxPool2d(kernel_size=2, stride=(2,1), padding=(0,1))\n",
    "model[5] = nn.MaxPool2d(kernel_size=2, stride=(2,1), padding=(0,1))\n",
    "model[10] = nn.MaxPool2d(kernel_size=2, stride=(2,1), padding=0)\n",
    "model[15] = nn.MaxPool2d(kernel_size=2, stride=(2,1), padding=0)\n",
    "model[18] = nn.Conv2d(512, 512, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model.add_module(\"linear\", nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0))\n",
    "model = Conv(model)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vIgdWg51hRP",
    "outputId": "b574cc0a-2788-4508-eb89-ac563544e8c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 5120 batches | lr 1.00 | ms/batch 122.65 | loss  0.83 |\n",
      "| epoch   1 |   400/ 5120 batches | lr 1.00 | ms/batch 121.98 | loss  0.83 |\n",
      "| epoch   1 |   600/ 5120 batches | lr 1.00 | ms/batch 122.04 | loss  0.82 |\n",
      "| epoch   1 |   800/ 5120 batches | lr 1.00 | ms/batch 122.01 | loss  0.83 |\n",
      "| epoch   1 |  1000/ 5120 batches | lr 1.00 | ms/batch 121.96 | loss  0.83 |\n",
      "| epoch   1 |  1200/ 5120 batches | lr 1.00 | ms/batch 121.91 | loss  0.83 |\n",
      "| epoch   1 |  1400/ 5120 batches | lr 1.00 | ms/batch 121.94 | loss  0.83 |\n",
      "| epoch   1 |  1600/ 5120 batches | lr 1.00 | ms/batch 121.98 | loss  0.82 |\n",
      "| epoch   1 |  1800/ 5120 batches | lr 1.00 | ms/batch 121.86 | loss  0.83 |\n",
      "| epoch   1 |  2000/ 5120 batches | lr 1.00 | ms/batch 121.88 | loss  0.83 |\n",
      "| epoch   1 |  2200/ 5120 batches | lr 1.00 | ms/batch 121.95 | loss  0.83 |\n",
      "| epoch   1 |  2400/ 5120 batches | lr 1.00 | ms/batch 122.00 | loss  0.82 |\n",
      "| epoch   1 |  2600/ 5120 batches | lr 1.00 | ms/batch 121.84 | loss  0.83 |\n",
      "| epoch   1 |  2800/ 5120 batches | lr 1.00 | ms/batch 121.92 | loss  0.83 |\n",
      "| epoch   1 |  3000/ 5120 batches | lr 1.00 | ms/batch 121.89 | loss  0.83 |\n",
      "| epoch   1 |  3200/ 5120 batches | lr 1.00 | ms/batch 121.89 | loss  0.82 |\n",
      "| epoch   1 |  3400/ 5120 batches | lr 1.00 | ms/batch 121.92 | loss  0.82 |\n",
      "| epoch   1 |  3600/ 5120 batches | lr 1.00 | ms/batch 121.86 | loss  0.82 |\n",
      "| epoch   1 |  3800/ 5120 batches | lr 1.00 | ms/batch 121.95 | loss  0.83 |\n",
      "| epoch   1 |  4000/ 5120 batches | lr 1.00 | ms/batch 121.82 | loss  0.82 |\n",
      "| epoch   1 |  4200/ 5120 batches | lr 1.00 | ms/batch 121.79 | loss  0.83 |\n",
      "| epoch   1 |  4400/ 5120 batches | lr 1.00 | ms/batch 121.86 | loss  0.83 |\n",
      "| epoch   1 |  4600/ 5120 batches | lr 1.00 | ms/batch 121.87 | loss  0.83 |\n",
      "| epoch   1 |  4800/ 5120 batches | lr 1.00 | ms/batch 121.90 | loss  0.83 |\n",
      "| epoch   1 |  5000/ 5120 batches | lr 1.00 | ms/batch 121.80 | loss  0.82 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 905.45s | valid loss  1.31 | valid ppl     3.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5120 batches | lr 0.95 | ms/batch 122.55 | loss  0.80 |\n",
      "| epoch   2 |   400/ 5120 batches | lr 0.95 | ms/batch 121.95 | loss  0.80 |\n",
      "| epoch   2 |   600/ 5120 batches | lr 0.95 | ms/batch 122.05 | loss  0.80 |\n",
      "| epoch   2 |   800/ 5120 batches | lr 0.95 | ms/batch 122.01 | loss  0.80 |\n",
      "| epoch   2 |  1000/ 5120 batches | lr 0.95 | ms/batch 121.95 | loss  0.80 |\n",
      "| epoch   2 |  1200/ 5120 batches | lr 0.95 | ms/batch 122.01 | loss  0.79 |\n",
      "| epoch   2 |  1400/ 5120 batches | lr 0.95 | ms/batch 121.99 | loss  0.80 |\n",
      "| epoch   2 |  1600/ 5120 batches | lr 0.95 | ms/batch 121.95 | loss  0.80 |\n",
      "| epoch   2 |  1800/ 5120 batches | lr 0.95 | ms/batch 121.88 | loss  0.79 |\n",
      "| epoch   2 |  2000/ 5120 batches | lr 0.95 | ms/batch 121.96 | loss  0.80 |\n",
      "| epoch   2 |  2200/ 5120 batches | lr 0.95 | ms/batch 121.91 | loss  0.80 |\n",
      "| epoch   2 |  2400/ 5120 batches | lr 0.95 | ms/batch 122.01 | loss  0.79 |\n",
      "| epoch   2 |  2600/ 5120 batches | lr 0.95 | ms/batch 122.04 | loss  0.80 |\n",
      "| epoch   2 |  2800/ 5120 batches | lr 0.95 | ms/batch 121.94 | loss  0.80 |\n",
      "| epoch   2 |  3000/ 5120 batches | lr 0.95 | ms/batch 121.98 | loss  0.80 |\n",
      "| epoch   2 |  3200/ 5120 batches | lr 0.95 | ms/batch 122.03 | loss  0.80 |\n",
      "| epoch   2 |  3400/ 5120 batches | lr 0.95 | ms/batch 122.07 | loss  0.80 |\n",
      "| epoch   2 |  3600/ 5120 batches | lr 0.95 | ms/batch 121.94 | loss  0.80 |\n",
      "| epoch   2 |  3800/ 5120 batches | lr 0.95 | ms/batch 122.00 | loss  0.80 |\n",
      "| epoch   2 |  4000/ 5120 batches | lr 0.95 | ms/batch 121.94 | loss  0.80 |\n",
      "| epoch   2 |  4200/ 5120 batches | lr 0.95 | ms/batch 122.05 | loss  0.79 |\n",
      "| epoch   2 |  4400/ 5120 batches | lr 0.95 | ms/batch 122.00 | loss  0.80 |\n",
      "| epoch   2 |  4600/ 5120 batches | lr 0.95 | ms/batch 122.02 | loss  0.80 |\n",
      "| epoch   2 |  4800/ 5120 batches | lr 0.95 | ms/batch 121.95 | loss  0.80 |\n",
      "| epoch   2 |  5000/ 5120 batches | lr 0.95 | ms/batch 122.07 | loss  0.80 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 905.90s | valid loss  1.31 | valid ppl     3.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5120 batches | lr 0.90 | ms/batch 122.60 | loss  0.77 |\n",
      "| epoch   3 |   400/ 5120 batches | lr 0.90 | ms/batch 121.97 | loss  0.77 |\n",
      "| epoch   3 |   600/ 5120 batches | lr 0.90 | ms/batch 122.03 | loss  0.77 |\n",
      "| epoch   3 |   800/ 5120 batches | lr 0.90 | ms/batch 121.99 | loss  0.77 |\n",
      "| epoch   3 |  1000/ 5120 batches | lr 0.90 | ms/batch 122.05 | loss  0.77 |\n",
      "| epoch   3 |  1200/ 5120 batches | lr 0.90 | ms/batch 122.02 | loss  0.77 |\n",
      "| epoch   3 |  1400/ 5120 batches | lr 0.90 | ms/batch 121.96 | loss  0.77 |\n",
      "| epoch   3 |  1600/ 5120 batches | lr 0.90 | ms/batch 121.98 | loss  0.77 |\n",
      "| epoch   3 |  1800/ 5120 batches | lr 0.90 | ms/batch 122.00 | loss  0.77 |\n",
      "| epoch   3 |  2000/ 5120 batches | lr 0.90 | ms/batch 122.00 | loss  0.77 |\n",
      "| epoch   3 |  2200/ 5120 batches | lr 0.90 | ms/batch 122.08 | loss  0.77 |\n",
      "| epoch   3 |  2400/ 5120 batches | lr 0.90 | ms/batch 121.99 | loss  0.77 |\n",
      "| epoch   3 |  2600/ 5120 batches | lr 0.90 | ms/batch 122.03 | loss  0.77 |\n",
      "| epoch   3 |  2800/ 5120 batches | lr 0.90 | ms/batch 122.04 | loss  0.77 |\n",
      "| epoch   3 |  3000/ 5120 batches | lr 0.90 | ms/batch 122.06 | loss  0.77 |\n",
      "| epoch   3 |  3200/ 5120 batches | lr 0.90 | ms/batch 122.15 | loss  0.77 |\n",
      "| epoch   3 |  3400/ 5120 batches | lr 0.90 | ms/batch 122.07 | loss  0.77 |\n",
      "| epoch   3 |  3600/ 5120 batches | lr 0.90 | ms/batch 122.07 | loss  0.77 |\n",
      "| epoch   3 |  3800/ 5120 batches | lr 0.90 | ms/batch 122.02 | loss  0.76 |\n",
      "| epoch   3 |  4000/ 5120 batches | lr 0.90 | ms/batch 122.07 | loss  0.76 |\n",
      "| epoch   3 |  4200/ 5120 batches | lr 0.90 | ms/batch 122.06 | loss  0.77 |\n",
      "| epoch   3 |  4400/ 5120 batches | lr 0.90 | ms/batch 121.97 | loss  0.77 |\n",
      "| epoch   3 |  4600/ 5120 batches | lr 0.90 | ms/batch 121.98 | loss  0.77 |\n",
      "| epoch   3 |  4800/ 5120 batches | lr 0.90 | ms/batch 121.98 | loss  0.77 |\n",
      "| epoch   3 |  5000/ 5120 batches | lr 0.90 | ms/batch 122.04 | loss  0.77 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 906.00s | valid loss  1.31 | valid ppl     3.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5120 batches | lr 0.86 | ms/batch 122.59 | loss  0.74 |\n",
      "| epoch   4 |   400/ 5120 batches | lr 0.86 | ms/batch 121.99 | loss  0.74 |\n",
      "| epoch   4 |   600/ 5120 batches | lr 0.86 | ms/batch 121.97 | loss  0.74 |\n",
      "| epoch   4 |   800/ 5120 batches | lr 0.86 | ms/batch 122.06 | loss  0.74 |\n",
      "| epoch   4 |  1000/ 5120 batches | lr 0.86 | ms/batch 122.01 | loss  0.74 |\n",
      "| epoch   4 |  1200/ 5120 batches | lr 0.86 | ms/batch 121.98 | loss  0.74 |\n",
      "| epoch   4 |  1400/ 5120 batches | lr 0.86 | ms/batch 121.91 | loss  0.74 |\n",
      "| epoch   4 |  1600/ 5120 batches | lr 0.86 | ms/batch 121.97 | loss  0.74 |\n",
      "| epoch   4 |  1800/ 5120 batches | lr 0.86 | ms/batch 121.99 | loss  0.74 |\n",
      "| epoch   4 |  2000/ 5120 batches | lr 0.86 | ms/batch 121.93 | loss  0.74 |\n",
      "| epoch   4 |  2200/ 5120 batches | lr 0.86 | ms/batch 121.95 | loss  0.74 |\n",
      "| epoch   4 |  2400/ 5120 batches | lr 0.86 | ms/batch 121.98 | loss  0.74 |\n",
      "| epoch   4 |  2600/ 5120 batches | lr 0.86 | ms/batch 121.93 | loss  0.74 |\n",
      "| epoch   4 |  2800/ 5120 batches | lr 0.86 | ms/batch 121.96 | loss  0.74 |\n",
      "| epoch   4 |  3000/ 5120 batches | lr 0.86 | ms/batch 121.95 | loss  0.74 |\n",
      "| epoch   4 |  3200/ 5120 batches | lr 0.86 | ms/batch 121.97 | loss  0.74 |\n",
      "| epoch   4 |  3400/ 5120 batches | lr 0.86 | ms/batch 121.88 | loss  0.74 |\n",
      "| epoch   4 |  3600/ 5120 batches | lr 0.86 | ms/batch 121.94 | loss  0.74 |\n",
      "| epoch   4 |  3800/ 5120 batches | lr 0.86 | ms/batch 121.90 | loss  0.74 |\n",
      "| epoch   4 |  4000/ 5120 batches | lr 0.86 | ms/batch 121.90 | loss  0.73 |\n",
      "| epoch   4 |  4200/ 5120 batches | lr 0.86 | ms/batch 121.94 | loss  0.74 |\n",
      "| epoch   4 |  4400/ 5120 batches | lr 0.86 | ms/batch 121.93 | loss  0.74 |\n",
      "| epoch   4 |  4600/ 5120 batches | lr 0.86 | ms/batch 121.84 | loss  0.74 |\n",
      "| epoch   4 |  4800/ 5120 batches | lr 0.86 | ms/batch 121.90 | loss  0.74 |\n",
      "| epoch   4 |  5000/ 5120 batches | lr 0.86 | ms/batch 121.83 | loss  0.74 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 905.42s | valid loss  1.31 | valid ppl     3.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5120 batches | lr 0.81 | ms/batch 122.53 | loss  0.72 |\n",
      "| epoch   5 |   400/ 5120 batches | lr 0.81 | ms/batch 121.97 | loss  0.71 |\n",
      "| epoch   5 |   600/ 5120 batches | lr 0.81 | ms/batch 122.01 | loss  0.72 |\n",
      "| epoch   5 |   800/ 5120 batches | lr 0.81 | ms/batch 121.97 | loss  0.71 |\n",
      "| epoch   5 |  1000/ 5120 batches | lr 0.81 | ms/batch 121.86 | loss  0.71 |\n",
      "| epoch   5 |  1200/ 5120 batches | lr 0.81 | ms/batch 121.86 | loss  0.71 |\n",
      "| epoch   5 |  1400/ 5120 batches | lr 0.81 | ms/batch 121.87 | loss  0.72 |\n",
      "| epoch   5 |  1600/ 5120 batches | lr 0.81 | ms/batch 121.94 | loss  0.71 |\n",
      "| epoch   5 |  1800/ 5120 batches | lr 0.81 | ms/batch 121.87 | loss  0.71 |\n",
      "| epoch   5 |  2000/ 5120 batches | lr 0.81 | ms/batch 121.83 | loss  0.71 |\n",
      "| epoch   5 |  2200/ 5120 batches | lr 0.81 | ms/batch 121.77 | loss  0.72 |\n",
      "| epoch   5 |  2400/ 5120 batches | lr 0.81 | ms/batch 121.81 | loss  0.71 |\n",
      "| epoch   5 |  2600/ 5120 batches | lr 0.81 | ms/batch 121.80 | loss  0.71 |\n",
      "| epoch   5 |  2800/ 5120 batches | lr 0.81 | ms/batch 121.90 | loss  0.71 |\n",
      "| epoch   5 |  3000/ 5120 batches | lr 0.81 | ms/batch 121.84 | loss  0.71 |\n",
      "| epoch   5 |  3200/ 5120 batches | lr 0.81 | ms/batch 121.81 | loss  0.72 |\n",
      "| epoch   5 |  3400/ 5120 batches | lr 0.81 | ms/batch 121.78 | loss  0.71 |\n",
      "| epoch   5 |  3600/ 5120 batches | lr 0.81 | ms/batch 121.86 | loss  0.71 |\n",
      "| epoch   5 |  3800/ 5120 batches | lr 0.81 | ms/batch 121.83 | loss  0.72 |\n",
      "| epoch   5 |  4000/ 5120 batches | lr 0.81 | ms/batch 121.72 | loss  0.71 |\n",
      "| epoch   5 |  4200/ 5120 batches | lr 0.81 | ms/batch 121.80 | loss  0.71 |\n",
      "| epoch   5 |  4400/ 5120 batches | lr 0.81 | ms/batch 121.78 | loss  0.71 |\n",
      "| epoch   5 |  4600/ 5120 batches | lr 0.81 | ms/batch 121.81 | loss  0.71 |\n",
      "| epoch   5 |  4800/ 5120 batches | lr 0.81 | ms/batch 121.90 | loss  0.71 |\n",
      "| epoch   5 |  5000/ 5120 batches | lr 0.81 | ms/batch 121.88 | loss  0.71 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 904.82s | valid loss  1.30 | valid ppl     3.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  1.30 | test ppl     3.69\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "seq_len = 1024\n",
    "divider = 16\n",
    "dataset_len = 102400\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "epochs = 5 # The number of epochs\n",
    "\n",
    "model = train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00ERMl4S1hUY",
    "outputId": "d9b77f76-c848-40f3-cb7b-110d44fec1d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: tensor([[0.0000, 0.1578],\n",
      "        [0.0000, 0.1991],\n",
      "        [0.0000, 0.9562],\n",
      "        [0.0000, 0.0171],\n",
      "        [0.0000, 0.7283],\n",
      "        [0.0000, 0.1087],\n",
      "        [0.0000, 0.2220],\n",
      "        [0.0000, 0.6999],\n",
      "        [0.0000, 0.9050],\n",
      "        [0.0000, 0.4827],\n",
      "        [0.0000, 0.8929],\n",
      "        [0.0000, 0.3969],\n",
      "        [0.0000, 0.1445],\n",
      "        [0.0000, 0.6179],\n",
      "        [0.0000, 0.8380],\n",
      "        [0.0000, 0.4753],\n",
      "        [0.0000, 0.6989],\n",
      "        [0.0000, 0.9309],\n",
      "        [1.0000, 0.9744],\n",
      "        [0.0000, 0.0209],\n",
      "        [0.0000, 0.7686],\n",
      "        [1.0000, 0.5637],\n",
      "        [0.0000, 0.3032],\n",
      "        [0.0000, 0.1705],\n",
      "        [0.0000, 0.7484],\n",
      "        [0.0000, 0.3289],\n",
      "        [0.0000, 0.0521],\n",
      "        [0.0000, 0.4144],\n",
      "        [0.0000, 0.2647],\n",
      "        [0.0000, 0.8528],\n",
      "        [0.0000, 0.2144],\n",
      "        [0.0000, 0.8776],\n",
      "        [0.0000, 0.7546],\n",
      "        [0.0000, 0.7464],\n",
      "        [1.0000, 0.9732],\n",
      "        [0.0000, 0.6804],\n",
      "        [0.0000, 0.1254],\n",
      "        [0.0000, 0.7739],\n",
      "        [0.0000, 0.8321],\n",
      "        [0.0000, 0.4663],\n",
      "        [0.0000, 0.5382],\n",
      "        [0.0000, 0.6184],\n",
      "        [0.0000, 0.6527],\n",
      "        [0.0000, 0.2243],\n",
      "        [0.0000, 0.4778],\n",
      "        [0.0000, 0.4105],\n",
      "        [0.0000, 0.0087],\n",
      "        [0.0000, 0.2360],\n",
      "        [0.0000, 0.4538],\n",
      "        [1.0000, 0.6617]], device='cuda:0')\n",
      "output: tensor([73.3645, 73.3644, 73.3645, 73.3644, 73.3644, 73.3644, 73.3644, 73.3644,\n",
      "        73.3644, 73.3644], device='cuda:0')\n",
      "targets: tensor([34.7800, 34.7476, 31.4978, 34.0282, 33.8993, 36.2574, 27.0724, 27.0513,\n",
      "        33.6134, 30.7847], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = get_batch(eval_batch_size, seq_len, divider).to(device)\n",
    "    targets = data.prod(-1).sum(-1).squeeze(-1)\n",
    "    output = model(data)\n",
    "    print('data[0]:', data[0,:50,:])\n",
    "\n",
    "    print('output:', output)\n",
    "    print('targets:', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUotUfNTyEIW"
   },
   "source": [
    "### Smart pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k17jb6go106m",
    "outputId": "73bf1ba6-4c04-4503-87e3-a08f242a1396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoXTimes(\n",
      "  (model): Smartpool(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (4): Dropout(p=0.1, inplace=False)\n",
      "      (5): GELU()\n",
      "      (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (7): Dropout(p=0.1, inplace=False)\n",
      "      (8): GELU()\n",
      "      (9): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (10): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| epoch   1 |   200/ 5120 batches | lr 1.00 | ms/batch 63.51 | loss  0.15 |\n",
      "| epoch   1 |   400/ 5120 batches | lr 1.00 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   1 |   600/ 5120 batches | lr 1.00 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   1 |   800/ 5120 batches | lr 1.00 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   1 |  1000/ 5120 batches | lr 1.00 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   1 |  1200/ 5120 batches | lr 1.00 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   1 |  1400/ 5120 batches | lr 1.00 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   1 |  1600/ 5120 batches | lr 1.00 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   1 |  1800/ 5120 batches | lr 1.00 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   1 |  2000/ 5120 batches | lr 1.00 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   1 |  2200/ 5120 batches | lr 1.00 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   1 |  2400/ 5120 batches | lr 1.00 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   1 |  2600/ 5120 batches | lr 1.00 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   1 |  2800/ 5120 batches | lr 1.00 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   1 |  3000/ 5120 batches | lr 1.00 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   1 |  3200/ 5120 batches | lr 1.00 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   1 |  3400/ 5120 batches | lr 1.00 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   1 |  3600/ 5120 batches | lr 1.00 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   1 |  3800/ 5120 batches | lr 1.00 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   1 |  4000/ 5120 batches | lr 1.00 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   1 |  4200/ 5120 batches | lr 1.00 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   1 |  4400/ 5120 batches | lr 1.00 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   1 |  4600/ 5120 batches | lr 1.00 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   1 |  4800/ 5120 batches | lr 1.00 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   1 |  5000/ 5120 batches | lr 1.00 | ms/batch 63.12 | loss  0.00 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 458.77s | valid loss  0.00 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5120 batches | lr 0.95 | ms/batch 63.45 | loss  0.00 |\n",
      "| epoch   2 |   400/ 5120 batches | lr 0.95 | ms/batch 63.15 | loss  0.00 |\n",
      "| epoch   2 |   600/ 5120 batches | lr 0.95 | ms/batch 63.15 | loss  0.00 |\n",
      "| epoch   2 |   800/ 5120 batches | lr 0.95 | ms/batch 63.15 | loss  0.00 |\n",
      "| epoch   2 |  1000/ 5120 batches | lr 0.95 | ms/batch 63.15 | loss  0.00 |\n",
      "| epoch   2 |  1200/ 5120 batches | lr 0.95 | ms/batch 63.15 | loss  0.00 |\n",
      "| epoch   2 |  1400/ 5120 batches | lr 0.95 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   2 |  1600/ 5120 batches | lr 0.95 | ms/batch 63.15 | loss  0.00 |\n",
      "| epoch   2 |  1800/ 5120 batches | lr 0.95 | ms/batch 63.15 | loss  0.00 |\n",
      "| epoch   2 |  2000/ 5120 batches | lr 0.95 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   2 |  2200/ 5120 batches | lr 0.95 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   2 |  2400/ 5120 batches | lr 0.95 | ms/batch 63.15 | loss  0.00 |\n",
      "| epoch   2 |  2600/ 5120 batches | lr 0.95 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   2 |  2800/ 5120 batches | lr 0.95 | ms/batch 63.16 | loss  0.00 |\n",
      "| epoch   2 |  3000/ 5120 batches | lr 0.95 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   2 |  3200/ 5120 batches | lr 0.95 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   2 |  3400/ 5120 batches | lr 0.95 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   2 |  3600/ 5120 batches | lr 0.95 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   2 |  3800/ 5120 batches | lr 0.95 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   2 |  4000/ 5120 batches | lr 0.95 | ms/batch 63.15 | loss  0.00 |\n",
      "| epoch   2 |  4200/ 5120 batches | lr 0.95 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   2 |  4400/ 5120 batches | lr 0.95 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   2 |  4600/ 5120 batches | lr 0.95 | ms/batch 63.15 | loss  0.00 |\n",
      "| epoch   2 |  4800/ 5120 batches | lr 0.95 | ms/batch 63.15 | loss  0.00 |\n",
      "| epoch   2 |  5000/ 5120 batches | lr 0.95 | ms/batch 63.14 | loss  0.00 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 458.92s | valid loss  0.00 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5120 batches | lr 0.90 | ms/batch 63.46 | loss  0.00 |\n",
      "| epoch   3 |   400/ 5120 batches | lr 0.90 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   3 |   600/ 5120 batches | lr 0.90 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   3 |   800/ 5120 batches | lr 0.90 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   3 |  1000/ 5120 batches | lr 0.90 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   3 |  1200/ 5120 batches | lr 0.90 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   3 |  1400/ 5120 batches | lr 0.90 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   3 |  1600/ 5120 batches | lr 0.90 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   3 |  1800/ 5120 batches | lr 0.90 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   3 |  2000/ 5120 batches | lr 0.90 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   3 |  2200/ 5120 batches | lr 0.90 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   3 |  2400/ 5120 batches | lr 0.90 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   3 |  2600/ 5120 batches | lr 0.90 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   3 |  2800/ 5120 batches | lr 0.90 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   3 |  3000/ 5120 batches | lr 0.90 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   3 |  3200/ 5120 batches | lr 0.90 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   3 |  3400/ 5120 batches | lr 0.90 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   3 |  3600/ 5120 batches | lr 0.90 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   3 |  3800/ 5120 batches | lr 0.90 | ms/batch 63.15 | loss  0.00 |\n",
      "| epoch   3 |  4000/ 5120 batches | lr 0.90 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   3 |  4200/ 5120 batches | lr 0.90 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   3 |  4400/ 5120 batches | lr 0.90 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   3 |  4600/ 5120 batches | lr 0.90 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   3 |  4800/ 5120 batches | lr 0.90 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   3 |  5000/ 5120 batches | lr 0.90 | ms/batch 63.13 | loss  0.00 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 458.75s | valid loss  0.00 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5120 batches | lr 0.86 | ms/batch 63.43 | loss  0.00 |\n",
      "| epoch   4 |   400/ 5120 batches | lr 0.86 | ms/batch 63.14 | loss  0.00 |\n",
      "| epoch   4 |   600/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   4 |   800/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   4 |  1000/ 5120 batches | lr 0.86 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   4 |  1200/ 5120 batches | lr 0.86 | ms/batch 63.11 | loss  0.00 |\n",
      "| epoch   4 |  1400/ 5120 batches | lr 0.86 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   4 |  1600/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   4 |  1800/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   4 |  2000/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   4 |  2200/ 5120 batches | lr 0.86 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   4 |  2400/ 5120 batches | lr 0.86 | ms/batch 63.11 | loss  0.00 |\n",
      "| epoch   4 |  2600/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   4 |  2800/ 5120 batches | lr 0.86 | ms/batch 63.11 | loss  0.00 |\n",
      "| epoch   4 |  3000/ 5120 batches | lr 0.86 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   4 |  3200/ 5120 batches | lr 0.86 | ms/batch 63.11 | loss  0.00 |\n",
      "| epoch   4 |  3400/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   4 |  3600/ 5120 batches | lr 0.86 | ms/batch 63.11 | loss  0.00 |\n",
      "| epoch   4 |  3800/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   4 |  4000/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   4 |  4200/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   4 |  4400/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   4 |  4600/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   4 |  4800/ 5120 batches | lr 0.86 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   4 |  5000/ 5120 batches | lr 0.86 | ms/batch 63.12 | loss  0.00 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 458.77s | valid loss  0.00 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5120 batches | lr 0.81 | ms/batch 63.44 | loss  0.00 |\n",
      "| epoch   5 |   400/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |   600/ 5120 batches | lr 0.81 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   5 |   800/ 5120 batches | lr 0.81 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   5 |  1000/ 5120 batches | lr 0.81 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   5 |  1200/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  1400/ 5120 batches | lr 0.81 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   5 |  1600/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  1800/ 5120 batches | lr 0.81 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   5 |  2000/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  2200/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  2400/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  2600/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  2800/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  3000/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  3200/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  3400/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  3600/ 5120 batches | lr 0.81 | ms/batch 63.13 | loss  0.00 |\n",
      "| epoch   5 |  3800/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  4000/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  4200/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  4400/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  4600/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  4800/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "| epoch   5 |  5000/ 5120 batches | lr 0.81 | ms/batch 63.12 | loss  0.00 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 458.86s | valid loss  0.00 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.00 | test ppl     1.00\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "seq_len = 1024\n",
    "divider = 16\n",
    "dataset_len = 102400\n",
    "epochs = 5 # The number of epochs\n",
    "\n",
    "model = DoXTimes(Smartpool(divider, 0.3))\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "model = train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOMLGHgA12cP",
    "outputId": "f1065bfc-fbfe-4883-ef19-b43a3d0d139a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: tensor([[1.0000, 0.9635],\n",
      "        [0.0000, 0.6513],\n",
      "        [0.0000, 0.1477],\n",
      "        [0.0000, 0.5895],\n",
      "        [0.0000, 0.7661],\n",
      "        [0.0000, 0.4402],\n",
      "        [0.0000, 0.7964],\n",
      "        [0.0000, 0.4043],\n",
      "        [0.0000, 0.3817],\n",
      "        [0.0000, 0.4003],\n",
      "        [0.0000, 0.7647],\n",
      "        [0.0000, 0.7788],\n",
      "        [0.0000, 0.3174],\n",
      "        [0.0000, 0.5332],\n",
      "        [0.0000, 0.9874],\n",
      "        [0.0000, 0.1417],\n",
      "        [1.0000, 0.2138],\n",
      "        [0.0000, 0.2606],\n",
      "        [1.0000, 0.4714],\n",
      "        [0.0000, 0.2989],\n",
      "        [0.0000, 0.8235],\n",
      "        [0.0000, 0.1116],\n",
      "        [0.0000, 0.7541],\n",
      "        [0.0000, 0.4020],\n",
      "        [1.0000, 0.6784],\n",
      "        [1.0000, 0.7113],\n",
      "        [0.0000, 0.5481],\n",
      "        [0.0000, 0.9088],\n",
      "        [0.0000, 0.2680],\n",
      "        [0.0000, 0.9701],\n",
      "        [0.0000, 0.2193],\n",
      "        [0.0000, 0.8612],\n",
      "        [0.0000, 0.4654],\n",
      "        [0.0000, 0.3765],\n",
      "        [0.0000, 0.5589],\n",
      "        [0.0000, 0.6168],\n",
      "        [0.0000, 0.0343],\n",
      "        [0.0000, 0.0091],\n",
      "        [0.0000, 0.7646],\n",
      "        [0.0000, 0.1852],\n",
      "        [0.0000, 0.8836],\n",
      "        [0.0000, 0.1186],\n",
      "        [1.0000, 0.8067],\n",
      "        [0.0000, 0.3529],\n",
      "        [0.0000, 0.3338],\n",
      "        [0.0000, 0.1691],\n",
      "        [0.0000, 0.4034],\n",
      "        [0.0000, 0.5362],\n",
      "        [0.0000, 0.2243],\n",
      "        [0.0000, 0.8713]], device='cuda:0')\n",
      "output: tensor([28.6111, 31.7701, 30.9115, 36.6633, 27.8306, 32.6912, 37.0948, 29.6992,\n",
      "        33.2462, 35.4836], device='cuda:0')\n",
      "targets: tensor([28.6111, 31.7701, 30.9116, 36.6633, 27.8306, 32.6913, 37.0948, 29.6992,\n",
      "        33.2462, 35.4836], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = get_batch(eval_batch_size, seq_len, divider).to(device)\n",
    "    targets = data.prod(-1).sum(-1).squeeze(-1)\n",
    "    output = model(data)\n",
    "    print('data[0]:', data[0,:50,:])\n",
    "\n",
    "    print('output:', output)\n",
    "    print('targets:', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VakAqWwX4jZ"
   },
   "source": [
    "# To 1 row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ra3-YwdbWbP9"
   },
   "source": [
    "## Pooling T/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45Da8PVqZZIY"
   },
   "source": [
    "### Average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxTnA1yQZZIf",
    "outputId": "757b60ad-97ec-4be3-f4ec-b4e8d7f09a0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/i273233/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): AvgPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): AvgPool2d(kernel_size=2, stride=(2, 1), padding=0)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (linear): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=False)\n",
    "model = model.features[0:9]\n",
    "model[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model[2] = nn.AvgPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "model[5] = nn.AvgPool2d(kernel_size=2, stride=(2,1), padding=0)\n",
    "model[8] = nn.Conv2d(256, 256, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model.add_module(\"linear\", nn.Conv2d(256, 1, kernel_size=1, stride=1, padding=0))\n",
    "model = Conv(model, 1)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vo-S9WIZZIg",
    "outputId": "7ab15729-423d-46e9-d64b-0bc216bce0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 5000 batches | lr 1.00 | ms/batch 31.55 | loss  0.51 |\n",
      "| epoch   1 |   400/ 5000 batches | lr 1.00 | ms/batch 31.34 | loss  0.50 |\n",
      "| epoch   1 |   600/ 5000 batches | lr 1.00 | ms/batch 31.36 | loss  0.50 |\n",
      "| epoch   1 |   800/ 5000 batches | lr 1.00 | ms/batch 31.37 | loss  0.50 |\n",
      "| epoch   1 |  1000/ 5000 batches | lr 1.00 | ms/batch 31.37 | loss  0.50 |\n",
      "| epoch   1 |  1200/ 5000 batches | lr 1.00 | ms/batch 31.37 | loss  0.50 |\n",
      "| epoch   1 |  1400/ 5000 batches | lr 1.00 | ms/batch 31.55 | loss  0.50 |\n",
      "| epoch   1 |  1600/ 5000 batches | lr 1.00 | ms/batch 32.08 | loss  0.50 |\n",
      "| epoch   1 |  1800/ 5000 batches | lr 1.00 | ms/batch 32.21 | loss  0.50 |\n",
      "| epoch   1 |  2000/ 5000 batches | lr 1.00 | ms/batch 32.52 | loss  0.50 |\n",
      "| epoch   1 |  2200/ 5000 batches | lr 1.00 | ms/batch 32.81 | loss  0.50 |\n",
      "| epoch   1 |  2400/ 5000 batches | lr 1.00 | ms/batch 32.83 | loss  0.50 |\n",
      "| epoch   1 |  2600/ 5000 batches | lr 1.00 | ms/batch 33.19 | loss  0.50 |\n",
      "| epoch   1 |  2800/ 5000 batches | lr 1.00 | ms/batch 33.28 | loss  0.50 |\n",
      "| epoch   1 |  3000/ 5000 batches | lr 1.00 | ms/batch 33.27 | loss  0.50 |\n",
      "| epoch   1 |  3200/ 5000 batches | lr 1.00 | ms/batch 33.28 | loss  0.50 |\n",
      "| epoch   1 |  3400/ 5000 batches | lr 1.00 | ms/batch 33.28 | loss  0.50 |\n",
      "| epoch   1 |  3600/ 5000 batches | lr 1.00 | ms/batch 33.86 | loss  0.50 |\n",
      "| epoch   1 |  3800/ 5000 batches | lr 1.00 | ms/batch 33.82 | loss  0.50 |\n",
      "| epoch   1 |  4000/ 5000 batches | lr 1.00 | ms/batch 33.82 | loss  0.50 |\n",
      "| epoch   1 |  4200/ 5000 batches | lr 1.00 | ms/batch 33.83 | loss  0.50 |\n",
      "| epoch   1 |  4400/ 5000 batches | lr 1.00 | ms/batch 33.84 | loss  0.50 |\n",
      "| epoch   1 |  4600/ 5000 batches | lr 1.00 | ms/batch 33.84 | loss  0.50 |\n",
      "| epoch   1 |  4800/ 5000 batches | lr 1.00 | ms/batch 33.85 | loss  0.50 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 243.91s | valid loss  0.77 | valid ppl     2.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5000 batches | lr 0.95 | ms/batch 34.72 | loss  0.48 |\n",
      "| epoch   2 |   400/ 5000 batches | lr 0.95 | ms/batch 34.56 | loss  0.48 |\n",
      "| epoch   2 |   600/ 5000 batches | lr 0.95 | ms/batch 34.52 | loss  0.48 |\n",
      "| epoch   2 |   800/ 5000 batches | lr 0.95 | ms/batch 34.54 | loss  0.48 |\n",
      "| epoch   2 |  1000/ 5000 batches | lr 0.95 | ms/batch 34.51 | loss  0.48 |\n",
      "| epoch   2 |  1200/ 5000 batches | lr 0.95 | ms/batch 34.53 | loss  0.48 |\n",
      "| epoch   2 |  1400/ 5000 batches | lr 0.95 | ms/batch 34.54 | loss  0.48 |\n",
      "| epoch   2 |  1600/ 5000 batches | lr 0.95 | ms/batch 34.52 | loss  0.48 |\n",
      "| epoch   2 |  1800/ 5000 batches | lr 0.95 | ms/batch 34.50 | loss  0.48 |\n",
      "| epoch   2 |  2000/ 5000 batches | lr 0.95 | ms/batch 34.52 | loss  0.47 |\n",
      "| epoch   2 |  2200/ 5000 batches | lr 0.95 | ms/batch 34.51 | loss  0.47 |\n",
      "| epoch   2 |  2400/ 5000 batches | lr 0.95 | ms/batch 34.50 | loss  0.47 |\n",
      "| epoch   2 |  2600/ 5000 batches | lr 0.95 | ms/batch 34.49 | loss  0.48 |\n",
      "| epoch   2 |  2800/ 5000 batches | lr 0.95 | ms/batch 34.49 | loss  0.48 |\n",
      "| epoch   2 |  3000/ 5000 batches | lr 0.95 | ms/batch 34.50 | loss  0.48 |\n",
      "| epoch   2 |  3200/ 5000 batches | lr 0.95 | ms/batch 34.52 | loss  0.47 |\n",
      "| epoch   2 |  3400/ 5000 batches | lr 0.95 | ms/batch 34.60 | loss  0.48 |\n",
      "| epoch   2 |  3600/ 5000 batches | lr 0.95 | ms/batch 34.60 | loss  0.48 |\n",
      "| epoch   2 |  3800/ 5000 batches | lr 0.95 | ms/batch 34.63 | loss  0.48 |\n",
      "| epoch   2 |  4000/ 5000 batches | lr 0.95 | ms/batch 34.62 | loss  0.48 |\n",
      "| epoch   2 |  4200/ 5000 batches | lr 0.95 | ms/batch 34.62 | loss  0.48 |\n",
      "| epoch   2 |  4400/ 5000 batches | lr 0.95 | ms/batch 34.62 | loss  0.48 |\n",
      "| epoch   2 |  4600/ 5000 batches | lr 0.95 | ms/batch 34.63 | loss  0.48 |\n",
      "| epoch   2 |  4800/ 5000 batches | lr 0.95 | ms/batch 34.62 | loss  0.48 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 254.01s | valid loss  0.77 | valid ppl     2.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5000 batches | lr 0.90 | ms/batch 34.81 | loss  0.45 |\n",
      "| epoch   3 |   400/ 5000 batches | lr 0.90 | ms/batch 34.62 | loss  0.45 |\n",
      "| epoch   3 |   600/ 5000 batches | lr 0.90 | ms/batch 34.64 | loss  0.45 |\n",
      "| epoch   3 |   800/ 5000 batches | lr 0.90 | ms/batch 34.63 | loss  0.45 |\n",
      "| epoch   3 |  1000/ 5000 batches | lr 0.90 | ms/batch 34.63 | loss  0.45 |\n",
      "| epoch   3 |  1200/ 5000 batches | lr 0.90 | ms/batch 34.65 | loss  0.45 |\n",
      "| epoch   3 |  1400/ 5000 batches | lr 0.90 | ms/batch 34.64 | loss  0.45 |\n",
      "| epoch   3 |  1600/ 5000 batches | lr 0.90 | ms/batch 34.64 | loss  0.45 |\n",
      "| epoch   3 |  1800/ 5000 batches | lr 0.90 | ms/batch 34.62 | loss  0.45 |\n",
      "| epoch   3 |  2000/ 5000 batches | lr 0.90 | ms/batch 34.62 | loss  0.45 |\n",
      "| epoch   3 |  2200/ 5000 batches | lr 0.90 | ms/batch 34.63 | loss  0.45 |\n",
      "| epoch   3 |  2400/ 5000 batches | lr 0.90 | ms/batch 34.63 | loss  0.45 |\n",
      "| epoch   3 |  2600/ 5000 batches | lr 0.90 | ms/batch 34.64 | loss  0.45 |\n",
      "| epoch   3 |  2800/ 5000 batches | lr 0.90 | ms/batch 34.65 | loss  0.45 |\n",
      "| epoch   3 |  3000/ 5000 batches | lr 0.90 | ms/batch 34.65 | loss  0.45 |\n",
      "| epoch   3 |  3200/ 5000 batches | lr 0.90 | ms/batch 34.63 | loss  0.45 |\n",
      "| epoch   3 |  3400/ 5000 batches | lr 0.90 | ms/batch 34.63 | loss  0.45 |\n",
      "| epoch   3 |  3600/ 5000 batches | lr 0.90 | ms/batch 34.64 | loss  0.45 |\n",
      "| epoch   3 |  3800/ 5000 batches | lr 0.90 | ms/batch 34.64 | loss  0.45 |\n",
      "| epoch   3 |  4000/ 5000 batches | lr 0.90 | ms/batch 34.64 | loss  0.45 |\n",
      "| epoch   3 |  4200/ 5000 batches | lr 0.90 | ms/batch 34.62 | loss  0.45 |\n",
      "| epoch   3 |  4400/ 5000 batches | lr 0.90 | ms/batch 34.64 | loss  0.45 |\n",
      "| epoch   3 |  4600/ 5000 batches | lr 0.90 | ms/batch 34.65 | loss  0.45 |\n",
      "| epoch   3 |  4800/ 5000 batches | lr 0.90 | ms/batch 34.64 | loss  0.45 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 254.47s | valid loss  0.77 | valid ppl     2.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5000 batches | lr 0.86 | ms/batch 34.80 | loss  0.43 |\n",
      "| epoch   4 |   400/ 5000 batches | lr 0.86 | ms/batch 34.62 | loss  0.43 |\n",
      "| epoch   4 |   600/ 5000 batches | lr 0.86 | ms/batch 34.62 | loss  0.43 |\n",
      "| epoch   4 |   800/ 5000 batches | lr 0.86 | ms/batch 34.64 | loss  0.43 |\n",
      "| epoch   4 |  1000/ 5000 batches | lr 0.86 | ms/batch 34.64 | loss  0.43 |\n",
      "| epoch   4 |  1200/ 5000 batches | lr 0.86 | ms/batch 34.63 | loss  0.43 |\n",
      "| epoch   4 |  1400/ 5000 batches | lr 0.86 | ms/batch 34.63 | loss  0.43 |\n",
      "| epoch   4 |  1600/ 5000 batches | lr 0.86 | ms/batch 34.63 | loss  0.43 |\n",
      "| epoch   4 |  1800/ 5000 batches | lr 0.86 | ms/batch 34.63 | loss  0.43 |\n",
      "| epoch   4 |  2000/ 5000 batches | lr 0.86 | ms/batch 34.62 | loss  0.43 |\n",
      "| epoch   4 |  2200/ 5000 batches | lr 0.86 | ms/batch 34.63 | loss  0.43 |\n",
      "| epoch   4 |  2400/ 5000 batches | lr 0.86 | ms/batch 34.63 | loss  0.43 |\n",
      "| epoch   4 |  2600/ 5000 batches | lr 0.86 | ms/batch 34.65 | loss  0.43 |\n",
      "| epoch   4 |  2800/ 5000 batches | lr 0.86 | ms/batch 34.64 | loss  0.43 |\n",
      "| epoch   4 |  3000/ 5000 batches | lr 0.86 | ms/batch 34.65 | loss  0.43 |\n",
      "| epoch   4 |  3200/ 5000 batches | lr 0.86 | ms/batch 34.64 | loss  0.43 |\n",
      "| epoch   4 |  3400/ 5000 batches | lr 0.86 | ms/batch 34.65 | loss  0.43 |\n",
      "| epoch   4 |  3600/ 5000 batches | lr 0.86 | ms/batch 34.65 | loss  0.43 |\n",
      "| epoch   4 |  3800/ 5000 batches | lr 0.86 | ms/batch 34.66 | loss  0.43 |\n",
      "| epoch   4 |  4000/ 5000 batches | lr 0.86 | ms/batch 34.64 | loss  0.43 |\n",
      "| epoch   4 |  4200/ 5000 batches | lr 0.86 | ms/batch 34.66 | loss  0.43 |\n",
      "| epoch   4 |  4400/ 5000 batches | lr 0.86 | ms/batch 34.65 | loss  0.43 |\n",
      "| epoch   4 |  4600/ 5000 batches | lr 0.86 | ms/batch 34.64 | loss  0.43 |\n",
      "| epoch   4 |  4800/ 5000 batches | lr 0.86 | ms/batch 34.63 | loss  0.43 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 254.50s | valid loss  0.77 | valid ppl     2.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5000 batches | lr 0.81 | ms/batch 34.84 | loss  0.41 |\n",
      "| epoch   5 |   400/ 5000 batches | lr 0.81 | ms/batch 34.66 | loss  0.41 |\n",
      "| epoch   5 |   600/ 5000 batches | lr 0.81 | ms/batch 34.67 | loss  0.41 |\n",
      "| epoch   5 |   800/ 5000 batches | lr 0.81 | ms/batch 34.66 | loss  0.41 |\n",
      "| epoch   5 |  1000/ 5000 batches | lr 0.81 | ms/batch 34.66 | loss  0.41 |\n",
      "| epoch   5 |  1200/ 5000 batches | lr 0.81 | ms/batch 34.67 | loss  0.41 |\n",
      "| epoch   5 |  1400/ 5000 batches | lr 0.81 | ms/batch 34.67 | loss  0.41 |\n",
      "| epoch   5 |  1600/ 5000 batches | lr 0.81 | ms/batch 34.65 | loss  0.41 |\n",
      "| epoch   5 |  1800/ 5000 batches | lr 0.81 | ms/batch 34.66 | loss  0.41 |\n",
      "| epoch   5 |  2000/ 5000 batches | lr 0.81 | ms/batch 34.65 | loss  0.41 |\n",
      "| epoch   5 |  2200/ 5000 batches | lr 0.81 | ms/batch 34.65 | loss  0.41 |\n",
      "| epoch   5 |  2400/ 5000 batches | lr 0.81 | ms/batch 34.64 | loss  0.41 |\n",
      "| epoch   5 |  2600/ 5000 batches | lr 0.81 | ms/batch 34.65 | loss  0.41 |\n",
      "| epoch   5 |  2800/ 5000 batches | lr 0.81 | ms/batch 34.66 | loss  0.41 |\n",
      "| epoch   5 |  3000/ 5000 batches | lr 0.81 | ms/batch 34.63 | loss  0.41 |\n",
      "| epoch   5 |  3200/ 5000 batches | lr 0.81 | ms/batch 34.64 | loss  0.41 |\n",
      "| epoch   5 |  3400/ 5000 batches | lr 0.81 | ms/batch 34.67 | loss  0.41 |\n",
      "| epoch   5 |  3600/ 5000 batches | lr 0.81 | ms/batch 34.64 | loss  0.41 |\n",
      "| epoch   5 |  3800/ 5000 batches | lr 0.81 | ms/batch 34.65 | loss  0.41 |\n",
      "| epoch   5 |  4000/ 5000 batches | lr 0.81 | ms/batch 34.65 | loss  0.41 |\n",
      "| epoch   5 |  4200/ 5000 batches | lr 0.81 | ms/batch 34.63 | loss  0.41 |\n",
      "| epoch   5 |  4400/ 5000 batches | lr 0.81 | ms/batch 34.63 | loss  0.41 |\n",
      "| epoch   5 |  4600/ 5000 batches | lr 0.81 | ms/batch 34.62 | loss  0.41 |\n",
      "| epoch   5 |  4800/ 5000 batches | lr 0.81 | ms/batch 34.64 | loss  0.41 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 254.54s | valid loss  0.72 | valid ppl     2.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.72 | test ppl     2.05\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "seq_len = 1000\n",
    "divider = 4\n",
    "dataset_len = 100000\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "epochs = 5 # The number of epochs\n",
    "\n",
    "model = train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PgRcYUzZZIg",
    "outputId": "42caac6b-cbfc-4c21-abb5-6f7090dbb86b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: tensor([[0.0000, 0.2617],\n",
      "        [0.0000, 0.3286],\n",
      "        [0.0000, 0.5726],\n",
      "        [0.0000, 0.9218],\n",
      "        [0.0000, 0.3122],\n",
      "        [1.0000, 0.6036],\n",
      "        [0.0000, 0.2931],\n",
      "        [0.0000, 0.2205],\n",
      "        [0.0000, 0.9611],\n",
      "        [0.0000, 0.6294],\n",
      "        [0.0000, 0.9402],\n",
      "        [0.0000, 0.4345],\n",
      "        [0.0000, 0.6048],\n",
      "        [0.0000, 0.7359],\n",
      "        [0.0000, 0.6780],\n",
      "        [0.0000, 0.0507],\n",
      "        [0.0000, 0.5398],\n",
      "        [1.0000, 0.0462],\n",
      "        [0.0000, 0.3863],\n",
      "        [0.0000, 0.4442],\n",
      "        [0.0000, 0.2296],\n",
      "        [1.0000, 0.0960],\n",
      "        [0.0000, 0.8954],\n",
      "        [0.0000, 0.1312],\n",
      "        [0.0000, 0.9982],\n",
      "        [0.0000, 0.8226],\n",
      "        [0.0000, 0.3142],\n",
      "        [0.0000, 0.3401],\n",
      "        [1.0000, 0.6303],\n",
      "        [1.0000, 0.5184],\n",
      "        [0.0000, 0.7984],\n",
      "        [0.0000, 0.4285],\n",
      "        [0.0000, 0.7007],\n",
      "        [0.0000, 0.9994],\n",
      "        [1.0000, 0.2633],\n",
      "        [1.0000, 0.9739],\n",
      "        [1.0000, 0.8579],\n",
      "        [0.0000, 0.1787],\n",
      "        [0.0000, 0.1542],\n",
      "        [0.0000, 0.6181],\n",
      "        [0.0000, 0.9850],\n",
      "        [1.0000, 0.7632],\n",
      "        [0.0000, 0.8211],\n",
      "        [0.0000, 0.9135],\n",
      "        [0.0000, 0.1294],\n",
      "        [0.0000, 0.4572],\n",
      "        [0.0000, 0.2037],\n",
      "        [0.0000, 0.1750],\n",
      "        [1.0000, 0.6225],\n",
      "        [0.0000, 0.7357]], device='cuda:0')\n",
      "output: tensor([35.3504, 35.3503, 35.3503, 35.3504, 35.3504, 35.3504, 35.3504, 35.3504,\n",
      "        35.3504, 35.3504], device='cuda:0')\n",
      "targets: tensor([130.9722, 122.9335, 128.7154, 137.8438, 122.2178, 119.1000, 121.1519,\n",
      "        131.2838, 125.7765, 132.0219], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = get_batch(eval_batch_size, seq_len, divider).to(device)\n",
    "    targets = data.prod(-1).sum(-1).squeeze(-1)\n",
    "    output = model(data)\n",
    "    print('data[0]:', data[0,:50,:])\n",
    "\n",
    "    print('output:', output)\n",
    "    print('targets:', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDMMy0rQZOsm"
   },
   "source": [
    "### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W6bz_4tZZOs0",
    "outputId": "0e86ade7-7fb2-4cd9-f4fd-ff24853c030a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/i273233/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (linear): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=False)\n",
    "model = model.features[0:9]\n",
    "model[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model[2] = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "model[5] = nn.MaxPool2d(kernel_size=2, stride=(2,1), padding=0)\n",
    "model[8] = nn.Conv2d(256, 256, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model.add_module(\"linear\", nn.Conv2d(256, 1, kernel_size=1, stride=1, padding=0))\n",
    "model = Conv(model, 1)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XdECFdvZOs1",
    "outputId": "215eb78b-cb2c-47a5-ad42-eaec9e7a32df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 5000 batches | lr 1.00 | ms/batch 34.37 | loss  0.51 |\n",
      "| epoch   1 |   400/ 5000 batches | lr 1.00 | ms/batch 34.32 | loss  0.50 |\n",
      "| epoch   1 |   600/ 5000 batches | lr 1.00 | ms/batch 34.32 | loss  0.50 |\n",
      "| epoch   1 |   800/ 5000 batches | lr 1.00 | ms/batch 34.33 | loss  0.50 |\n",
      "| epoch   1 |  1000/ 5000 batches | lr 1.00 | ms/batch 34.34 | loss  0.50 |\n",
      "| epoch   1 |  1200/ 5000 batches | lr 1.00 | ms/batch 34.31 | loss  0.50 |\n",
      "| epoch   1 |  1400/ 5000 batches | lr 1.00 | ms/batch 34.33 | loss  0.50 |\n",
      "| epoch   1 |  1600/ 5000 batches | lr 1.00 | ms/batch 34.33 | loss  0.50 |\n",
      "| epoch   1 |  1800/ 5000 batches | lr 1.00 | ms/batch 34.33 | loss  0.50 |\n",
      "| epoch   1 |  2000/ 5000 batches | lr 1.00 | ms/batch 34.33 | loss  0.50 |\n",
      "| epoch   1 |  2200/ 5000 batches | lr 1.00 | ms/batch 34.34 | loss  0.50 |\n",
      "| epoch   1 |  2400/ 5000 batches | lr 1.00 | ms/batch 34.31 | loss  0.50 |\n",
      "| epoch   1 |  2600/ 5000 batches | lr 1.00 | ms/batch 34.31 | loss  0.50 |\n",
      "| epoch   1 |  2800/ 5000 batches | lr 1.00 | ms/batch 34.32 | loss  0.50 |\n",
      "| epoch   1 |  3000/ 5000 batches | lr 1.00 | ms/batch 34.33 | loss  0.50 |\n",
      "| epoch   1 |  3200/ 5000 batches | lr 1.00 | ms/batch 34.32 | loss  0.50 |\n",
      "| epoch   1 |  3400/ 5000 batches | lr 1.00 | ms/batch 34.31 | loss  0.50 |\n",
      "| epoch   1 |  3600/ 5000 batches | lr 1.00 | ms/batch 34.33 | loss  0.50 |\n",
      "| epoch   1 |  3800/ 5000 batches | lr 1.00 | ms/batch 34.34 | loss  0.50 |\n",
      "| epoch   1 |  4000/ 5000 batches | lr 1.00 | ms/batch 34.34 | loss  0.50 |\n",
      "| epoch   1 |  4200/ 5000 batches | lr 1.00 | ms/batch 34.34 | loss  0.50 |\n",
      "| epoch   1 |  4400/ 5000 batches | lr 1.00 | ms/batch 34.33 | loss  0.50 |\n",
      "| epoch   1 |  4600/ 5000 batches | lr 1.00 | ms/batch 34.34 | loss  0.50 |\n",
      "| epoch   1 |  4800/ 5000 batches | lr 1.00 | ms/batch 34.34 | loss  0.50 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 252.63s | valid loss  0.65 | valid ppl     1.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5000 batches | lr 0.95 | ms/batch 34.47 | loss  0.48 |\n",
      "| epoch   2 |   400/ 5000 batches | lr 0.95 | ms/batch 34.30 | loss  0.48 |\n",
      "| epoch   2 |   600/ 5000 batches | lr 0.95 | ms/batch 34.31 | loss  0.48 |\n",
      "| epoch   2 |   800/ 5000 batches | lr 0.95 | ms/batch 34.28 | loss  0.48 |\n",
      "| epoch   2 |  1000/ 5000 batches | lr 0.95 | ms/batch 34.30 | loss  0.48 |\n",
      "| epoch   2 |  1200/ 5000 batches | lr 0.95 | ms/batch 34.30 | loss  0.48 |\n",
      "| epoch   2 |  1400/ 5000 batches | lr 0.95 | ms/batch 34.29 | loss  0.48 |\n",
      "| epoch   2 |  1600/ 5000 batches | lr 0.95 | ms/batch 34.30 | loss  0.48 |\n",
      "| epoch   2 |  1800/ 5000 batches | lr 0.95 | ms/batch 34.29 | loss  0.48 |\n",
      "| epoch   2 |  2000/ 5000 batches | lr 0.95 | ms/batch 34.29 | loss  0.48 |\n",
      "| epoch   2 |  2200/ 5000 batches | lr 0.95 | ms/batch 34.29 | loss  0.48 |\n",
      "| epoch   2 |  2400/ 5000 batches | lr 0.95 | ms/batch 34.31 | loss  0.48 |\n",
      "| epoch   2 |  2600/ 5000 batches | lr 0.95 | ms/batch 34.30 | loss  0.48 |\n",
      "| epoch   2 |  2800/ 5000 batches | lr 0.95 | ms/batch 34.29 | loss  0.48 |\n",
      "| epoch   2 |  3000/ 5000 batches | lr 0.95 | ms/batch 34.29 | loss  0.47 |\n",
      "| epoch   2 |  3200/ 5000 batches | lr 0.95 | ms/batch 34.29 | loss  0.48 |\n",
      "| epoch   2 |  3400/ 5000 batches | lr 0.95 | ms/batch 34.29 | loss  0.48 |\n",
      "| epoch   2 |  3600/ 5000 batches | lr 0.95 | ms/batch 34.30 | loss  0.48 |\n",
      "| epoch   2 |  3800/ 5000 batches | lr 0.95 | ms/batch 34.30 | loss  0.47 |\n",
      "| epoch   2 |  4000/ 5000 batches | lr 0.95 | ms/batch 34.29 | loss  0.47 |\n",
      "| epoch   2 |  4200/ 5000 batches | lr 0.95 | ms/batch 34.28 | loss  0.48 |\n",
      "| epoch   2 |  4400/ 5000 batches | lr 0.95 | ms/batch 34.28 | loss  0.47 |\n",
      "| epoch   2 |  4600/ 5000 batches | lr 0.95 | ms/batch 34.29 | loss  0.48 |\n",
      "| epoch   2 |  4800/ 5000 batches | lr 0.95 | ms/batch 34.27 | loss  0.48 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 252.38s | valid loss  0.65 | valid ppl     1.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5000 batches | lr 0.90 | ms/batch 34.45 | loss  0.45 |\n",
      "| epoch   3 |   400/ 5000 batches | lr 0.90 | ms/batch 34.27 | loss  0.45 |\n",
      "| epoch   3 |   600/ 5000 batches | lr 0.90 | ms/batch 34.25 | loss  0.45 |\n",
      "| epoch   3 |   800/ 5000 batches | lr 0.90 | ms/batch 34.25 | loss  0.45 |\n",
      "| epoch   3 |  1000/ 5000 batches | lr 0.90 | ms/batch 34.25 | loss  0.45 |\n",
      "| epoch   3 |  1200/ 5000 batches | lr 0.90 | ms/batch 34.25 | loss  0.45 |\n",
      "| epoch   3 |  1400/ 5000 batches | lr 0.90 | ms/batch 34.26 | loss  0.45 |\n",
      "| epoch   3 |  1600/ 5000 batches | lr 0.90 | ms/batch 34.27 | loss  0.45 |\n",
      "| epoch   3 |  1800/ 5000 batches | lr 0.90 | ms/batch 34.25 | loss  0.45 |\n",
      "| epoch   3 |  2000/ 5000 batches | lr 0.90 | ms/batch 34.26 | loss  0.45 |\n",
      "| epoch   3 |  2200/ 5000 batches | lr 0.90 | ms/batch 34.25 | loss  0.45 |\n",
      "| epoch   3 |  2400/ 5000 batches | lr 0.90 | ms/batch 34.26 | loss  0.45 |\n",
      "| epoch   3 |  2600/ 5000 batches | lr 0.90 | ms/batch 34.26 | loss  0.45 |\n",
      "| epoch   3 |  2800/ 5000 batches | lr 0.90 | ms/batch 34.26 | loss  0.45 |\n",
      "| epoch   3 |  3000/ 5000 batches | lr 0.90 | ms/batch 34.26 | loss  0.45 |\n",
      "| epoch   3 |  3200/ 5000 batches | lr 0.90 | ms/batch 34.27 | loss  0.45 |\n",
      "| epoch   3 |  3400/ 5000 batches | lr 0.90 | ms/batch 34.28 | loss  0.45 |\n",
      "| epoch   3 |  3600/ 5000 batches | lr 0.90 | ms/batch 34.27 | loss  0.45 |\n",
      "| epoch   3 |  3800/ 5000 batches | lr 0.90 | ms/batch 34.28 | loss  0.45 |\n",
      "| epoch   3 |  4000/ 5000 batches | lr 0.90 | ms/batch 34.25 | loss  0.45 |\n",
      "| epoch   3 |  4200/ 5000 batches | lr 0.90 | ms/batch 34.24 | loss  0.45 |\n",
      "| epoch   3 |  4400/ 5000 batches | lr 0.90 | ms/batch 34.25 | loss  0.45 |\n",
      "| epoch   3 |  4600/ 5000 batches | lr 0.90 | ms/batch 34.26 | loss  0.45 |\n",
      "| epoch   3 |  4800/ 5000 batches | lr 0.90 | ms/batch 34.25 | loss  0.45 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 252.17s | valid loss  0.65 | valid ppl     1.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5000 batches | lr 0.86 | ms/batch 34.42 | loss  0.43 |\n",
      "| epoch   4 |   400/ 5000 batches | lr 0.86 | ms/batch 34.27 | loss  0.43 |\n",
      "| epoch   4 |   600/ 5000 batches | lr 0.86 | ms/batch 34.26 | loss  0.43 |\n",
      "| epoch   4 |   800/ 5000 batches | lr 0.86 | ms/batch 34.30 | loss  0.43 |\n",
      "| epoch   4 |  1000/ 5000 batches | lr 0.86 | ms/batch 34.28 | loss  0.43 |\n",
      "| epoch   4 |  1200/ 5000 batches | lr 0.86 | ms/batch 34.29 | loss  0.43 |\n",
      "| epoch   4 |  1400/ 5000 batches | lr 0.86 | ms/batch 34.28 | loss  0.43 |\n",
      "| epoch   4 |  1600/ 5000 batches | lr 0.86 | ms/batch 34.27 | loss  0.43 |\n",
      "| epoch   4 |  1800/ 5000 batches | lr 0.86 | ms/batch 34.27 | loss  0.43 |\n",
      "| epoch   4 |  2000/ 5000 batches | lr 0.86 | ms/batch 34.27 | loss  0.43 |\n",
      "| epoch   4 |  2200/ 5000 batches | lr 0.86 | ms/batch 34.26 | loss  0.43 |\n",
      "| epoch   4 |  2400/ 5000 batches | lr 0.86 | ms/batch 34.26 | loss  0.43 |\n",
      "| epoch   4 |  2600/ 5000 batches | lr 0.86 | ms/batch 34.27 | loss  0.43 |\n",
      "| epoch   4 |  2800/ 5000 batches | lr 0.86 | ms/batch 34.27 | loss  0.43 |\n",
      "| epoch   4 |  3000/ 5000 batches | lr 0.86 | ms/batch 34.26 | loss  0.43 |\n",
      "| epoch   4 |  3200/ 5000 batches | lr 0.86 | ms/batch 34.27 | loss  0.43 |\n",
      "| epoch   4 |  3400/ 5000 batches | lr 0.86 | ms/batch 34.27 | loss  0.43 |\n",
      "| epoch   4 |  3600/ 5000 batches | lr 0.86 | ms/batch 34.27 | loss  0.43 |\n",
      "| epoch   4 |  3800/ 5000 batches | lr 0.86 | ms/batch 34.27 | loss  0.43 |\n",
      "| epoch   4 |  4000/ 5000 batches | lr 0.86 | ms/batch 34.26 | loss  0.43 |\n",
      "| epoch   4 |  4200/ 5000 batches | lr 0.86 | ms/batch 34.27 | loss  0.43 |\n",
      "| epoch   4 |  4400/ 5000 batches | lr 0.86 | ms/batch 34.26 | loss  0.43 |\n",
      "| epoch   4 |  4600/ 5000 batches | lr 0.86 | ms/batch 34.29 | loss  0.43 |\n",
      "| epoch   4 |  4800/ 5000 batches | lr 0.86 | ms/batch 34.26 | loss  0.43 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 252.29s | valid loss  0.65 | valid ppl     1.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5000 batches | lr 0.81 | ms/batch 34.45 | loss  0.41 |\n",
      "| epoch   5 |   400/ 5000 batches | lr 0.81 | ms/batch 34.27 | loss  0.41 |\n",
      "| epoch   5 |   600/ 5000 batches | lr 0.81 | ms/batch 34.27 | loss  0.41 |\n",
      "| epoch   5 |   800/ 5000 batches | lr 0.81 | ms/batch 34.27 | loss  0.41 |\n",
      "| epoch   5 |  1000/ 5000 batches | lr 0.81 | ms/batch 34.25 | loss  0.41 |\n",
      "| epoch   5 |  1200/ 5000 batches | lr 0.81 | ms/batch 34.27 | loss  0.41 |\n",
      "| epoch   5 |  1400/ 5000 batches | lr 0.81 | ms/batch 34.27 | loss  0.41 |\n",
      "| epoch   5 |  1600/ 5000 batches | lr 0.81 | ms/batch 34.26 | loss  0.41 |\n",
      "| epoch   5 |  1800/ 5000 batches | lr 0.81 | ms/batch 34.27 | loss  0.41 |\n",
      "| epoch   5 |  2000/ 5000 batches | lr 0.81 | ms/batch 34.28 | loss  0.41 |\n",
      "| epoch   5 |  2200/ 5000 batches | lr 0.81 | ms/batch 34.26 | loss  0.41 |\n",
      "| epoch   5 |  2400/ 5000 batches | lr 0.81 | ms/batch 34.23 | loss  0.41 |\n",
      "| epoch   5 |  2600/ 5000 batches | lr 0.81 | ms/batch 34.24 | loss  0.41 |\n",
      "| epoch   5 |  2800/ 5000 batches | lr 0.81 | ms/batch 34.23 | loss  0.41 |\n",
      "| epoch   5 |  3000/ 5000 batches | lr 0.81 | ms/batch 34.24 | loss  0.41 |\n",
      "| epoch   5 |  3200/ 5000 batches | lr 0.81 | ms/batch 34.26 | loss  0.41 |\n",
      "| epoch   5 |  3400/ 5000 batches | lr 0.81 | ms/batch 34.25 | loss  0.41 |\n",
      "| epoch   5 |  3600/ 5000 batches | lr 0.81 | ms/batch 34.25 | loss  0.41 |\n",
      "| epoch   5 |  3800/ 5000 batches | lr 0.81 | ms/batch 34.26 | loss  0.41 |\n",
      "| epoch   5 |  4000/ 5000 batches | lr 0.81 | ms/batch 34.27 | loss  0.41 |\n",
      "| epoch   5 |  4200/ 5000 batches | lr 0.81 | ms/batch 34.26 | loss  0.41 |\n",
      "| epoch   5 |  4400/ 5000 batches | lr 0.81 | ms/batch 34.27 | loss  0.41 |\n",
      "| epoch   5 |  4600/ 5000 batches | lr 0.81 | ms/batch 34.27 | loss  0.41 |\n",
      "| epoch   5 |  4800/ 5000 batches | lr 0.81 | ms/batch 34.28 | loss  0.41 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 252.22s | valid loss  0.65 | valid ppl     1.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.65 | test ppl     1.92\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "seq_len = 1000\n",
    "divider = 4\n",
    "dataset_len = 100000\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "epochs = 5 # The number of epochs\n",
    "\n",
    "model = train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDhuiN10ZOs1",
    "outputId": "874f9161-b32e-4e45-eb5d-92badf2e3acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: tensor([[0.0000, 0.3971],\n",
      "        [0.0000, 0.7832],\n",
      "        [1.0000, 0.7652],\n",
      "        [0.0000, 0.2995],\n",
      "        [0.0000, 0.5102],\n",
      "        [0.0000, 0.1984],\n",
      "        [0.0000, 0.4456],\n",
      "        [1.0000, 0.4550],\n",
      "        [0.0000, 0.9313],\n",
      "        [1.0000, 0.1801],\n",
      "        [1.0000, 0.1968],\n",
      "        [0.0000, 0.4223],\n",
      "        [0.0000, 0.9183],\n",
      "        [0.0000, 0.0417],\n",
      "        [0.0000, 0.6817],\n",
      "        [0.0000, 0.5202],\n",
      "        [0.0000, 0.8040],\n",
      "        [1.0000, 0.4950],\n",
      "        [0.0000, 0.0670],\n",
      "        [0.0000, 0.5878],\n",
      "        [1.0000, 0.2242],\n",
      "        [0.0000, 0.8064],\n",
      "        [0.0000, 0.7436],\n",
      "        [1.0000, 0.8182],\n",
      "        [0.0000, 0.3291],\n",
      "        [1.0000, 0.3496],\n",
      "        [0.0000, 0.0335],\n",
      "        [1.0000, 0.0215],\n",
      "        [0.0000, 0.1326],\n",
      "        [0.0000, 0.6483],\n",
      "        [0.0000, 0.8870],\n",
      "        [0.0000, 0.6982],\n",
      "        [0.0000, 0.2311],\n",
      "        [1.0000, 0.7782],\n",
      "        [1.0000, 0.2888],\n",
      "        [0.0000, 0.1660],\n",
      "        [0.0000, 0.1541],\n",
      "        [0.0000, 0.9906],\n",
      "        [0.0000, 0.3401],\n",
      "        [0.0000, 0.9369],\n",
      "        [1.0000, 0.6185],\n",
      "        [0.0000, 0.2701],\n",
      "        [1.0000, 0.0662],\n",
      "        [1.0000, 0.3710],\n",
      "        [0.0000, 0.4112],\n",
      "        [1.0000, 0.2546],\n",
      "        [0.0000, 0.8135],\n",
      "        [0.0000, 0.6596],\n",
      "        [0.0000, 0.6201],\n",
      "        [0.0000, 0.8674]], device='cuda:0')\n",
      "output: tensor([43.1101, 43.1101, 43.1103, 43.1101, 43.1101, 43.1101, 43.1101, 43.1101,\n",
      "        43.1101, 43.1101], device='cuda:0')\n",
      "targets: tensor([128.0948, 122.4410, 124.1064, 128.2139, 118.4931, 118.2855, 124.2150,\n",
      "        128.9219, 122.3726, 129.6235], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = get_batch(eval_batch_size, seq_len, divider).to(device)\n",
    "    targets = data.prod(-1).sum(-1).squeeze(-1)\n",
    "    output = model(data)\n",
    "    print('data[0]:', data[0,:50,:])\n",
    "\n",
    "    print('output:', output)\n",
    "    print('targets:', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJHfbyzxZOs2"
   },
   "source": [
    "### Smart pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lc0QH-OqZOs2",
    "outputId": "7dd89e37-4cc4-4c75-d2d5-58f0d5034ec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoXTimes(\n",
      "  (model): Smartpool(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (4): Dropout(p=0.1, inplace=False)\n",
      "      (5): GELU()\n",
      "      (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (7): Dropout(p=0.1, inplace=False)\n",
      "      (8): GELU()\n",
      "      (9): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (10): Sigmoid()\n",
      "    )\n",
      "    (mlp2): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (4): Dropout(p=0.1, inplace=False)\n",
      "      (5): GELU()\n",
      "      (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (7): Dropout(p=0.1, inplace=False)\n",
      "      (8): GELU()\n",
      "      (9): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| epoch   1 |   200/ 5000 batches | lr 1.00 | ms/batch 96.44 | loss  0.51 |\n",
      "| epoch   1 |   400/ 5000 batches | lr 1.00 | ms/batch 95.86 | loss  0.50 |\n",
      "| epoch   1 |   600/ 5000 batches | lr 1.00 | ms/batch 95.84 | loss  0.50 |\n",
      "| epoch   1 |   800/ 5000 batches | lr 1.00 | ms/batch 95.84 | loss  0.50 |\n",
      "| epoch   1 |  1000/ 5000 batches | lr 1.00 | ms/batch 95.85 | loss  0.50 |\n",
      "| epoch   1 |  1200/ 5000 batches | lr 1.00 | ms/batch 95.85 | loss  0.50 |\n",
      "| epoch   1 |  1400/ 5000 batches | lr 1.00 | ms/batch 99.00 | loss  0.50 |\n",
      "| epoch   1 |  1600/ 5000 batches | lr 1.00 | ms/batch 118.93 | loss  0.50 |\n",
      "| epoch   1 |  1800/ 5000 batches | lr 1.00 | ms/batch 97.01 | loss  0.50 |\n",
      "| epoch   1 |  2000/ 5000 batches | lr 1.00 | ms/batch 96.17 | loss  0.50 |\n",
      "| epoch   1 |  2200/ 5000 batches | lr 1.00 | ms/batch 96.17 | loss  0.50 |\n",
      "| epoch   1 |  2400/ 5000 batches | lr 1.00 | ms/batch 96.18 | loss  0.50 |\n",
      "| epoch   1 |  2600/ 5000 batches | lr 1.00 | ms/batch 96.22 | loss  0.50 |\n",
      "| epoch   1 |  2800/ 5000 batches | lr 1.00 | ms/batch 96.16 | loss  0.50 |\n",
      "| epoch   1 |  3000/ 5000 batches | lr 1.00 | ms/batch 96.15 | loss  0.50 |\n",
      "| epoch   1 |  3200/ 5000 batches | lr 1.00 | ms/batch 96.16 | loss  0.50 |\n",
      "| epoch   1 |  3400/ 5000 batches | lr 1.00 | ms/batch 96.13 | loss  0.50 |\n",
      "| epoch   1 |  3600/ 5000 batches | lr 1.00 | ms/batch 96.14 | loss  0.50 |\n",
      "| epoch   1 |  3800/ 5000 batches | lr 1.00 | ms/batch 96.14 | loss  0.50 |\n",
      "| epoch   1 |  4000/ 5000 batches | lr 1.00 | ms/batch 96.19 | loss  0.50 |\n",
      "| epoch   1 |  4200/ 5000 batches | lr 1.00 | ms/batch 96.12 | loss  0.50 |\n",
      "| epoch   1 |  4400/ 5000 batches | lr 1.00 | ms/batch 96.16 | loss  0.50 |\n",
      "| epoch   1 |  4600/ 5000 batches | lr 1.00 | ms/batch 96.13 | loss  0.50 |\n",
      "| epoch   1 |  4800/ 5000 batches | lr 1.00 | ms/batch 96.16 | loss  0.50 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 672.38s | valid loss  0.75 | valid ppl     2.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5000 batches | lr 0.95 | ms/batch 96.66 | loss  0.48 |\n",
      "| epoch   2 |   400/ 5000 batches | lr 0.95 | ms/batch 96.14 | loss  0.47 |\n",
      "| epoch   2 |   600/ 5000 batches | lr 0.95 | ms/batch 96.16 | loss  0.48 |\n",
      "| epoch   2 |   800/ 5000 batches | lr 0.95 | ms/batch 96.18 | loss  0.48 |\n",
      "| epoch   2 |  1000/ 5000 batches | lr 0.95 | ms/batch 96.17 | loss  0.48 |\n",
      "| epoch   2 |  1200/ 5000 batches | lr 0.95 | ms/batch 96.15 | loss  0.48 |\n",
      "| epoch   2 |  1400/ 5000 batches | lr 0.95 | ms/batch 96.16 | loss  0.47 |\n",
      "| epoch   2 |  1600/ 5000 batches | lr 0.95 | ms/batch 96.14 | loss  0.48 |\n",
      "| epoch   2 |  1800/ 5000 batches | lr 0.95 | ms/batch 96.18 | loss  0.48 |\n",
      "| epoch   2 |  2000/ 5000 batches | lr 0.95 | ms/batch 96.22 | loss  0.48 |\n",
      "| epoch   2 |  2200/ 5000 batches | lr 0.95 | ms/batch 96.18 | loss  0.47 |\n",
      "| epoch   2 |  2400/ 5000 batches | lr 0.95 | ms/batch 96.15 | loss  0.48 |\n",
      "| epoch   2 |  2600/ 5000 batches | lr 0.95 | ms/batch 96.18 | loss  0.48 |\n",
      "| epoch   2 |  2800/ 5000 batches | lr 0.95 | ms/batch 96.14 | loss  0.48 |\n",
      "| epoch   2 |  3000/ 5000 batches | lr 0.95 | ms/batch 96.16 | loss  0.48 |\n",
      "| epoch   2 |  3200/ 5000 batches | lr 0.95 | ms/batch 96.22 | loss  0.47 |\n",
      "| epoch   2 |  3400/ 5000 batches | lr 0.95 | ms/batch 96.15 | loss  0.48 |\n",
      "| epoch   2 |  3600/ 5000 batches | lr 0.95 | ms/batch 96.15 | loss  0.48 |\n",
      "| epoch   2 |  3800/ 5000 batches | lr 0.95 | ms/batch 96.17 | loss  0.48 |\n",
      "| epoch   2 |  4000/ 5000 batches | lr 0.95 | ms/batch 96.14 | loss  0.48 |\n",
      "| epoch   2 |  4200/ 5000 batches | lr 0.95 | ms/batch 96.17 | loss  0.48 |\n",
      "| epoch   2 |  4400/ 5000 batches | lr 0.95 | ms/batch 96.18 | loss  0.48 |\n",
      "| epoch   2 |  4600/ 5000 batches | lr 0.95 | ms/batch 96.19 | loss  0.48 |\n",
      "| epoch   2 |  4800/ 5000 batches | lr 0.95 | ms/batch 96.16 | loss  0.48 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 667.42s | valid loss  0.75 | valid ppl     2.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5000 batches | lr 0.90 | ms/batch 96.67 | loss  0.46 |\n",
      "| epoch   3 |   400/ 5000 batches | lr 0.90 | ms/batch 96.14 | loss  0.45 |\n",
      "| epoch   3 |   600/ 5000 batches | lr 0.90 | ms/batch 96.16 | loss  0.45 |\n",
      "| epoch   3 |   800/ 5000 batches | lr 0.90 | ms/batch 96.13 | loss  0.45 |\n",
      "| epoch   3 |  1000/ 5000 batches | lr 0.90 | ms/batch 96.15 | loss  0.45 |\n",
      "| epoch   3 |  1200/ 5000 batches | lr 0.90 | ms/batch 96.12 | loss  0.45 |\n",
      "| epoch   3 |  1400/ 5000 batches | lr 0.90 | ms/batch 96.15 | loss  0.45 |\n",
      "| epoch   3 |  1600/ 5000 batches | lr 0.90 | ms/batch 96.15 | loss  0.45 |\n",
      "| epoch   3 |  1800/ 5000 batches | lr 0.90 | ms/batch 96.15 | loss  0.45 |\n",
      "| epoch   3 |  2000/ 5000 batches | lr 0.90 | ms/batch 96.15 | loss  0.45 |\n",
      "| epoch   3 |  2200/ 5000 batches | lr 0.90 | ms/batch 96.14 | loss  0.45 |\n",
      "| epoch   3 |  2400/ 5000 batches | lr 0.90 | ms/batch 96.15 | loss  0.45 |\n",
      "| epoch   3 |  2600/ 5000 batches | lr 0.90 | ms/batch 96.17 | loss  0.45 |\n",
      "| epoch   3 |  2800/ 5000 batches | lr 0.90 | ms/batch 96.15 | loss  0.45 |\n",
      "| epoch   3 |  3000/ 5000 batches | lr 0.90 | ms/batch 96.12 | loss  0.45 |\n",
      "| epoch   3 |  3200/ 5000 batches | lr 0.90 | ms/batch 96.14 | loss  0.45 |\n",
      "| epoch   3 |  3400/ 5000 batches | lr 0.90 | ms/batch 96.14 | loss  0.45 |\n",
      "| epoch   3 |  3600/ 5000 batches | lr 0.90 | ms/batch 96.15 | loss  0.45 |\n",
      "| epoch   3 |  3800/ 5000 batches | lr 0.90 | ms/batch 96.17 | loss  0.45 |\n",
      "| epoch   3 |  4000/ 5000 batches | lr 0.90 | ms/batch 98.25 | loss  0.45 |\n",
      "| epoch   3 |  4200/ 5000 batches | lr 0.90 | ms/batch 96.15 | loss  0.45 |\n",
      "| epoch   3 |  4400/ 5000 batches | lr 0.90 | ms/batch 96.15 | loss  0.45 |\n",
      "| epoch   3 |  4600/ 5000 batches | lr 0.90 | ms/batch 96.16 | loss  0.45 |\n",
      "| epoch   3 |  4800/ 5000 batches | lr 0.90 | ms/batch 96.15 | loss  0.45 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 667.73s | valid loss  0.75 | valid ppl     2.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5000 batches | lr 0.86 | ms/batch 96.64 | loss  0.43 |\n",
      "| epoch   4 |   400/ 5000 batches | lr 0.86 | ms/batch 96.15 | loss  0.43 |\n",
      "| epoch   4 |   600/ 5000 batches | lr 0.86 | ms/batch 96.15 | loss  0.43 |\n",
      "| epoch   4 |   800/ 5000 batches | lr 0.86 | ms/batch 96.28 | loss  0.43 |\n",
      "| epoch   4 |  1000/ 5000 batches | lr 0.86 | ms/batch 96.16 | loss  0.43 |\n",
      "| epoch   4 |  1200/ 5000 batches | lr 0.86 | ms/batch 96.15 | loss  0.43 |\n",
      "| epoch   4 |  1400/ 5000 batches | lr 0.86 | ms/batch 96.15 | loss  0.43 |\n",
      "| epoch   4 |  1600/ 5000 batches | lr 0.86 | ms/batch 96.15 | loss  0.43 |\n",
      "| epoch   4 |  1800/ 5000 batches | lr 0.86 | ms/batch 96.16 | loss  0.43 |\n",
      "| epoch   4 |  2000/ 5000 batches | lr 0.86 | ms/batch 96.19 | loss  0.43 |\n",
      "| epoch   4 |  2200/ 5000 batches | lr 0.86 | ms/batch 96.16 | loss  0.43 |\n",
      "| epoch   4 |  2400/ 5000 batches | lr 0.86 | ms/batch 96.16 | loss  0.43 |\n",
      "| epoch   4 |  2600/ 5000 batches | lr 0.86 | ms/batch 96.15 | loss  0.43 |\n",
      "| epoch   4 |  2800/ 5000 batches | lr 0.86 | ms/batch 96.15 | loss  0.43 |\n",
      "| epoch   4 |  3000/ 5000 batches | lr 0.86 | ms/batch 96.18 | loss  0.43 |\n",
      "| epoch   4 |  3200/ 5000 batches | lr 0.86 | ms/batch 96.28 | loss  0.43 |\n",
      "| epoch   4 |  3400/ 5000 batches | lr 0.86 | ms/batch 96.17 | loss  0.43 |\n",
      "| epoch   4 |  3600/ 5000 batches | lr 0.86 | ms/batch 96.16 | loss  0.43 |\n",
      "| epoch   4 |  3800/ 5000 batches | lr 0.86 | ms/batch 96.16 | loss  0.43 |\n",
      "| epoch   4 |  4000/ 5000 batches | lr 0.86 | ms/batch 96.16 | loss  0.43 |\n",
      "| epoch   4 |  4200/ 5000 batches | lr 0.86 | ms/batch 96.16 | loss  0.43 |\n",
      "| epoch   4 |  4400/ 5000 batches | lr 0.86 | ms/batch 96.19 | loss  0.43 |\n",
      "| epoch   4 |  4600/ 5000 batches | lr 0.86 | ms/batch 96.15 | loss  0.43 |\n",
      "| epoch   4 |  4800/ 5000 batches | lr 0.86 | ms/batch 96.14 | loss  0.43 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 667.20s | valid loss  0.75 | valid ppl     2.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5000 batches | lr 0.81 | ms/batch 96.65 | loss  0.41 |\n",
      "| epoch   5 |   400/ 5000 batches | lr 0.81 | ms/batch 96.18 | loss  0.41 |\n",
      "| epoch   5 |   600/ 5000 batches | lr 0.81 | ms/batch 96.15 | loss  0.41 |\n",
      "| epoch   5 |   800/ 5000 batches | lr 0.81 | ms/batch 96.16 | loss  0.41 |\n",
      "| epoch   5 |  1000/ 5000 batches | lr 0.81 | ms/batch 96.17 | loss  0.41 |\n",
      "| epoch   5 |  1200/ 5000 batches | lr 0.81 | ms/batch 96.13 | loss  0.41 |\n",
      "| epoch   5 |  1400/ 5000 batches | lr 0.81 | ms/batch 96.19 | loss  0.41 |\n",
      "| epoch   5 |  1600/ 5000 batches | lr 0.81 | ms/batch 96.13 | loss  0.41 |\n",
      "| epoch   5 |  1800/ 5000 batches | lr 0.81 | ms/batch 96.13 | loss  0.41 |\n",
      "| epoch   5 |  2000/ 5000 batches | lr 0.81 | ms/batch 96.14 | loss  0.41 |\n",
      "| epoch   5 |  2200/ 5000 batches | lr 0.81 | ms/batch 96.16 | loss  0.41 |\n",
      "| epoch   5 |  2400/ 5000 batches | lr 0.81 | ms/batch 96.16 | loss  0.41 |\n",
      "| epoch   5 |  2600/ 5000 batches | lr 0.81 | ms/batch 96.20 | loss  0.41 |\n",
      "| epoch   5 |  2800/ 5000 batches | lr 0.81 | ms/batch 96.16 | loss  0.41 |\n",
      "| epoch   5 |  3000/ 5000 batches | lr 0.81 | ms/batch 96.15 | loss  0.41 |\n",
      "| epoch   5 |  3200/ 5000 batches | lr 0.81 | ms/batch 96.14 | loss  0.41 |\n",
      "| epoch   5 |  3400/ 5000 batches | lr 0.81 | ms/batch 96.15 | loss  0.41 |\n",
      "| epoch   5 |  3600/ 5000 batches | lr 0.81 | ms/batch 96.14 | loss  0.41 |\n",
      "| epoch   5 |  3800/ 5000 batches | lr 0.81 | ms/batch 96.17 | loss  0.41 |\n",
      "| epoch   5 |  4000/ 5000 batches | lr 0.81 | ms/batch 96.15 | loss  0.41 |\n",
      "| epoch   5 |  4200/ 5000 batches | lr 0.81 | ms/batch 96.15 | loss  0.41 |\n",
      "| epoch   5 |  4400/ 5000 batches | lr 0.81 | ms/batch 96.15 | loss  0.41 |\n",
      "| epoch   5 |  4600/ 5000 batches | lr 0.81 | ms/batch 96.17 | loss  0.41 |\n",
      "| epoch   5 |  4800/ 5000 batches | lr 0.81 | ms/batch 96.18 | loss  0.41 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 667.23s | valid loss  0.74 | valid ppl     2.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.74 | test ppl     2.11\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "seq_len = 1000\n",
    "divider = 4\n",
    "dataset_len = 100000\n",
    "epochs = 5 # The number of epochs\n",
    "\n",
    "model = DoXTimes(Smartpool(divider, 0.3, mlp2=True))\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "model = train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_7radp_ZOs2",
    "outputId": "abef3b35-dc08-49f1-fdc7-ea1773f932e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: tensor([[1.0000, 0.6150],\n",
      "        [0.0000, 0.0595],\n",
      "        [0.0000, 0.5246],\n",
      "        ...,\n",
      "        [0.0000, 0.6626],\n",
      "        [0.0000, 0.9729],\n",
      "        [0.0000, 0.3818]], device='cuda:0')\n",
      "output: tensor([31.8880, 31.8882, 31.8881, 31.8885, 31.8886, 31.8884, 31.8888, 31.8887,\n",
      "        31.8880, 31.8886], device='cuda:0')\n",
      "targets: tensor([127.4935, 130.1996, 131.9557, 127.1092, 122.5810, 124.3430, 120.3016,\n",
      "        120.7829, 123.7960, 131.3994], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = get_batch(eval_batch_size, seq_len, divider).to(device)\n",
    "    targets = data.prod(-1).sum(-1).squeeze(-1)\n",
    "    output = model(data)\n",
    "    print('data[0]:', data[0,:,:])\n",
    "\n",
    "    print('output:', output)\n",
    "    print('targets:', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJXTDtqYZhOX"
   },
   "source": [
    "## Pooling T/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qEjuca5ZhOb"
   },
   "source": [
    "### Average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGPv7w9xZhOb",
    "outputId": "f95cd1a3-65f7-493d-b8d4-213854f57e83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/i273233/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): AvgPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): AvgPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): AvgPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): AvgPool2d(kernel_size=2, stride=(2, 1), padding=0)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (linear): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=False)\n",
    "model = model.features[0:19]\n",
    "model[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model[2] = nn.AvgPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "model[5] = nn.AvgPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "model[10] = nn.AvgPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "model[15] = nn.AvgPool2d(kernel_size=2, stride=(2,1), padding=0)\n",
    "model[18] = nn.Conv2d(512, 512, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model.add_module(\"linear\", nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0))\n",
    "model = Conv(model, 1)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "677pIwmSZhOd",
    "outputId": "7e4e2b95-431d-485e-a4d0-5871e89c4916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 5120 batches | lr 1.00 | ms/batch 107.09 | loss  0.50 |\n",
      "| epoch   1 |   400/ 5120 batches | lr 1.00 | ms/batch 107.01 | loss  0.50 |\n",
      "| epoch   1 |   600/ 5120 batches | lr 1.00 | ms/batch 107.42 | loss  0.50 |\n",
      "| epoch   1 |   800/ 5120 batches | lr 1.00 | ms/batch 107.66 | loss  0.50 |\n",
      "| epoch   1 |  1000/ 5120 batches | lr 1.00 | ms/batch 108.56 | loss  0.50 |\n",
      "| epoch   1 |  1200/ 5120 batches | lr 1.00 | ms/batch 108.75 | loss  0.50 |\n",
      "| epoch   1 |  1400/ 5120 batches | lr 1.00 | ms/batch 108.91 | loss  0.50 |\n",
      "| epoch   1 |  1600/ 5120 batches | lr 1.00 | ms/batch 108.88 | loss  0.50 |\n",
      "| epoch   1 |  1800/ 5120 batches | lr 1.00 | ms/batch 109.49 | loss  0.50 |\n",
      "| epoch   1 |  2000/ 5120 batches | lr 1.00 | ms/batch 110.21 | loss  0.50 |\n",
      "| epoch   1 |  2200/ 5120 batches | lr 1.00 | ms/batch 112.75 | loss  0.50 |\n",
      "| epoch   1 |  2400/ 5120 batches | lr 1.00 | ms/batch 112.83 | loss  0.50 |\n",
      "| epoch   1 |  2600/ 5120 batches | lr 1.00 | ms/batch 112.87 | loss  0.50 |\n",
      "| epoch   1 |  2800/ 5120 batches | lr 1.00 | ms/batch 114.30 | loss  0.50 |\n",
      "| epoch   1 |  3000/ 5120 batches | lr 1.00 | ms/batch 115.47 | loss  0.51 |\n",
      "| epoch   1 |  3200/ 5120 batches | lr 1.00 | ms/batch 115.28 | loss  0.50 |\n",
      "| epoch   1 |  3400/ 5120 batches | lr 1.00 | ms/batch 117.82 | loss  0.50 |\n",
      "| epoch   1 |  3600/ 5120 batches | lr 1.00 | ms/batch 116.51 | loss  0.50 |\n",
      "| epoch   1 |  3800/ 5120 batches | lr 1.00 | ms/batch 116.82 | loss  0.50 |\n",
      "| epoch   1 |  4000/ 5120 batches | lr 1.00 | ms/batch 117.91 | loss  0.50 |\n",
      "| epoch   1 |  4200/ 5120 batches | lr 1.00 | ms/batch 118.22 | loss  0.50 |\n",
      "| epoch   1 |  4400/ 5120 batches | lr 1.00 | ms/batch 118.22 | loss  0.50 |\n",
      "| epoch   1 |  4600/ 5120 batches | lr 1.00 | ms/batch 118.04 | loss  0.50 |\n",
      "| epoch   1 |  4800/ 5120 batches | lr 1.00 | ms/batch 118.16 | loss  0.50 |\n",
      "| epoch   1 |  5000/ 5120 batches | lr 1.00 | ms/batch 118.26 | loss  0.50 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 857.10s | valid loss  0.51 | valid ppl     1.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5120 batches | lr 0.95 | ms/batch 119.23 | loss  0.48 |\n",
      "| epoch   2 |   400/ 5120 batches | lr 0.95 | ms/batch 118.77 | loss  0.48 |\n",
      "| epoch   2 |   600/ 5120 batches | lr 0.95 | ms/batch 118.79 | loss  0.48 |\n",
      "| epoch   2 |   800/ 5120 batches | lr 0.95 | ms/batch 118.61 | loss  0.48 |\n",
      "| epoch   2 |  1000/ 5120 batches | lr 0.95 | ms/batch 118.79 | loss  0.48 |\n",
      "| epoch   2 |  1200/ 5120 batches | lr 0.95 | ms/batch 118.53 | loss  0.48 |\n",
      "| epoch   2 |  1400/ 5120 batches | lr 0.95 | ms/batch 118.51 | loss  0.48 |\n",
      "| epoch   2 |  1600/ 5120 batches | lr 0.95 | ms/batch 118.43 | loss  0.47 |\n",
      "| epoch   2 |  1800/ 5120 batches | lr 0.95 | ms/batch 118.51 | loss  0.48 |\n",
      "| epoch   2 |  2000/ 5120 batches | lr 0.95 | ms/batch 118.52 | loss  0.48 |\n",
      "| epoch   2 |  2200/ 5120 batches | lr 0.95 | ms/batch 118.71 | loss  0.48 |\n",
      "| epoch   2 |  2400/ 5120 batches | lr 0.95 | ms/batch 118.67 | loss  0.48 |\n",
      "| epoch   2 |  2600/ 5120 batches | lr 0.95 | ms/batch 118.88 | loss  0.48 |\n",
      "| epoch   2 |  2800/ 5120 batches | lr 0.95 | ms/batch 118.72 | loss  0.48 |\n",
      "| epoch   2 |  3000/ 5120 batches | lr 0.95 | ms/batch 118.53 | loss  0.48 |\n",
      "| epoch   2 |  3200/ 5120 batches | lr 0.95 | ms/batch 118.48 | loss  0.48 |\n",
      "| epoch   2 |  3400/ 5120 batches | lr 0.95 | ms/batch 118.22 | loss  0.48 |\n",
      "| epoch   2 |  3600/ 5120 batches | lr 0.95 | ms/batch 118.43 | loss  0.48 |\n",
      "| epoch   2 |  3800/ 5120 batches | lr 0.95 | ms/batch 118.34 | loss  0.48 |\n",
      "| epoch   2 |  4000/ 5120 batches | lr 0.95 | ms/batch 118.53 | loss  0.48 |\n",
      "| epoch   2 |  4200/ 5120 batches | lr 0.95 | ms/batch 118.51 | loss  0.48 |\n",
      "| epoch   2 |  4400/ 5120 batches | lr 0.95 | ms/batch 118.36 | loss  0.48 |\n",
      "| epoch   2 |  4600/ 5120 batches | lr 0.95 | ms/batch 118.65 | loss  0.48 |\n",
      "| epoch   2 |  4800/ 5120 batches | lr 0.95 | ms/batch 118.71 | loss  0.48 |\n",
      "| epoch   2 |  5000/ 5120 batches | lr 0.95 | ms/batch 118.56 | loss  0.48 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 885.47s | valid loss  0.51 | valid ppl     1.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5120 batches | lr 0.90 | ms/batch 119.37 | loss  0.46 |\n",
      "| epoch   3 |   400/ 5120 batches | lr 0.90 | ms/batch 118.78 | loss  0.46 |\n",
      "| epoch   3 |   600/ 5120 batches | lr 0.90 | ms/batch 118.66 | loss  0.45 |\n",
      "| epoch   3 |   800/ 5120 batches | lr 0.90 | ms/batch 118.61 | loss  0.45 |\n",
      "| epoch   3 |  1000/ 5120 batches | lr 0.90 | ms/batch 118.54 | loss  0.45 |\n",
      "| epoch   3 |  1200/ 5120 batches | lr 0.90 | ms/batch 118.54 | loss  0.45 |\n",
      "| epoch   3 |  1400/ 5120 batches | lr 0.90 | ms/batch 118.66 | loss  0.45 |\n",
      "| epoch   3 |  1600/ 5120 batches | lr 0.90 | ms/batch 118.53 | loss  0.45 |\n",
      "| epoch   3 |  1800/ 5120 batches | lr 0.90 | ms/batch 118.50 | loss  0.45 |\n",
      "| epoch   3 |  2000/ 5120 batches | lr 0.90 | ms/batch 118.69 | loss  0.46 |\n",
      "| epoch   3 |  2200/ 5120 batches | lr 0.90 | ms/batch 118.52 | loss  0.45 |\n",
      "| epoch   3 |  2400/ 5120 batches | lr 0.90 | ms/batch 118.32 | loss  0.45 |\n",
      "| epoch   3 |  2600/ 5120 batches | lr 0.90 | ms/batch 118.62 | loss  0.45 |\n",
      "| epoch   3 |  2800/ 5120 batches | lr 0.90 | ms/batch 118.60 | loss  0.45 |\n",
      "| epoch   3 |  3000/ 5120 batches | lr 0.90 | ms/batch 118.65 | loss  0.45 |\n",
      "| epoch   3 |  3200/ 5120 batches | lr 0.90 | ms/batch 118.38 | loss  0.45 |\n",
      "| epoch   3 |  3400/ 5120 batches | lr 0.90 | ms/batch 118.34 | loss  0.45 |\n",
      "| epoch   3 |  3600/ 5120 batches | lr 0.90 | ms/batch 118.42 | loss  0.45 |\n",
      "| epoch   3 |  3800/ 5120 batches | lr 0.90 | ms/batch 118.50 | loss  0.45 |\n",
      "| epoch   3 |  4000/ 5120 batches | lr 0.90 | ms/batch 118.24 | loss  0.45 |\n",
      "| epoch   3 |  4200/ 5120 batches | lr 0.90 | ms/batch 118.38 | loss  0.45 |\n",
      "| epoch   3 |  4400/ 5120 batches | lr 0.90 | ms/batch 118.58 | loss  0.45 |\n",
      "| epoch   3 |  4600/ 5120 batches | lr 0.90 | ms/batch 118.69 | loss  0.46 |\n",
      "| epoch   3 |  4800/ 5120 batches | lr 0.90 | ms/batch 118.53 | loss  0.45 |\n",
      "| epoch   3 |  5000/ 5120 batches | lr 0.90 | ms/batch 118.48 | loss  0.45 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 884.80s | valid loss  0.51 | valid ppl     1.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5120 batches | lr 0.86 | ms/batch 119.03 | loss  0.43 |\n",
      "| epoch   4 |   400/ 5120 batches | lr 0.86 | ms/batch 118.45 | loss  0.43 |\n",
      "| epoch   4 |   600/ 5120 batches | lr 0.86 | ms/batch 118.46 | loss  0.43 |\n",
      "| epoch   4 |   800/ 5120 batches | lr 0.86 | ms/batch 118.38 | loss  0.43 |\n",
      "| epoch   4 |  1000/ 5120 batches | lr 0.86 | ms/batch 118.46 | loss  0.43 |\n",
      "| epoch   4 |  1200/ 5120 batches | lr 0.86 | ms/batch 118.58 | loss  0.43 |\n",
      "| epoch   4 |  1400/ 5120 batches | lr 0.86 | ms/batch 118.53 | loss  0.43 |\n",
      "| epoch   4 |  1600/ 5120 batches | lr 0.86 | ms/batch 118.57 | loss  0.43 |\n",
      "| epoch   4 |  1800/ 5120 batches | lr 0.86 | ms/batch 118.67 | loss  0.43 |\n",
      "| epoch   4 |  2000/ 5120 batches | lr 0.86 | ms/batch 118.74 | loss  0.43 |\n",
      "| epoch   4 |  2200/ 5120 batches | lr 0.86 | ms/batch 118.78 | loss  0.43 |\n",
      "| epoch   4 |  2400/ 5120 batches | lr 0.86 | ms/batch 118.47 | loss  0.43 |\n",
      "| epoch   4 |  2600/ 5120 batches | lr 0.86 | ms/batch 118.41 | loss  0.43 |\n",
      "| epoch   4 |  2800/ 5120 batches | lr 0.86 | ms/batch 118.51 | loss  0.43 |\n",
      "| epoch   4 |  3000/ 5120 batches | lr 0.86 | ms/batch 118.56 | loss  0.43 |\n",
      "| epoch   4 |  3200/ 5120 batches | lr 0.86 | ms/batch 118.53 | loss  0.43 |\n",
      "| epoch   4 |  3400/ 5120 batches | lr 0.86 | ms/batch 118.70 | loss  0.43 |\n",
      "| epoch   4 |  3600/ 5120 batches | lr 0.86 | ms/batch 118.64 | loss  0.43 |\n",
      "| epoch   4 |  3800/ 5120 batches | lr 0.86 | ms/batch 118.43 | loss  0.43 |\n",
      "| epoch   4 |  4000/ 5120 batches | lr 0.86 | ms/batch 118.41 | loss  0.43 |\n",
      "| epoch   4 |  4200/ 5120 batches | lr 0.86 | ms/batch 118.64 | loss  0.43 |\n",
      "| epoch   4 |  4400/ 5120 batches | lr 0.86 | ms/batch 118.80 | loss  0.43 |\n",
      "| epoch   4 |  4600/ 5120 batches | lr 0.86 | ms/batch 118.51 | loss  0.43 |\n",
      "| epoch   4 |  4800/ 5120 batches | lr 0.86 | ms/batch 118.74 | loss  0.43 |\n",
      "| epoch   4 |  5000/ 5120 batches | lr 0.86 | ms/batch 118.73 | loss  0.43 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 886.10s | valid loss  0.51 | valid ppl     1.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5120 batches | lr 0.81 | ms/batch 119.42 | loss  0.41 |\n",
      "| epoch   5 |   400/ 5120 batches | lr 0.81 | ms/batch 118.77 | loss  0.41 |\n",
      "| epoch   5 |   600/ 5120 batches | lr 0.81 | ms/batch 118.63 | loss  0.41 |\n",
      "| epoch   5 |   800/ 5120 batches | lr 0.81 | ms/batch 118.69 | loss  0.41 |\n",
      "| epoch   5 |  1000/ 5120 batches | lr 0.81 | ms/batch 118.79 | loss  0.41 |\n",
      "| epoch   5 |  1200/ 5120 batches | lr 0.81 | ms/batch 118.79 | loss  0.41 |\n",
      "| epoch   5 |  1400/ 5120 batches | lr 0.81 | ms/batch 118.71 | loss  0.41 |\n",
      "| epoch   5 |  1600/ 5120 batches | lr 0.81 | ms/batch 118.76 | loss  0.41 |\n",
      "| epoch   5 |  1800/ 5120 batches | lr 0.81 | ms/batch 118.62 | loss  0.41 |\n",
      "| epoch   5 |  2000/ 5120 batches | lr 0.81 | ms/batch 118.55 | loss  0.41 |\n",
      "| epoch   5 |  2200/ 5120 batches | lr 0.81 | ms/batch 118.81 | loss  0.41 |\n",
      "| epoch   5 |  2400/ 5120 batches | lr 0.81 | ms/batch 118.89 | loss  0.41 |\n",
      "| epoch   5 |  2600/ 5120 batches | lr 0.81 | ms/batch 118.74 | loss  0.41 |\n",
      "| epoch   5 |  2800/ 5120 batches | lr 0.81 | ms/batch 118.62 | loss  0.41 |\n",
      "| epoch   5 |  3000/ 5120 batches | lr 0.81 | ms/batch 118.91 | loss  0.41 |\n",
      "| epoch   5 |  3200/ 5120 batches | lr 0.81 | ms/batch 118.75 | loss  0.41 |\n",
      "| epoch   5 |  3400/ 5120 batches | lr 0.81 | ms/batch 118.80 | loss  0.41 |\n",
      "| epoch   5 |  3600/ 5120 batches | lr 0.81 | ms/batch 118.76 | loss  0.41 |\n",
      "| epoch   5 |  3800/ 5120 batches | lr 0.81 | ms/batch 118.76 | loss  0.41 |\n",
      "| epoch   5 |  4000/ 5120 batches | lr 0.81 | ms/batch 118.58 | loss  0.41 |\n",
      "| epoch   5 |  4200/ 5120 batches | lr 0.81 | ms/batch 118.62 | loss  0.41 |\n",
      "| epoch   5 |  4400/ 5120 batches | lr 0.81 | ms/batch 118.66 | loss  0.41 |\n",
      "| epoch   5 |  4600/ 5120 batches | lr 0.81 | ms/batch 118.81 | loss  0.41 |\n",
      "| epoch   5 |  4800/ 5120 batches | lr 0.81 | ms/batch 118.73 | loss  0.41 |\n",
      "| epoch   5 |  5000/ 5120 batches | lr 0.81 | ms/batch 119.04 | loss  0.41 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 887.23s | valid loss  0.51 | valid ppl     1.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.51 | test ppl     1.66\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "seq_len = 1024\n",
    "divider = 16\n",
    "dataset_len = 102400\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "epochs = 5 # The number of epochs\n",
    "\n",
    "model = train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "SatDK8bcZhOd",
    "outputId": "60567777-3faf-4a53-a229-6ff92fcaf974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: tensor([[0.0000, 0.5810],\n",
      "        [0.0000, 0.6137],\n",
      "        [0.0000, 0.3804],\n",
      "        [0.0000, 0.7532],\n",
      "        [0.0000, 0.5587],\n",
      "        [0.0000, 0.4855],\n",
      "        [0.0000, 0.1757],\n",
      "        [0.0000, 0.1030],\n",
      "        [0.0000, 0.4494],\n",
      "        [0.0000, 0.7263],\n",
      "        [0.0000, 0.5921],\n",
      "        [0.0000, 0.9249],\n",
      "        [1.0000, 0.6204],\n",
      "        [0.0000, 0.8577],\n",
      "        [0.0000, 0.8268],\n",
      "        [0.0000, 0.3762],\n",
      "        [1.0000, 0.6489],\n",
      "        [0.0000, 0.9748],\n",
      "        [0.0000, 0.6496],\n",
      "        [0.0000, 0.5912],\n",
      "        [0.0000, 0.3919],\n",
      "        [0.0000, 0.8667],\n",
      "        [0.0000, 0.4704],\n",
      "        [0.0000, 0.1632],\n",
      "        [0.0000, 0.5355],\n",
      "        [0.0000, 0.1074],\n",
      "        [0.0000, 0.3211],\n",
      "        [0.0000, 0.4606],\n",
      "        [0.0000, 0.3553],\n",
      "        [0.0000, 0.0128],\n",
      "        [0.0000, 0.1509],\n",
      "        [0.0000, 0.0864],\n",
      "        [0.0000, 0.8225],\n",
      "        [0.0000, 0.0753],\n",
      "        [0.0000, 0.4122],\n",
      "        [0.0000, 0.6764],\n",
      "        [0.0000, 0.6808],\n",
      "        [0.0000, 0.7612],\n",
      "        [0.0000, 0.2584],\n",
      "        [0.0000, 0.4378],\n",
      "        [1.0000, 0.3192],\n",
      "        [0.0000, 0.0829],\n",
      "        [0.0000, 0.2837],\n",
      "        [1.0000, 0.2408],\n",
      "        [0.0000, 0.2520],\n",
      "        [0.0000, 0.7281],\n",
      "        [0.0000, 0.4087],\n",
      "        [0.0000, 0.4585],\n",
      "        [0.0000, 0.4028],\n",
      "        [0.0000, 0.5639]], device='cuda:0')\n",
      "output: tensor([15.6888, 15.6888, 15.6888, 15.6888, 15.6888, 15.6888, 15.6888, 15.6888,\n",
      "        15.6888, 15.6888], device='cuda:0')\n",
      "targets: tensor([28.6718, 32.1147, 32.6677, 30.7145, 33.6836, 33.2020, 28.7610, 32.8572,\n",
      "        34.0891, 30.2188], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = get_batch(eval_batch_size, seq_len, divider).to(device)\n",
    "    targets = data.prod(-1).sum(-1).squeeze(-1)\n",
    "    output = model(data)\n",
    "    print('data[0]:', data[0,:50,:])\n",
    "\n",
    "    print('output:', output)\n",
    "    print('targets:', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmTVSH-5ZhOd"
   },
   "source": [
    "### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1nOR_FoZhOd",
    "outputId": "5921fab3-2859-4e86-e49b-b9f3c6a4fe69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/i273233/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (linear): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=False)\n",
    "model = model.features[0:19]\n",
    "model[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model[2] = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "model[5] = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "model[10] = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "model[15] = nn.MaxPool2d(kernel_size=2, stride=(2,1), padding=0)\n",
    "model[18] = nn.Conv2d(512, 512, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "model.add_module(\"linear\", nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0))\n",
    "model = Conv(model, 1)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "icdEBc6sZhOe",
    "outputId": "d612c4e3-7f0f-4bb9-92c5-3204b4a52391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 5120 batches | lr 1.00 | ms/batch 119.06 | loss  0.51 |\n",
      "| epoch   1 |   400/ 5120 batches | lr 1.00 | ms/batch 118.54 | loss  0.50 |\n",
      "| epoch   1 |   600/ 5120 batches | lr 1.00 | ms/batch 118.50 | loss  0.50 |\n",
      "| epoch   1 |   800/ 5120 batches | lr 1.00 | ms/batch 118.57 | loss  0.50 |\n",
      "| epoch   1 |  1000/ 5120 batches | lr 1.00 | ms/batch 118.42 | loss  0.50 |\n",
      "| epoch   1 |  1200/ 5120 batches | lr 1.00 | ms/batch 118.27 | loss  0.50 |\n",
      "| epoch   1 |  1400/ 5120 batches | lr 1.00 | ms/batch 118.39 | loss  0.50 |\n",
      "| epoch   1 |  1600/ 5120 batches | lr 1.00 | ms/batch 118.31 | loss  0.50 |\n",
      "| epoch   1 |  1800/ 5120 batches | lr 1.00 | ms/batch 118.18 | loss  0.50 |\n",
      "| epoch   1 |  2000/ 5120 batches | lr 1.00 | ms/batch 118.23 | loss  0.50 |\n",
      "| epoch   1 |  2200/ 5120 batches | lr 1.00 | ms/batch 118.17 | loss  0.50 |\n",
      "| epoch   1 |  2400/ 5120 batches | lr 1.00 | ms/batch 117.90 | loss  0.50 |\n",
      "| epoch   1 |  2600/ 5120 batches | lr 1.00 | ms/batch 118.12 | loss  0.50 |\n",
      "| epoch   1 |  2800/ 5120 batches | lr 1.00 | ms/batch 118.17 | loss  0.50 |\n",
      "| epoch   1 |  3000/ 5120 batches | lr 1.00 | ms/batch 118.09 | loss  0.50 |\n",
      "| epoch   1 |  3200/ 5120 batches | lr 1.00 | ms/batch 118.21 | loss  0.50 |\n",
      "| epoch   1 |  3400/ 5120 batches | lr 1.00 | ms/batch 118.14 | loss  0.50 |\n",
      "| epoch   1 |  3600/ 5120 batches | lr 1.00 | ms/batch 118.11 | loss  0.50 |\n",
      "| epoch   1 |  3800/ 5120 batches | lr 1.00 | ms/batch 118.27 | loss  0.51 |\n",
      "| epoch   1 |  4000/ 5120 batches | lr 1.00 | ms/batch 118.28 | loss  0.50 |\n",
      "| epoch   1 |  4200/ 5120 batches | lr 1.00 | ms/batch 117.93 | loss  0.50 |\n",
      "| epoch   1 |  4400/ 5120 batches | lr 1.00 | ms/batch 118.03 | loss  0.50 |\n",
      "| epoch   1 |  4600/ 5120 batches | lr 1.00 | ms/batch 118.11 | loss  0.50 |\n",
      "| epoch   1 |  4800/ 5120 batches | lr 1.00 | ms/batch 118.26 | loss  0.50 |\n",
      "| epoch   1 |  5000/ 5120 batches | lr 1.00 | ms/batch 118.22 | loss  0.50 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 883.64s | valid loss  0.46 | valid ppl     1.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5120 batches | lr 0.95 | ms/batch 119.12 | loss  0.48 |\n",
      "| epoch   2 |   400/ 5120 batches | lr 0.95 | ms/batch 118.27 | loss  0.48 |\n",
      "| epoch   2 |   600/ 5120 batches | lr 0.95 | ms/batch 118.02 | loss  0.48 |\n",
      "| epoch   2 |   800/ 5120 batches | lr 0.95 | ms/batch 118.28 | loss  0.48 |\n",
      "| epoch   2 |  1000/ 5120 batches | lr 0.95 | ms/batch 118.25 | loss  0.48 |\n",
      "| epoch   2 |  1200/ 5120 batches | lr 0.95 | ms/batch 118.09 | loss  0.48 |\n",
      "| epoch   2 |  1400/ 5120 batches | lr 0.95 | ms/batch 118.16 | loss  0.48 |\n",
      "| epoch   2 |  1600/ 5120 batches | lr 0.95 | ms/batch 117.99 | loss  0.48 |\n",
      "| epoch   2 |  1800/ 5120 batches | lr 0.95 | ms/batch 118.07 | loss  0.48 |\n",
      "| epoch   2 |  2000/ 5120 batches | lr 0.95 | ms/batch 117.87 | loss  0.48 |\n",
      "| epoch   2 |  2200/ 5120 batches | lr 0.95 | ms/batch 118.04 | loss  0.48 |\n",
      "| epoch   2 |  2400/ 5120 batches | lr 0.95 | ms/batch 117.97 | loss  0.48 |\n",
      "| epoch   2 |  2600/ 5120 batches | lr 0.95 | ms/batch 117.87 | loss  0.48 |\n",
      "| epoch   2 |  2800/ 5120 batches | lr 0.95 | ms/batch 117.95 | loss  0.48 |\n",
      "| epoch   2 |  3000/ 5120 batches | lr 0.95 | ms/batch 118.09 | loss  0.48 |\n",
      "| epoch   2 |  3200/ 5120 batches | lr 0.95 | ms/batch 118.07 | loss  0.48 |\n",
      "| epoch   2 |  3400/ 5120 batches | lr 0.95 | ms/batch 118.23 | loss  0.48 |\n",
      "| epoch   2 |  3600/ 5120 batches | lr 0.95 | ms/batch 118.08 | loss  0.48 |\n",
      "| epoch   2 |  3800/ 5120 batches | lr 0.95 | ms/batch 117.86 | loss  0.48 |\n",
      "| epoch   2 |  4000/ 5120 batches | lr 0.95 | ms/batch 117.96 | loss  0.48 |\n",
      "| epoch   2 |  4200/ 5120 batches | lr 0.95 | ms/batch 118.09 | loss  0.48 |\n",
      "| epoch   2 |  4400/ 5120 batches | lr 0.95 | ms/batch 118.18 | loss  0.48 |\n",
      "| epoch   2 |  4600/ 5120 batches | lr 0.95 | ms/batch 118.27 | loss  0.48 |\n",
      "| epoch   2 |  4800/ 5120 batches | lr 0.95 | ms/batch 118.21 | loss  0.48 |\n",
      "| epoch   2 |  5000/ 5120 batches | lr 0.95 | ms/batch 118.15 | loss  0.48 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 882.96s | valid loss  0.46 | valid ppl     1.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5120 batches | lr 0.90 | ms/batch 118.66 | loss  0.46 |\n",
      "| epoch   3 |   400/ 5120 batches | lr 0.90 | ms/batch 118.41 | loss  0.45 |\n",
      "| epoch   3 |   600/ 5120 batches | lr 0.90 | ms/batch 118.19 | loss  0.45 |\n",
      "| epoch   3 |   800/ 5120 batches | lr 0.90 | ms/batch 118.07 | loss  0.45 |\n",
      "| epoch   3 |  1000/ 5120 batches | lr 0.90 | ms/batch 118.13 | loss  0.45 |\n",
      "| epoch   3 |  1200/ 5120 batches | lr 0.90 | ms/batch 118.17 | loss  0.45 |\n",
      "| epoch   3 |  1400/ 5120 batches | lr 0.90 | ms/batch 118.07 | loss  0.45 |\n",
      "| epoch   3 |  1600/ 5120 batches | lr 0.90 | ms/batch 118.04 | loss  0.45 |\n",
      "| epoch   3 |  1800/ 5120 batches | lr 0.90 | ms/batch 118.16 | loss  0.45 |\n",
      "| epoch   3 |  2000/ 5120 batches | lr 0.90 | ms/batch 118.39 | loss  0.45 |\n",
      "| epoch   3 |  2200/ 5120 batches | lr 0.90 | ms/batch 118.23 | loss  0.45 |\n",
      "| epoch   3 |  2400/ 5120 batches | lr 0.90 | ms/batch 118.00 | loss  0.45 |\n",
      "| epoch   3 |  2600/ 5120 batches | lr 0.90 | ms/batch 118.03 | loss  0.45 |\n",
      "| epoch   3 |  2800/ 5120 batches | lr 0.90 | ms/batch 117.93 | loss  0.45 |\n",
      "| epoch   3 |  3000/ 5120 batches | lr 0.90 | ms/batch 118.14 | loss  0.45 |\n",
      "| epoch   3 |  3200/ 5120 batches | lr 0.90 | ms/batch 118.14 | loss  0.45 |\n",
      "| epoch   3 |  3400/ 5120 batches | lr 0.90 | ms/batch 118.00 | loss  0.45 |\n",
      "| epoch   3 |  3600/ 5120 batches | lr 0.90 | ms/batch 118.06 | loss  0.45 |\n",
      "| epoch   3 |  3800/ 5120 batches | lr 0.90 | ms/batch 118.18 | loss  0.46 |\n",
      "| epoch   3 |  4000/ 5120 batches | lr 0.90 | ms/batch 118.16 | loss  0.45 |\n",
      "| epoch   3 |  4200/ 5120 batches | lr 0.90 | ms/batch 118.07 | loss  0.45 |\n",
      "| epoch   3 |  4400/ 5120 batches | lr 0.90 | ms/batch 118.23 | loss  0.45 |\n",
      "| epoch   3 |  4600/ 5120 batches | lr 0.90 | ms/batch 118.18 | loss  0.45 |\n",
      "| epoch   3 |  4800/ 5120 batches | lr 0.90 | ms/batch 117.97 | loss  0.45 |\n",
      "| epoch   3 |  5000/ 5120 batches | lr 0.90 | ms/batch 118.21 | loss  0.45 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 883.32s | valid loss  0.46 | valid ppl     1.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5120 batches | lr 0.86 | ms/batch 118.61 | loss  0.43 |\n",
      "| epoch   4 |   400/ 5120 batches | lr 0.86 | ms/batch 117.97 | loss  0.43 |\n",
      "| epoch   4 |   600/ 5120 batches | lr 0.86 | ms/batch 118.21 | loss  0.43 |\n",
      "| epoch   4 |   800/ 5120 batches | lr 0.86 | ms/batch 118.08 | loss  0.43 |\n",
      "| epoch   4 |  1000/ 5120 batches | lr 0.86 | ms/batch 118.13 | loss  0.43 |\n",
      "| epoch   4 |  1200/ 5120 batches | lr 0.86 | ms/batch 118.19 | loss  0.43 |\n",
      "| epoch   4 |  1400/ 5120 batches | lr 0.86 | ms/batch 118.21 | loss  0.43 |\n",
      "| epoch   4 |  1600/ 5120 batches | lr 0.86 | ms/batch 118.33 | loss  0.43 |\n",
      "| epoch   4 |  1800/ 5120 batches | lr 0.86 | ms/batch 118.03 | loss  0.43 |\n",
      "| epoch   4 |  2000/ 5120 batches | lr 0.86 | ms/batch 118.27 | loss  0.43 |\n",
      "| epoch   4 |  2200/ 5120 batches | lr 0.86 | ms/batch 118.20 | loss  0.43 |\n",
      "| epoch   4 |  2400/ 5120 batches | lr 0.86 | ms/batch 118.11 | loss  0.43 |\n",
      "| epoch   4 |  2600/ 5120 batches | lr 0.86 | ms/batch 118.27 | loss  0.43 |\n",
      "| epoch   4 |  2800/ 5120 batches | lr 0.86 | ms/batch 118.42 | loss  0.43 |\n",
      "| epoch   4 |  3000/ 5120 batches | lr 0.86 | ms/batch 118.37 | loss  0.43 |\n",
      "| epoch   4 |  3200/ 5120 batches | lr 0.86 | ms/batch 118.32 | loss  0.43 |\n",
      "| epoch   4 |  3400/ 5120 batches | lr 0.86 | ms/batch 118.25 | loss  0.43 |\n",
      "| epoch   4 |  3600/ 5120 batches | lr 0.86 | ms/batch 118.37 | loss  0.43 |\n",
      "| epoch   4 |  3800/ 5120 batches | lr 0.86 | ms/batch 117.96 | loss  0.43 |\n",
      "| epoch   4 |  4000/ 5120 batches | lr 0.86 | ms/batch 118.10 | loss  0.43 |\n",
      "| epoch   4 |  4200/ 5120 batches | lr 0.86 | ms/batch 118.12 | loss  0.43 |\n",
      "| epoch   4 |  4400/ 5120 batches | lr 0.86 | ms/batch 118.26 | loss  0.43 |\n",
      "| epoch   4 |  4600/ 5120 batches | lr 0.86 | ms/batch 118.14 | loss  0.43 |\n",
      "| epoch   4 |  4800/ 5120 batches | lr 0.86 | ms/batch 118.21 | loss  0.43 |\n",
      "| epoch   4 |  5000/ 5120 batches | lr 0.86 | ms/batch 118.29 | loss  0.43 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 883.62s | valid loss  0.46 | valid ppl     1.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5120 batches | lr 0.81 | ms/batch 118.84 | loss  0.41 |\n",
      "| epoch   5 |   400/ 5120 batches | lr 0.81 | ms/batch 118.39 | loss  0.41 |\n",
      "| epoch   5 |   600/ 5120 batches | lr 0.81 | ms/batch 118.46 | loss  0.41 |\n",
      "| epoch   5 |   800/ 5120 batches | lr 0.81 | ms/batch 118.38 | loss  0.41 |\n",
      "| epoch   5 |  1000/ 5120 batches | lr 0.81 | ms/batch 118.58 | loss  0.41 |\n",
      "| epoch   5 |  1200/ 5120 batches | lr 0.81 | ms/batch 118.28 | loss  0.41 |\n",
      "| epoch   5 |  1400/ 5120 batches | lr 0.81 | ms/batch 118.28 | loss  0.41 |\n",
      "| epoch   5 |  1600/ 5120 batches | lr 0.81 | ms/batch 118.21 | loss  0.41 |\n",
      "| epoch   5 |  1800/ 5120 batches | lr 0.81 | ms/batch 118.27 | loss  0.41 |\n",
      "| epoch   5 |  2000/ 5120 batches | lr 0.81 | ms/batch 118.37 | loss  0.41 |\n",
      "| epoch   5 |  2200/ 5120 batches | lr 0.81 | ms/batch 118.43 | loss  0.41 |\n",
      "| epoch   5 |  2400/ 5120 batches | lr 0.81 | ms/batch 118.16 | loss  0.41 |\n",
      "| epoch   5 |  2600/ 5120 batches | lr 0.81 | ms/batch 118.45 | loss  0.41 |\n",
      "| epoch   5 |  2800/ 5120 batches | lr 0.81 | ms/batch 118.10 | loss  0.41 |\n",
      "| epoch   5 |  3000/ 5120 batches | lr 0.81 | ms/batch 118.38 | loss  0.41 |\n",
      "| epoch   5 |  3200/ 5120 batches | lr 0.81 | ms/batch 118.38 | loss  0.41 |\n",
      "| epoch   5 |  3400/ 5120 batches | lr 0.81 | ms/batch 118.47 | loss  0.41 |\n",
      "| epoch   5 |  3600/ 5120 batches | lr 0.81 | ms/batch 118.34 | loss  0.41 |\n",
      "| epoch   5 |  3800/ 5120 batches | lr 0.81 | ms/batch 117.95 | loss  0.41 |\n",
      "| epoch   5 |  4000/ 5120 batches | lr 0.81 | ms/batch 118.30 | loss  0.41 |\n",
      "| epoch   5 |  4200/ 5120 batches | lr 0.81 | ms/batch 118.27 | loss  0.41 |\n",
      "| epoch   5 |  4400/ 5120 batches | lr 0.81 | ms/batch 118.38 | loss  0.41 |\n",
      "| epoch   5 |  4600/ 5120 batches | lr 0.81 | ms/batch 118.43 | loss  0.41 |\n",
      "| epoch   5 |  4800/ 5120 batches | lr 0.81 | ms/batch 118.35 | loss  0.41 |\n",
      "| epoch   5 |  5000/ 5120 batches | lr 0.81 | ms/batch 118.43 | loss  0.41 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 884.63s | valid loss  0.46 | valid ppl     1.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.46 | test ppl     1.58\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "seq_len = 1024\n",
    "divider = 16\n",
    "dataset_len = 102400\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "epochs = 5 # The number of epochs\n",
    "\n",
    "model = train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPNViB4vZhOe",
    "outputId": "97a978ec-9b4c-4e24-9840-1b8f6e943993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: tensor([[0.0000, 0.7545],\n",
      "        [0.0000, 0.5093],\n",
      "        [0.0000, 0.5365],\n",
      "        [0.0000, 0.0089],\n",
      "        [0.0000, 0.7281],\n",
      "        [0.0000, 0.7589],\n",
      "        [0.0000, 0.5609],\n",
      "        [0.0000, 0.8734],\n",
      "        [0.0000, 0.8879],\n",
      "        [0.0000, 0.9234],\n",
      "        [0.0000, 0.7817],\n",
      "        [0.0000, 0.3907],\n",
      "        [0.0000, 0.0874],\n",
      "        [0.0000, 0.9121],\n",
      "        [0.0000, 0.8126],\n",
      "        [0.0000, 0.0738],\n",
      "        [0.0000, 0.5736],\n",
      "        [0.0000, 0.9918],\n",
      "        [0.0000, 0.4867],\n",
      "        [0.0000, 0.6933],\n",
      "        [0.0000, 0.3557],\n",
      "        [0.0000, 0.4842],\n",
      "        [1.0000, 0.1213],\n",
      "        [0.0000, 0.7083],\n",
      "        [0.0000, 0.1244],\n",
      "        [0.0000, 0.0733],\n",
      "        [0.0000, 0.6778],\n",
      "        [0.0000, 0.9451],\n",
      "        [0.0000, 0.4623],\n",
      "        [0.0000, 0.6713],\n",
      "        [0.0000, 0.4773],\n",
      "        [0.0000, 0.9635],\n",
      "        [0.0000, 0.8699],\n",
      "        [0.0000, 0.2970],\n",
      "        [0.0000, 0.8436],\n",
      "        [0.0000, 0.2610],\n",
      "        [0.0000, 0.7576],\n",
      "        [0.0000, 0.2387],\n",
      "        [1.0000, 0.1822],\n",
      "        [1.0000, 0.6135],\n",
      "        [0.0000, 0.5607],\n",
      "        [0.0000, 0.3519],\n",
      "        [0.0000, 0.6554],\n",
      "        [0.0000, 0.0392],\n",
      "        [0.0000, 0.3510],\n",
      "        [0.0000, 0.1371],\n",
      "        [0.0000, 0.3131],\n",
      "        [0.0000, 0.2484],\n",
      "        [0.0000, 0.1667],\n",
      "        [1.0000, 0.8486]], device='cuda:0')\n",
      "output: tensor([17.2102, 17.2102, 17.2102, 17.2102, 17.2102, 17.2102, 17.2102, 17.2102,\n",
      "        17.2102, 17.2102], device='cuda:0')\n",
      "targets: tensor([31.9716, 32.4535, 29.2805, 32.6871, 31.3211, 34.2413, 32.7659, 30.8108,\n",
      "        29.9659, 33.2249], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = get_batch(eval_batch_size, seq_len, divider).to(device)\n",
    "    targets = data.prod(-1).sum(-1).squeeze(-1)\n",
    "    output = model(data)\n",
    "    print('data[0]:', data[0,:50,:])\n",
    "\n",
    "    print('output:', output)\n",
    "    print('targets:', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJOjI2NaZhOe"
   },
   "source": [
    "### Smart pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFx86pizZhOe",
    "outputId": "8dd123f9-b26c-4964-ffa6-ee33bcf17f20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoXTimes(\n",
      "  (model): Smartpool(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (4): Dropout(p=0.1, inplace=False)\n",
      "      (5): GELU()\n",
      "      (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (7): Dropout(p=0.1, inplace=False)\n",
      "      (8): GELU()\n",
      "      (9): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (10): Sigmoid()\n",
      "    )\n",
      "    (mlp2): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (4): Dropout(p=0.1, inplace=False)\n",
      "      (5): GELU()\n",
      "      (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (7): Dropout(p=0.1, inplace=False)\n",
      "      (8): GELU()\n",
      "      (9): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| epoch   1 |   200/ 5120 batches | lr 1.00 | ms/batch 96.45 | loss  0.51 |\n",
      "| epoch   1 |   400/ 5120 batches | lr 1.00 | ms/batch 96.00 | loss  0.50 |\n",
      "| epoch   1 |   600/ 5120 batches | lr 1.00 | ms/batch 95.97 | loss  0.50 |\n",
      "| epoch   1 |   800/ 5120 batches | lr 1.00 | ms/batch 95.95 | loss  0.50 |\n",
      "| epoch   1 |  1000/ 5120 batches | lr 1.00 | ms/batch 95.94 | loss  0.50 |\n",
      "| epoch   1 |  1200/ 5120 batches | lr 1.00 | ms/batch 95.98 | loss  0.50 |\n",
      "| epoch   1 |  1400/ 5120 batches | lr 1.00 | ms/batch 95.97 | loss  0.50 |\n",
      "| epoch   1 |  1600/ 5120 batches | lr 1.00 | ms/batch 95.98 | loss  0.50 |\n",
      "| epoch   1 |  1800/ 5120 batches | lr 1.00 | ms/batch 95.97 | loss  0.50 |\n",
      "| epoch   1 |  2000/ 5120 batches | lr 1.00 | ms/batch 95.96 | loss  0.50 |\n",
      "| epoch   1 |  2200/ 5120 batches | lr 1.00 | ms/batch 95.97 | loss  0.50 |\n",
      "| epoch   1 |  2400/ 5120 batches | lr 1.00 | ms/batch 95.96 | loss  0.50 |\n",
      "| epoch   1 |  2600/ 5120 batches | lr 1.00 | ms/batch 95.96 | loss  0.50 |\n",
      "| epoch   1 |  2800/ 5120 batches | lr 1.00 | ms/batch 95.96 | loss  0.50 |\n",
      "| epoch   1 |  3000/ 5120 batches | lr 1.00 | ms/batch 95.96 | loss  0.50 |\n",
      "| epoch   1 |  3200/ 5120 batches | lr 1.00 | ms/batch 95.94 | loss  0.50 |\n",
      "| epoch   1 |  3400/ 5120 batches | lr 1.00 | ms/batch 95.95 | loss  0.50 |\n",
      "| epoch   1 |  3600/ 5120 batches | lr 1.00 | ms/batch 98.30 | loss  0.50 |\n",
      "| epoch   1 |  3800/ 5120 batches | lr 1.00 | ms/batch 96.29 | loss  0.50 |\n",
      "| epoch   1 |  4000/ 5120 batches | lr 1.00 | ms/batch 96.27 | loss  0.50 |\n",
      "| epoch   1 |  4200/ 5120 batches | lr 1.00 | ms/batch 96.30 | loss  0.50 |\n",
      "| epoch   1 |  4400/ 5120 batches | lr 1.00 | ms/batch 97.40 | loss  0.50 |\n",
      "| epoch   1 |  4600/ 5120 batches | lr 1.00 | ms/batch 94.25 | loss  0.50 |\n",
      "| epoch   1 |  4800/ 5120 batches | lr 1.00 | ms/batch 94.28 | loss  0.50 |\n",
      "| epoch   1 |  5000/ 5120 batches | lr 1.00 | ms/batch 94.29 | loss  0.50 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 677.20s | valid loss  0.78 | valid ppl     2.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5120 batches | lr 0.95 | ms/batch 94.73 | loss  0.48 |\n",
      "| epoch   2 |   400/ 5120 batches | lr 0.95 | ms/batch 94.26 | loss  0.48 |\n",
      "| epoch   2 |   600/ 5120 batches | lr 0.95 | ms/batch 94.28 | loss  0.48 |\n",
      "| epoch   2 |   800/ 5120 batches | lr 0.95 | ms/batch 94.27 | loss  0.48 |\n",
      "| epoch   2 |  1000/ 5120 batches | lr 0.95 | ms/batch 94.28 | loss  0.48 |\n",
      "| epoch   2 |  1200/ 5120 batches | lr 0.95 | ms/batch 94.26 | loss  0.48 |\n",
      "| epoch   2 |  1400/ 5120 batches | lr 0.95 | ms/batch 94.27 | loss  0.48 |\n",
      "| epoch   2 |  1600/ 5120 batches | lr 0.95 | ms/batch 94.25 | loss  0.48 |\n",
      "| epoch   2 |  1800/ 5120 batches | lr 0.95 | ms/batch 94.26 | loss  0.48 |\n",
      "| epoch   2 |  2000/ 5120 batches | lr 0.95 | ms/batch 94.26 | loss  0.48 |\n",
      "| epoch   2 |  2200/ 5120 batches | lr 0.95 | ms/batch 94.27 | loss  0.48 |\n",
      "| epoch   2 |  2400/ 5120 batches | lr 0.95 | ms/batch 94.27 | loss  0.48 |\n",
      "| epoch   2 |  2600/ 5120 batches | lr 0.95 | ms/batch 94.26 | loss  0.48 |\n",
      "| epoch   2 |  2800/ 5120 batches | lr 0.95 | ms/batch 94.26 | loss  0.48 |\n",
      "| epoch   2 |  3000/ 5120 batches | lr 0.95 | ms/batch 94.27 | loss  0.48 |\n",
      "| epoch   2 |  3200/ 5120 batches | lr 0.95 | ms/batch 94.26 | loss  0.48 |\n",
      "| epoch   2 |  3400/ 5120 batches | lr 0.95 | ms/batch 94.27 | loss  0.48 |\n",
      "| epoch   2 |  3600/ 5120 batches | lr 0.95 | ms/batch 94.28 | loss  0.48 |\n",
      "| epoch   2 |  3800/ 5120 batches | lr 0.95 | ms/batch 94.27 | loss  0.48 |\n",
      "| epoch   2 |  4000/ 5120 batches | lr 0.95 | ms/batch 94.25 | loss  0.48 |\n",
      "| epoch   2 |  4200/ 5120 batches | lr 0.95 | ms/batch 94.25 | loss  0.48 |\n",
      "| epoch   2 |  4400/ 5120 batches | lr 0.95 | ms/batch 94.25 | loss  0.48 |\n",
      "| epoch   2 |  4600/ 5120 batches | lr 0.95 | ms/batch 94.29 | loss  0.48 |\n",
      "| epoch   2 |  4800/ 5120 batches | lr 0.95 | ms/batch 94.26 | loss  0.48 |\n",
      "| epoch   2 |  5000/ 5120 batches | lr 0.95 | ms/batch 94.27 | loss  0.48 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 668.86s | valid loss  0.78 | valid ppl     2.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5120 batches | lr 0.90 | ms/batch 94.74 | loss  0.46 |\n",
      "| epoch   3 |   400/ 5120 batches | lr 0.90 | ms/batch 94.27 | loss  0.46 |\n",
      "| epoch   3 |   600/ 5120 batches | lr 0.90 | ms/batch 94.26 | loss  0.46 |\n",
      "| epoch   3 |   800/ 5120 batches | lr 0.90 | ms/batch 94.26 | loss  0.45 |\n",
      "| epoch   3 |  1000/ 5120 batches | lr 0.90 | ms/batch 94.26 | loss  0.46 |\n",
      "| epoch   3 |  1200/ 5120 batches | lr 0.90 | ms/batch 94.26 | loss  0.46 |\n",
      "| epoch   3 |  1400/ 5120 batches | lr 0.90 | ms/batch 94.25 | loss  0.46 |\n",
      "| epoch   3 |  1600/ 5120 batches | lr 0.90 | ms/batch 94.25 | loss  0.46 |\n",
      "| epoch   3 |  1800/ 5120 batches | lr 0.90 | ms/batch 94.25 | loss  0.45 |\n",
      "| epoch   3 |  2000/ 5120 batches | lr 0.90 | ms/batch 94.28 | loss  0.45 |\n",
      "| epoch   3 |  2200/ 5120 batches | lr 0.90 | ms/batch 94.25 | loss  0.45 |\n",
      "| epoch   3 |  2400/ 5120 batches | lr 0.90 | ms/batch 94.23 | loss  0.46 |\n",
      "| epoch   3 |  2600/ 5120 batches | lr 0.90 | ms/batch 94.22 | loss  0.46 |\n",
      "| epoch   3 |  2800/ 5120 batches | lr 0.90 | ms/batch 94.24 | loss  0.45 |\n",
      "| epoch   3 |  3000/ 5120 batches | lr 0.90 | ms/batch 94.22 | loss  0.45 |\n",
      "| epoch   3 |  3200/ 5120 batches | lr 0.90 | ms/batch 94.25 | loss  0.46 |\n",
      "| epoch   3 |  3400/ 5120 batches | lr 0.90 | ms/batch 94.24 | loss  0.45 |\n",
      "| epoch   3 |  3600/ 5120 batches | lr 0.90 | ms/batch 94.22 | loss  0.45 |\n",
      "| epoch   3 |  3800/ 5120 batches | lr 0.90 | ms/batch 94.20 | loss  0.46 |\n",
      "| epoch   3 |  4000/ 5120 batches | lr 0.90 | ms/batch 94.24 | loss  0.46 |\n",
      "| epoch   3 |  4200/ 5120 batches | lr 0.90 | ms/batch 94.24 | loss  0.45 |\n",
      "| epoch   3 |  4400/ 5120 batches | lr 0.90 | ms/batch 94.24 | loss  0.46 |\n",
      "| epoch   3 |  4600/ 5120 batches | lr 0.90 | ms/batch 94.25 | loss  0.45 |\n",
      "| epoch   3 |  4800/ 5120 batches | lr 0.90 | ms/batch 94.23 | loss  0.45 |\n",
      "| epoch   3 |  5000/ 5120 batches | lr 0.90 | ms/batch 94.24 | loss  0.46 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 668.51s | valid loss  0.77 | valid ppl     2.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5120 batches | lr 0.86 | ms/batch 94.69 | loss  0.44 |\n",
      "| epoch   4 |   400/ 5120 batches | lr 0.86 | ms/batch 94.19 | loss  0.44 |\n",
      "| epoch   4 |   600/ 5120 batches | lr 0.86 | ms/batch 94.23 | loss  0.44 |\n",
      "| epoch   4 |   800/ 5120 batches | lr 0.86 | ms/batch 94.23 | loss  0.43 |\n",
      "| epoch   4 |  1000/ 5120 batches | lr 0.86 | ms/batch 94.20 | loss  0.44 |\n",
      "| epoch   4 |  1200/ 5120 batches | lr 0.86 | ms/batch 94.21 | loss  0.44 |\n",
      "| epoch   4 |  1400/ 5120 batches | lr 0.86 | ms/batch 94.24 | loss  0.43 |\n",
      "| epoch   4 |  1600/ 5120 batches | lr 0.86 | ms/batch 94.23 | loss  0.43 |\n",
      "| epoch   4 |  1800/ 5120 batches | lr 0.86 | ms/batch 94.23 | loss  0.43 |\n",
      "| epoch   4 |  2000/ 5120 batches | lr 0.86 | ms/batch 94.21 | loss  0.43 |\n",
      "| epoch   4 |  2200/ 5120 batches | lr 0.86 | ms/batch 94.23 | loss  0.43 |\n",
      "| epoch   4 |  2400/ 5120 batches | lr 0.86 | ms/batch 94.21 | loss  0.43 |\n",
      "| epoch   4 |  2600/ 5120 batches | lr 0.86 | ms/batch 94.24 | loss  0.44 |\n",
      "| epoch   4 |  2800/ 5120 batches | lr 0.86 | ms/batch 94.21 | loss  0.44 |\n",
      "| epoch   4 |  3000/ 5120 batches | lr 0.86 | ms/batch 94.23 | loss  0.43 |\n",
      "| epoch   4 |  3200/ 5120 batches | lr 0.86 | ms/batch 94.24 | loss  0.43 |\n",
      "| epoch   4 |  3400/ 5120 batches | lr 0.86 | ms/batch 94.23 | loss  0.43 |\n",
      "| epoch   4 |  3600/ 5120 batches | lr 0.86 | ms/batch 94.23 | loss  0.43 |\n",
      "| epoch   4 |  3800/ 5120 batches | lr 0.86 | ms/batch 94.28 | loss  0.43 |\n",
      "| epoch   4 |  4000/ 5120 batches | lr 0.86 | ms/batch 94.23 | loss  0.43 |\n",
      "| epoch   4 |  4200/ 5120 batches | lr 0.86 | ms/batch 94.25 | loss  0.43 |\n",
      "| epoch   4 |  4400/ 5120 batches | lr 0.86 | ms/batch 94.26 | loss  0.43 |\n",
      "| epoch   4 |  4600/ 5120 batches | lr 0.86 | ms/batch 94.24 | loss  0.43 |\n",
      "| epoch   4 |  4800/ 5120 batches | lr 0.86 | ms/batch 94.31 | loss  0.44 |\n",
      "| epoch   4 |  5000/ 5120 batches | lr 0.86 | ms/batch 94.23 | loss  0.43 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 668.65s | valid loss  0.76 | valid ppl     2.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5120 batches | lr 0.81 | ms/batch 94.70 | loss  0.41 |\n",
      "| epoch   5 |   400/ 5120 batches | lr 0.81 | ms/batch 94.23 | loss  0.41 |\n",
      "| epoch   5 |   600/ 5120 batches | lr 0.81 | ms/batch 94.22 | loss  0.41 |\n",
      "| epoch   5 |   800/ 5120 batches | lr 0.81 | ms/batch 94.22 | loss  0.41 |\n",
      "| epoch   5 |  1000/ 5120 batches | lr 0.81 | ms/batch 94.22 | loss  0.41 |\n",
      "| epoch   5 |  1200/ 5120 batches | lr 0.81 | ms/batch 94.22 | loss  0.41 |\n",
      "| epoch   5 |  1400/ 5120 batches | lr 0.81 | ms/batch 94.24 | loss  0.41 |\n",
      "| epoch   5 |  1600/ 5120 batches | lr 0.81 | ms/batch 94.22 | loss  0.41 |\n",
      "| epoch   5 |  1800/ 5120 batches | lr 0.81 | ms/batch 94.22 | loss  0.41 |\n",
      "| epoch   5 |  2000/ 5120 batches | lr 0.81 | ms/batch 94.23 | loss  0.41 |\n",
      "| epoch   5 |  2200/ 5120 batches | lr 0.81 | ms/batch 94.23 | loss  0.41 |\n",
      "| epoch   5 |  2400/ 5120 batches | lr 0.81 | ms/batch 94.22 | loss  0.41 |\n",
      "| epoch   5 |  2600/ 5120 batches | lr 0.81 | ms/batch 94.21 | loss  0.41 |\n",
      "| epoch   5 |  2800/ 5120 batches | lr 0.81 | ms/batch 94.22 | loss  0.41 |\n",
      "| epoch   5 |  3000/ 5120 batches | lr 0.81 | ms/batch 94.23 | loss  0.41 |\n",
      "| epoch   5 |  3200/ 5120 batches | lr 0.81 | ms/batch 94.22 | loss  0.41 |\n",
      "| epoch   5 |  3400/ 5120 batches | lr 0.81 | ms/batch 94.20 | loss  0.41 |\n",
      "| epoch   5 |  3600/ 5120 batches | lr 0.81 | ms/batch 94.21 | loss  0.41 |\n",
      "| epoch   5 |  3800/ 5120 batches | lr 0.81 | ms/batch 94.22 | loss  0.41 |\n",
      "| epoch   5 |  4000/ 5120 batches | lr 0.81 | ms/batch 94.21 | loss  0.41 |\n",
      "| epoch   5 |  4200/ 5120 batches | lr 0.81 | ms/batch 94.22 | loss  0.41 |\n",
      "| epoch   5 |  4400/ 5120 batches | lr 0.81 | ms/batch 94.25 | loss  0.41 |\n",
      "| epoch   5 |  4600/ 5120 batches | lr 0.81 | ms/batch 94.25 | loss  0.41 |\n",
      "| epoch   5 |  4800/ 5120 batches | lr 0.81 | ms/batch 94.28 | loss  0.41 |\n",
      "| epoch   5 |  5000/ 5120 batches | lr 0.81 | ms/batch 94.26 | loss  0.41 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 668.31s | valid loss  0.15 | valid ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  0.15 | test ppl     1.16\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "seq_len = 1024\n",
    "divider = 16\n",
    "dataset_len = 102400\n",
    "epochs = 5 # The number of epochs\n",
    "\n",
    "model = DoXTimes(Smartpool(divider, 0.3, mlp2=True))\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "model = train_model(model, epochs, batch_size, eval_batch_size, dataset_len, seq_len, divider, optimizer, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shoKXM8zZhOf",
    "outputId": "7e1f1599-be37-4c4f-f8e0-f7b724d652ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: tensor([[0.0000, 0.5378],\n",
      "        [1.0000, 0.5299],\n",
      "        [0.0000, 0.9057],\n",
      "        [0.0000, 0.8706],\n",
      "        [0.0000, 0.1930],\n",
      "        [0.0000, 0.8721],\n",
      "        [0.0000, 0.3009],\n",
      "        [0.0000, 0.1162],\n",
      "        [1.0000, 0.0651],\n",
      "        [0.0000, 0.7684],\n",
      "        [0.0000, 0.4840],\n",
      "        [0.0000, 0.4395],\n",
      "        [0.0000, 0.1227],\n",
      "        [0.0000, 0.9553],\n",
      "        [0.0000, 0.4086],\n",
      "        [0.0000, 0.1031],\n",
      "        [0.0000, 0.8250],\n",
      "        [0.0000, 0.7717],\n",
      "        [0.0000, 0.1606],\n",
      "        [0.0000, 0.0623],\n",
      "        [0.0000, 0.9686],\n",
      "        [0.0000, 0.6312],\n",
      "        [0.0000, 0.4187],\n",
      "        [0.0000, 0.3914],\n",
      "        [0.0000, 0.6275],\n",
      "        [0.0000, 0.7836],\n",
      "        [0.0000, 0.9253],\n",
      "        [1.0000, 0.8567],\n",
      "        [1.0000, 0.4542],\n",
      "        [0.0000, 0.1590],\n",
      "        [0.0000, 0.6848],\n",
      "        [0.0000, 0.5018],\n",
      "        [0.0000, 0.2396],\n",
      "        [0.0000, 0.4275],\n",
      "        [0.0000, 0.4752],\n",
      "        [0.0000, 0.5145],\n",
      "        [0.0000, 0.8207],\n",
      "        [0.0000, 0.0278],\n",
      "        [1.0000, 0.7614],\n",
      "        [0.0000, 0.4437],\n",
      "        [0.0000, 0.1933],\n",
      "        [0.0000, 0.1328],\n",
      "        [0.0000, 0.6986],\n",
      "        [0.0000, 0.9801],\n",
      "        [0.0000, 0.1887],\n",
      "        [0.0000, 0.2022],\n",
      "        [0.0000, 0.4904],\n",
      "        [0.0000, 0.3834],\n",
      "        [0.0000, 0.4684],\n",
      "        [0.0000, 0.9203]], device='cuda:0')\n",
      "output: tensor([27.0208, 27.0208, 27.0208, 27.0208, 27.0208, 27.0207, 27.0209, 27.0208,\n",
      "        27.0208, 27.0209], device='cuda:0')\n",
      "targets: tensor([32.8675, 30.0858, 31.7577, 31.3642, 30.4849, 36.7939, 27.4491, 34.0155,\n",
      "        28.9980, 30.1141], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = get_batch(eval_batch_size, seq_len, divider).to(device)\n",
    "    targets = data.prod(-1).sum(-1).squeeze(-1)\n",
    "    output = model(data)\n",
    "    print('data[0]:', data[0,:50,:])\n",
    "\n",
    "    print('output:', output)\n",
    "    print('targets:', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUFnk6G-JHnu"
   },
   "source": [
    "# MNIST - 2 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "G7IV5OZuJHrM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "GFxPrB8tE7kq"
   },
   "outputs": [],
   "source": [
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=False)\n",
    "# or any of these variants\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11_bn', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg13', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg13_bn', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16_bn', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19_bn', pretrained=True)\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "3SvcSw131eON"
   },
   "outputs": [],
   "source": [
    "class ConvMNIST(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv,\n",
    "        classifier\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = conv\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        #print('after conv:', x.shape)\n",
    "        x = x.squeeze(2).transpose(1,2)\n",
    "        #print('after sq and tr', x.shape)\n",
    "        x = self.classifier(x)\n",
    "        #print('after classification:', x.shape)\n",
    "        x = x.view(x.shape[0] * x.shape[1], -1)\n",
    "        #print('after x view', x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "lpLifQs091Yv"
   },
   "outputs": [],
   "source": [
    "class Smartpool(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        factor,\n",
    "        search_perc,\n",
    "        mlp2=False\n",
    "    ):\n",
    "        \"\"\"Smart pooling algorithm\n",
    "\n",
    "        Args:\n",
    "            factor: factor by which the sequence's length will be reduced\n",
    "            search_perc: percentage of length of sequence after smartpooling to search for border. Ideally the border is located somewhere in +-search_perc\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.search_perc = search_perc\n",
    "        self.factor = factor\n",
    "        self.register_buffer(\"filters\", torch.FloatTensor([[[[-1,1],[1,-1]]]]), persistent=False)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(28, 2048),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, 1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        if mlp2 == True:\n",
    "            self.mlp2 = nn.Sequential(\n",
    "                nn.Linear(2, 256),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(256,512),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(512,256),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(256,1))\n",
    "        else:\n",
    "            self.mlp2 = None\n",
    "\n",
    "    def warp(self, X, new_lens):\n",
    "        new_lens_cs = new_lens.cumsum(1)\n",
    "        # This really searches for the low boundary of each new pixel\n",
    "        pixel_contributions = new_lens_cs.view(1, -1, 1) - torch.arange(torch.round(new_lens_cs[0, -1]).item(), device=X.device).view(1, 1, -1)\n",
    "        pixel_contributions = pixel_contributions.view(X.size(0), X.size(1), pixel_contributions.size(2))\n",
    "        # Zero out the negative contributions, i.e. pixels which come before each row                              \n",
    "        pixel_contributions = torch.max(torch.tensor(0.0, device=X.device), pixel_contributions)       \n",
    "        \n",
    "        # # This contains the cumulated pixel lengths for all pixels in each \n",
    "        # pixel_contributions\n",
    "    \n",
    "        pixel_contributions = pixel_contributions.unsqueeze(1)\n",
    "        interp_weights = F.conv2d(pixel_contributions, self.filters, padding=1)\n",
    "        interp_weights = interp_weights[:,:,:-1,1:] # Removing padding\n",
    "        interp_weights = interp_weights.squeeze(1)\n",
    "\n",
    "        # # Each column corresponds to a new element. Its values are the \n",
    "        # # weights associated with the original data.\n",
    "        # interp_weights\n",
    "\n",
    "        interp_weights = interp_weights.transpose(1, 2)\n",
    "        Xnew = interp_weights @ X\n",
    "        return Xnew, interp_weights\n",
    "\n",
    "    def nonzero_interval_length(self, x, dim):\n",
    "        nonz = (x > 0)\n",
    "        _, low = ((nonz.cumsum(dim) == 1) & nonz).max(dim, keepdim=True)\n",
    "        rev_cumsum = nonz.long().flip(dim).cumsum(dim).flip(dim)\n",
    "        _, high = ((rev_cumsum == 1) & nonz).max(dim, keepdim=True)\n",
    "        \n",
    "        return high - low + 1\n",
    "\n",
    "    def forward(self, features):\n",
    "        B,T,C = features.size()\n",
    "\n",
    "        padding_mask = torch.zeros(B,T, dtype=torch.bool, device=features.device)\n",
    "        padding_per_batch = (padding_mask > 0).sum(1)\n",
    "        total_T = padding_mask.numel() - padding_per_batch.sum()\n",
    "\n",
    "        # MLP test\n",
    "        new_lens = self.mlp(features.view(B*T,C)).view(1,-1)\n",
    "        new_lens = new_lens / new_lens.sum(1, keepdim=True) * (total_T / self.factor) # Reducing the original length T by some factor\n",
    "       \n",
    "        features, interp_weights = self.warp(features, new_lens)\n",
    "        \n",
    "        if self.mlp2 is not None:\n",
    "            features = self.mlp2(features)\n",
    "\n",
    "        return features\n",
    "        \n",
    "\n",
    "class DoXTimes(nn.Module):\n",
    "    def __init__(self, model, classifier):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1,2)\n",
    "        B = x.shape[0]\n",
    "        x = torch.cat([self.model(x[i].unsqueeze(0)) for i in range(B)])\n",
    "        x = self.classifier(x)\n",
    "        x = x.view(B * x.shape[1], -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128\n",
    "digits_per_batch = 2\n",
    "divider = seq_len // digits_per_batch\n",
    "\n",
    "dataset_root = \".\"\n",
    "mnist_mean = 0.1307\n",
    "mnist_std = 0.3081\n",
    "batch_size_train = 32\n",
    "batch_size_test = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(dataset_root, train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(\n",
    "                                 (mnist_mean,), (mnist_std,))\n",
    "                             ])),\n",
    "    batch_size=digits_per_batch * batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(dataset_root, train=False, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(\n",
    "                                 (mnist_mean,), (mnist_std,))\n",
    "                             ])),\n",
    "    batch_size=digits_per_batch * batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "SSUEYv2oBYZG"
   },
   "outputs": [],
   "source": [
    "def get_batch(batch, seq_len, digits_per_batch):\n",
    "    batch_size = batch[0].shape[0] // digits_per_batch\n",
    "    width = batch[0].shape[-1]\n",
    "    data = (torch.zeros(batch_size, batch[0].shape[2], seq_len) - mnist_mean) / mnist_std\n",
    "    choices = torch.multinomial(torch.ones(batch_size, seq_len - (width - 1) * digits_per_batch), digits_per_batch)\n",
    "    choices = choices.sort()[0] + torch.arange(digits_per_batch) * (width - 1)\n",
    "\n",
    "    a = batch[0][torch.arange(batch[0].shape[0]),:,:].view(-1)\n",
    "    b = torch.arange(batch_size).repeat_interleave(digits_per_batch * width * width)\n",
    "    c = torch.arange(width).repeat_interleave(width).repeat(digits_per_batch * batch_size)\n",
    "    d = (torch.arange(width).repeat(digits_per_batch * batch_size * width).view(digits_per_batch * batch_size, width, width) + choices.view(digits_per_batch * batch_size, 1, 1)).view(-1)\n",
    "    data[b,c,d] = a\n",
    "    batch[0] = data\n",
    "    \n",
    "    return data, batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    data, targets = get_batch(batch, seq_len, digits_per_batch)\n",
    "\n",
    "    plt.matshow((data.view(-1, data.shape[-1]) * mnist_std + mnist_mean).numpy())\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "QEN8XMlE9CDm"
   },
   "outputs": [],
   "source": [
    "def train(model, epoch, optimizer, scheduler, loader, seq_len, digits_per_batch):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, data in enumerate(loader):\n",
    "        data, targets = get_batch(data, seq_len, digits_per_batch)\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = F.cross_entropy(output, targets, reduction=\"sum\")\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.5f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(loader), scheduler.get_last_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "AEyFhNHW9EJA"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, seq_len, digits_per_batch):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data, targets = get_batch(data, seq_len, digits_per_batch)\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += F.cross_entropy(output, targets, reduction=\"sum\").item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "-TTTvnM0D9Ru"
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs, train_loader, test_loader, seq_len, digits_per_batch, optimizer, scheduler):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model, epoch, optimizer, scheduler, train_loader, seq_len, digits_per_batch)\n",
    "        val_loss = evaluate(model, test_loader, seq_len, digits_per_batch)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                        val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    test_loss = evaluate(best_model, test_loader, seq_len, digits_per_batch)\n",
    "    print('=' * 89)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "        test_loss, math.exp(test_loss)))\n",
    "    print('=' * 89)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /tmp/i273233/i273233/cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvMNIST(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): AvgPool2d(kernel_size=2, stride=2, padding=(1, 0))\n",
      "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (45): ReLU(inplace=True)\n",
      "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (48): ReLU(inplace=True)\n",
      "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (51): ReLU(inplace=True)\n",
      "    (52): AvgPool2d(kernel_size=(2, 4), stride=(2, 4), padding=0)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1028, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=1028, out_features=1028, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=1028, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19_bn', pretrained=False)\n",
    "#print(model)\n",
    "features = model.features\n",
    "features[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "features[6] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "features[13] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "features[26] = nn.AvgPool2d(kernel_size=2, stride=2, padding=(1,0))\n",
    "features[39] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "features[52] = nn.AvgPool2d(kernel_size=(2,4), stride=(2,4), padding=0)\n",
    "\n",
    "classifier = nn.Sequential(nn.Linear(512, 1028),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(p=0.5, inplace=False),\n",
    "                            nn.Linear(in_features=1028, out_features=1028, bias=True),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(p=0.5, inplace=False),\n",
    "                            nn.Linear(in_features=1028, out_features=10, bias=True))\n",
    "model = ConvMNIST(features, classifier)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/  938 batches | lr 0.00100 | ms/batch 82.41 | loss 88.84 | ppl 381282060058949941703200980720922132480.00\n",
      "| epoch   1 |   400/  938 batches | lr 0.00100 | ms/batch 81.94 | loss 32.74 | ppl 164851370601645.78\n",
      "| epoch   1 |   600/  938 batches | lr 0.00100 | ms/batch 81.93 | loss 19.81 | ppl 402809730.32\n",
      "| epoch   1 |   800/  938 batches | lr 0.00100 | ms/batch 81.92 | loss 14.82 | ppl 2727884.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 81.45s | valid loss 21.68 | valid ppl 2601036891.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/  938 batches | lr 0.00095 | ms/batch 82.29 | loss 11.47 | ppl 95379.60\n",
      "| epoch   2 |   400/  938 batches | lr 0.00095 | ms/batch 81.93 | loss 10.56 | ppl 38512.20\n",
      "| epoch   2 |   600/  938 batches | lr 0.00095 | ms/batch 81.92 | loss  9.77 | ppl 17555.41\n",
      "| epoch   2 |   800/  938 batches | lr 0.00095 | ms/batch 81.92 | loss  8.78 | ppl  6532.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 81.42s | valid loss  8.60 | valid ppl  5404.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/  938 batches | lr 0.00090 | ms/batch 82.31 | loss  7.87 | ppl  2608.24\n",
      "| epoch   3 |   400/  938 batches | lr 0.00090 | ms/batch 81.92 | loss  7.12 | ppl  1236.98\n",
      "| epoch   3 |   600/  938 batches | lr 0.00090 | ms/batch 81.90 | loss  9.25 | ppl 10363.15\n",
      "| epoch   3 |   800/  938 batches | lr 0.00090 | ms/batch 81.88 | loss  7.72 | ppl  2249.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 81.40s | valid loss 10.86 | valid ppl 52265.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/  938 batches | lr 0.00086 | ms/batch 82.14 | loss  7.83 | ppl  2516.25\n",
      "| epoch   4 |   400/  938 batches | lr 0.00086 | ms/batch 81.78 | loss  7.53 | ppl  1865.01\n",
      "| epoch   4 |   600/  938 batches | lr 0.00086 | ms/batch 81.79 | loss  7.53 | ppl  1857.23\n",
      "| epoch   4 |   800/  938 batches | lr 0.00086 | ms/batch 81.63 | loss  6.36 | ppl   579.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 81.25s | valid loss 11.59 | valid ppl 107920.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/  938 batches | lr 0.00081 | ms/batch 81.99 | loss  5.58 | ppl   264.87\n",
      "| epoch   5 |   400/  938 batches | lr 0.00081 | ms/batch 81.45 | loss  6.29 | ppl   536.56\n",
      "| epoch   5 |   600/  938 batches | lr 0.00081 | ms/batch 81.54 | loss  6.26 | ppl   520.94\n",
      "| epoch   5 |   800/  938 batches | lr 0.00081 | ms/batch 81.48 | loss  6.69 | ppl   805.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 81.02s | valid loss  6.10 | valid ppl   445.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/  938 batches | lr 0.00077 | ms/batch 81.74 | loss  5.54 | ppl   254.88\n",
      "| epoch   6 |   400/  938 batches | lr 0.00077 | ms/batch 81.47 | loss  6.28 | ppl   534.77\n",
      "| epoch   6 |   600/  938 batches | lr 0.00077 | ms/batch 81.25 | loss  5.21 | ppl   183.07\n",
      "| epoch   6 |   800/  938 batches | lr 0.00077 | ms/batch 81.25 | loss  5.27 | ppl   195.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 80.90s | valid loss 10.99 | valid ppl 59076.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/  938 batches | lr 0.00074 | ms/batch 81.76 | loss  5.57 | ppl   263.46\n",
      "| epoch   7 |   400/  938 batches | lr 0.00074 | ms/batch 81.17 | loss  4.51 | ppl    91.19\n",
      "| epoch   7 |   600/  938 batches | lr 0.00074 | ms/batch 80.82 | loss  4.11 | ppl    60.75\n",
      "| epoch   7 |   800/  938 batches | lr 0.00074 | ms/batch 80.87 | loss  4.95 | ppl   141.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 80.59s | valid loss  8.33 | valid ppl  4139.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/  938 batches | lr 0.00070 | ms/batch 81.08 | loss  4.07 | ppl    58.43\n",
      "| epoch   8 |   400/  938 batches | lr 0.00070 | ms/batch 80.97 | loss  4.93 | ppl   137.89\n",
      "| epoch   8 |   600/  938 batches | lr 0.00070 | ms/batch 80.86 | loss  4.57 | ppl    96.33\n",
      "| epoch   8 |   800/  938 batches | lr 0.00070 | ms/batch 80.89 | loss  4.66 | ppl   105.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 80.41s | valid loss  7.71 | valid ppl  2229.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/  938 batches | lr 0.00066 | ms/batch 81.02 | loss  3.72 | ppl    41.16\n",
      "| epoch   9 |   400/  938 batches | lr 0.00066 | ms/batch 80.50 | loss  3.41 | ppl    30.28\n",
      "| epoch   9 |   600/  938 batches | lr 0.00066 | ms/batch 80.57 | loss  3.85 | ppl    46.94\n",
      "| epoch   9 |   800/  938 batches | lr 0.00066 | ms/batch 80.73 | loss  4.25 | ppl    70.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 80.23s | valid loss  8.38 | valid ppl  4361.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/  938 batches | lr 0.00063 | ms/batch 81.16 | loss  4.42 | ppl    82.91\n",
      "| epoch  10 |   400/  938 batches | lr 0.00063 | ms/batch 80.69 | loss  3.37 | ppl    29.00\n",
      "| epoch  10 |   600/  938 batches | lr 0.00063 | ms/batch 80.36 | loss  3.87 | ppl    47.80\n",
      "| epoch  10 |   800/  938 batches | lr 0.00063 | ms/batch 80.79 | loss  3.59 | ppl    36.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 80.19s | valid loss  6.83 | valid ppl   927.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/  938 batches | lr 0.00060 | ms/batch 80.59 | loss  3.27 | ppl    26.33\n",
      "| epoch  11 |   400/  938 batches | lr 0.00060 | ms/batch 80.19 | loss  3.27 | ppl    26.38\n",
      "| epoch  11 |   600/  938 batches | lr 0.00060 | ms/batch 80.63 | loss  2.44 | ppl    11.45\n",
      "| epoch  11 |   800/  938 batches | lr 0.00060 | ms/batch 81.59 | loss  3.16 | ppl    23.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 80.34s | valid loss 12.08 | valid ppl 176515.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/  938 batches | lr 0.00057 | ms/batch 81.80 | loss  3.07 | ppl    21.49\n",
      "| epoch  12 |   400/  938 batches | lr 0.00057 | ms/batch 81.40 | loss  2.34 | ppl    10.36\n",
      "| epoch  12 |   600/  938 batches | lr 0.00057 | ms/batch 81.46 | loss  2.71 | ppl    15.08\n",
      "| epoch  12 |   800/  938 batches | lr 0.00057 | ms/batch 81.88 | loss  3.51 | ppl    33.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 81.09s | valid loss  7.28 | valid ppl  1445.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/  938 batches | lr 0.00054 | ms/batch 82.31 | loss  2.22 | ppl     9.21\n",
      "| epoch  13 |   400/  938 batches | lr 0.00054 | ms/batch 80.83 | loss  2.94 | ppl    18.90\n",
      "| epoch  13 |   600/  938 batches | lr 0.00054 | ms/batch 80.23 | loss  2.75 | ppl    15.62\n",
      "| epoch  13 |   800/  938 batches | lr 0.00054 | ms/batch 80.24 | loss  2.40 | ppl    11.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 80.26s | valid loss  6.68 | valid ppl   797.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/  938 batches | lr 0.00051 | ms/batch 80.17 | loss  1.34 | ppl     3.82\n",
      "| epoch  14 |   400/  938 batches | lr 0.00051 | ms/batch 79.96 | loss  2.25 | ppl     9.53\n",
      "| epoch  14 |   600/  938 batches | lr 0.00051 | ms/batch 80.15 | loss  2.65 | ppl    14.10\n",
      "| epoch  14 |   800/  938 batches | lr 0.00051 | ms/batch 80.13 | loss  2.67 | ppl    14.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 79.59s | valid loss  7.33 | valid ppl  1530.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/  938 batches | lr 0.00049 | ms/batch 80.29 | loss  2.07 | ppl     7.89\n",
      "| epoch  15 |   400/  938 batches | lr 0.00049 | ms/batch 79.85 | loss  1.51 | ppl     4.53\n",
      "| epoch  15 |   600/  938 batches | lr 0.00049 | ms/batch 79.95 | loss  2.82 | ppl    16.80\n",
      "| epoch  15 |   800/  938 batches | lr 0.00049 | ms/batch 79.94 | loss  1.87 | ppl     6.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 79.51s | valid loss  7.81 | valid ppl  2466.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/  938 batches | lr 0.00046 | ms/batch 80.25 | loss  1.84 | ppl     6.27\n",
      "| epoch  16 |   400/  938 batches | lr 0.00046 | ms/batch 79.76 | loss  1.51 | ppl     4.53\n",
      "| epoch  16 |   600/  938 batches | lr 0.00046 | ms/batch 79.84 | loss  1.69 | ppl     5.40\n",
      "| epoch  16 |   800/  938 batches | lr 0.00046 | ms/batch 79.84 | loss  2.40 | ppl    10.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 79.46s | valid loss  5.68 | valid ppl   292.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/  938 batches | lr 0.00044 | ms/batch 80.20 | loss  1.75 | ppl     5.77\n",
      "| epoch  17 |   400/  938 batches | lr 0.00044 | ms/batch 79.74 | loss  1.92 | ppl     6.82\n",
      "| epoch  17 |   600/  938 batches | lr 0.00044 | ms/batch 79.62 | loss  1.36 | ppl     3.91\n",
      "| epoch  17 |   800/  938 batches | lr 0.00044 | ms/batch 79.87 | loss  2.29 | ppl     9.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 79.42s | valid loss  7.87 | valid ppl  2619.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/  938 batches | lr 0.00042 | ms/batch 80.19 | loss  1.52 | ppl     4.56\n",
      "| epoch  18 |   400/  938 batches | lr 0.00042 | ms/batch 79.53 | loss  1.43 | ppl     4.18\n",
      "| epoch  18 |   600/  938 batches | lr 0.00042 | ms/batch 79.57 | loss  1.58 | ppl     4.84\n",
      "| epoch  18 |   800/  938 batches | lr 0.00042 | ms/batch 79.51 | loss  1.76 | ppl     5.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 79.25s | valid loss  7.80 | valid ppl  2452.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/  938 batches | lr 0.00040 | ms/batch 79.98 | loss  1.12 | ppl     3.06\n",
      "| epoch  19 |   400/  938 batches | lr 0.00040 | ms/batch 79.44 | loss  1.08 | ppl     2.93\n",
      "| epoch  19 |   600/  938 batches | lr 0.00040 | ms/batch 79.40 | loss  1.02 | ppl     2.77\n",
      "| epoch  19 |   800/  938 batches | lr 0.00040 | ms/batch 79.72 | loss  1.61 | ppl     5.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 79.20s | valid loss  6.04 | valid ppl   419.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/  938 batches | lr 0.00038 | ms/batch 79.78 | loss  1.01 | ppl     2.73\n",
      "| epoch  20 |   400/  938 batches | lr 0.00038 | ms/batch 79.40 | loss  1.11 | ppl     3.04\n",
      "| epoch  20 |   600/  938 batches | lr 0.00038 | ms/batch 79.61 | loss  1.47 | ppl     4.36\n",
      "| epoch  20 |   800/  938 batches | lr 0.00038 | ms/batch 79.51 | loss  1.36 | ppl     3.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 79.15s | valid loss  6.09 | valid ppl   439.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/  938 batches | lr 0.00036 | ms/batch 79.82 | loss  0.93 | ppl     2.55\n",
      "| epoch  21 |   400/  938 batches | lr 0.00036 | ms/batch 79.31 | loss  1.08 | ppl     2.95\n",
      "| epoch  21 |   600/  938 batches | lr 0.00036 | ms/batch 79.23 | loss  0.90 | ppl     2.45\n",
      "| epoch  21 |   800/  938 batches | lr 0.00036 | ms/batch 79.54 | loss  1.30 | ppl     3.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 79.05s | valid loss  7.32 | valid ppl  1510.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/  938 batches | lr 0.00034 | ms/batch 79.62 | loss  0.94 | ppl     2.57\n",
      "| epoch  22 |   400/  938 batches | lr 0.00034 | ms/batch 79.23 | loss  0.81 | ppl     2.25\n",
      "| epoch  22 |   600/  938 batches | lr 0.00034 | ms/batch 79.30 | loss  1.02 | ppl     2.78\n",
      "| epoch  22 |   800/  938 batches | lr 0.00034 | ms/batch 79.42 | loss  1.30 | ppl     3.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 78.99s | valid loss  6.98 | valid ppl  1080.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   200/  938 batches | lr 0.00032 | ms/batch 79.86 | loss  1.22 | ppl     3.39\n",
      "| epoch  23 |   400/  938 batches | lr 0.00032 | ms/batch 79.38 | loss  1.17 | ppl     3.22\n",
      "| epoch  23 |   600/  938 batches | lr 0.00032 | ms/batch 79.33 | loss  0.95 | ppl     2.60\n",
      "| epoch  23 |   800/  938 batches | lr 0.00032 | ms/batch 79.15 | loss  0.44 | ppl     1.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 79.00s | valid loss  7.28 | valid ppl  1458.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/  938 batches | lr 0.00031 | ms/batch 79.81 | loss  0.85 | ppl     2.33\n",
      "| epoch  24 |   400/  938 batches | lr 0.00031 | ms/batch 79.37 | loss  1.25 | ppl     3.48\n",
      "| epoch  24 |   600/  938 batches | lr 0.00031 | ms/batch 79.25 | loss  0.64 | ppl     1.89\n",
      "| epoch  24 |   800/  938 batches | lr 0.00031 | ms/batch 79.38 | loss  1.29 | ppl     3.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 78.98s | valid loss  7.37 | valid ppl  1579.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   200/  938 batches | lr 0.00029 | ms/batch 79.60 | loss  0.65 | ppl     1.92\n",
      "| epoch  25 |   400/  938 batches | lr 0.00029 | ms/batch 79.26 | loss  0.72 | ppl     2.05\n",
      "| epoch  25 |   600/  938 batches | lr 0.00029 | ms/batch 79.26 | loss  0.58 | ppl     1.79\n",
      "| epoch  25 |   800/  938 batches | lr 0.00029 | ms/batch 79.12 | loss  0.46 | ppl     1.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 78.91s | valid loss  7.81 | valid ppl  2458.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   200/  938 batches | lr 0.00028 | ms/batch 79.65 | loss  0.79 | ppl     2.20\n",
      "| epoch  26 |   400/  938 batches | lr 0.00028 | ms/batch 79.13 | loss  0.50 | ppl     1.65\n",
      "| epoch  26 |   600/  938 batches | lr 0.00028 | ms/batch 79.22 | loss  0.85 | ppl     2.34\n",
      "| epoch  26 |   800/  938 batches | lr 0.00028 | ms/batch 79.22 | loss  0.81 | ppl     2.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 78.89s | valid loss  8.68 | valid ppl  5910.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   200/  938 batches | lr 0.00026 | ms/batch 79.59 | loss  0.60 | ppl     1.82\n",
      "| epoch  27 |   400/  938 batches | lr 0.00026 | ms/batch 79.12 | loss  0.40 | ppl     1.49\n",
      "| epoch  27 |   600/  938 batches | lr 0.00026 | ms/batch 79.08 | loss  0.54 | ppl     1.72\n",
      "| epoch  27 |   800/  938 batches | lr 0.00026 | ms/batch 80.43 | loss  1.06 | ppl     2.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 79.25s | valid loss  8.16 | valid ppl  3513.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   200/  938 batches | lr 0.00025 | ms/batch 80.09 | loss  0.67 | ppl     1.95\n",
      "| epoch  28 |   400/  938 batches | lr 0.00025 | ms/batch 78.97 | loss  0.25 | ppl     1.29\n",
      "| epoch  28 |   600/  938 batches | lr 0.00025 | ms/batch 79.10 | loss  0.45 | ppl     1.57\n",
      "| epoch  28 |   800/  938 batches | lr 0.00025 | ms/batch 79.24 | loss  0.96 | ppl     2.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 78.88s | valid loss  7.34 | valid ppl  1537.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   200/  938 batches | lr 0.00024 | ms/batch 79.54 | loss  0.47 | ppl     1.61\n",
      "| epoch  29 |   400/  938 batches | lr 0.00024 | ms/batch 79.11 | loss  0.54 | ppl     1.71\n",
      "| epoch  29 |   600/  938 batches | lr 0.00024 | ms/batch 79.07 | loss  0.37 | ppl     1.45\n",
      "| epoch  29 |   800/  938 batches | lr 0.00024 | ms/batch 79.05 | loss  0.64 | ppl     1.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 78.76s | valid loss  8.80 | valid ppl  6638.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   200/  938 batches | lr 0.00023 | ms/batch 79.43 | loss  0.51 | ppl     1.67\n",
      "| epoch  30 |   400/  938 batches | lr 0.00023 | ms/batch 79.13 | loss  0.34 | ppl     1.41\n",
      "| epoch  30 |   600/  938 batches | lr 0.00023 | ms/batch 79.18 | loss  0.89 | ppl     2.44\n",
      "| epoch  30 |   800/  938 batches | lr 0.00023 | ms/batch 79.12 | loss  0.62 | ppl     1.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 78.78s | valid loss  8.54 | valid ppl  5103.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  6.56 | test ppl   706.24\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "model = train_model(model, epochs, train_loader, test_loader, seq_len, digits_per_batch, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    data, targets = get_batch(batch, seq_len, digits_per_batch)\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(output, targets)\n",
    "    print(\"loss: \", loss.item())\n",
    "    #plt.matshow((data.view(-1, data.shape[-1]) * mnist_std + mnist_mean).numpy())\n",
    "    #plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.309042453765869\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    data, targets = get_batch(batch, seq_len, digits_per_batch)\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(output, targets)\n",
    "    print(\"loss: \", loss.item())\n",
    "    #plt.matshow((data.view(-1, data.shape[-1]) * mnist_std + mnist_mean).numpy())\n",
    "    #plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /tmp/i273233/i273233/cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvMNIST(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): MaxPool2d(kernel_size=2, stride=2, padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (45): ReLU(inplace=True)\n",
      "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (48): ReLU(inplace=True)\n",
      "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (51): ReLU(inplace=True)\n",
      "    (52): MaxPool2d(kernel_size=(2, 4), stride=(2, 4), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1028, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=1028, out_features=1028, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=1028, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19_bn', pretrained=False)\n",
    "#print(model)\n",
    "features = model.features\n",
    "features[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "features[6] = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "features[13] = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "features[26] = nn.MaxPool2d(kernel_size=2, stride=2, padding=(1,0))\n",
    "features[39] = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "features[52] = nn.MaxPool2d(kernel_size=(2,4), stride=(2,4), padding=0)\n",
    "\n",
    "classifier = nn.Sequential(nn.Linear(512, 1028),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(p=0.5, inplace=False),\n",
    "                            nn.Linear(in_features=1028, out_features=1028, bias=True),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(p=0.5, inplace=False),\n",
    "                            nn.Linear(in_features=1028, out_features=10, bias=True))\n",
    "model = ConvMNIST(features, classifier)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/  938 batches | lr 0.00100 | ms/batch 82.24 | loss 105.30 | ppl 5395211431270114825879216080281024623129133056.00\n",
      "| epoch   1 |   400/  938 batches | lr 0.00100 | ms/batch 81.72 | loss 61.67 | ppl 607008977964079963632041984.00\n",
      "| epoch   1 |   600/  938 batches | lr 0.00100 | ms/batch 81.73 | loss 35.52 | ppl 2660916238478914.50\n",
      "| epoch   1 |   800/  938 batches | lr 0.00100 | ms/batch 81.86 | loss 24.55 | ppl 45804174267.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 81.27s | valid loss 27.69 | valid ppl 1063420044109.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/  938 batches | lr 0.00095 | ms/batch 82.74 | loss 14.98 | ppl 3210910.75\n",
      "| epoch   2 |   400/  938 batches | lr 0.00095 | ms/batch 81.97 | loss 13.48 | ppl 715301.58\n",
      "| epoch   2 |   600/  938 batches | lr 0.00095 | ms/batch 81.84 | loss 11.68 | ppl 118534.76\n",
      "| epoch   2 |   800/  938 batches | lr 0.00095 | ms/batch 81.98 | loss 10.40 | ppl 32869.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 81.44s | valid loss 21.19 | valid ppl 1598308799.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/  938 batches | lr 0.00090 | ms/batch 82.32 | loss  9.17 | ppl  9626.13\n",
      "| epoch   3 |   400/  938 batches | lr 0.00090 | ms/batch 82.00 | loss  9.25 | ppl 10411.71\n",
      "| epoch   3 |   600/  938 batches | lr 0.00090 | ms/batch 81.89 | loss  9.10 | ppl  8971.56\n",
      "| epoch   3 |   800/  938 batches | lr 0.00090 | ms/batch 82.00 | loss  8.48 | ppl  4816.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 81.38s | valid loss 12.83 | valid ppl 374702.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/  938 batches | lr 0.00086 | ms/batch 82.16 | loss  7.37 | ppl  1591.22\n",
      "| epoch   4 |   400/  938 batches | lr 0.00086 | ms/batch 81.89 | loss  7.59 | ppl  1983.55\n",
      "| epoch   4 |   600/  938 batches | lr 0.00086 | ms/batch 81.63 | loss  7.08 | ppl  1183.07\n",
      "| epoch   4 |   800/  938 batches | lr 0.00086 | ms/batch 81.56 | loss  7.24 | ppl  1389.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 81.15s | valid loss 14.89 | valid ppl 2938958.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/  938 batches | lr 0.00081 | ms/batch 81.74 | loss  6.87 | ppl   958.34\n",
      "| epoch   5 |   400/  938 batches | lr 0.00081 | ms/batch 81.63 | loss  6.72 | ppl   832.08\n",
      "| epoch   5 |   600/  938 batches | lr 0.00081 | ms/batch 81.72 | loss  7.30 | ppl  1481.93\n",
      "| epoch   5 |   800/  938 batches | lr 0.00081 | ms/batch 81.70 | loss  6.83 | ppl   922.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 81.03s | valid loss 13.56 | valid ppl 771233.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/  938 batches | lr 0.00077 | ms/batch 81.42 | loss  5.60 | ppl   270.51\n",
      "| epoch   6 |   400/  938 batches | lr 0.00077 | ms/batch 81.41 | loss  6.47 | ppl   643.63\n",
      "| epoch   6 |   600/  938 batches | lr 0.00077 | ms/batch 81.36 | loss  5.68 | ppl   293.62\n",
      "| epoch   6 |   800/  938 batches | lr 0.00077 | ms/batch 80.29 | loss  5.07 | ppl   159.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 80.39s | valid loss 15.61 | valid ppl 6042146.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/  938 batches | lr 0.00074 | ms/batch 80.39 | loss  6.49 | ppl   656.84\n",
      "| epoch   7 |   400/  938 batches | lr 0.00074 | ms/batch 79.93 | loss  5.20 | ppl   181.97\n",
      "| epoch   7 |   600/  938 batches | lr 0.00074 | ms/batch 79.69 | loss  4.84 | ppl   126.97\n",
      "| epoch   7 |   800/  938 batches | lr 0.00074 | ms/batch 79.97 | loss  6.01 | ppl   405.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 79.43s | valid loss  7.91 | valid ppl  2729.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/  938 batches | lr 0.00070 | ms/batch 80.12 | loss  4.83 | ppl   124.69\n",
      "| epoch   8 |   400/  938 batches | lr 0.00070 | ms/batch 79.77 | loss  5.48 | ppl   239.98\n",
      "| epoch   8 |   600/  938 batches | lr 0.00070 | ms/batch 79.77 | loss  4.17 | ppl    64.53\n",
      "| epoch   8 |   800/  938 batches | lr 0.00070 | ms/batch 79.79 | loss  4.82 | ppl   123.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 79.30s | valid loss  9.55 | valid ppl 14010.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/  938 batches | lr 0.00066 | ms/batch 80.10 | loss  4.75 | ppl   115.21\n",
      "| epoch   9 |   400/  938 batches | lr 0.00066 | ms/batch 79.36 | loss  4.70 | ppl   110.08\n",
      "| epoch   9 |   600/  938 batches | lr 0.00066 | ms/batch 79.65 | loss  4.75 | ppl   116.06\n",
      "| epoch   9 |   800/  938 batches | lr 0.00066 | ms/batch 79.75 | loss  4.66 | ppl   106.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 79.19s | valid loss  9.76 | valid ppl 17263.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/  938 batches | lr 0.00063 | ms/batch 79.84 | loss  3.70 | ppl    40.59\n",
      "| epoch  10 |   400/  938 batches | lr 0.00063 | ms/batch 79.48 | loss  3.66 | ppl    38.69\n",
      "| epoch  10 |   600/  938 batches | lr 0.00063 | ms/batch 79.45 | loss  3.44 | ppl    31.15\n",
      "| epoch  10 |   800/  938 batches | lr 0.00063 | ms/batch 79.40 | loss  4.65 | ppl   105.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 79.05s | valid loss  8.47 | valid ppl  4752.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/  938 batches | lr 0.00060 | ms/batch 79.60 | loss  2.90 | ppl    18.26\n",
      "| epoch  11 |   400/  938 batches | lr 0.00060 | ms/batch 78.89 | loss  2.76 | ppl    15.84\n",
      "| epoch  11 |   600/  938 batches | lr 0.00060 | ms/batch 78.87 | loss  3.67 | ppl    39.45\n",
      "| epoch  11 |   800/  938 batches | lr 0.00060 | ms/batch 79.07 | loss  3.58 | ppl    35.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 78.66s | valid loss  9.14 | valid ppl  9322.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/  938 batches | lr 0.00057 | ms/batch 79.47 | loss  3.24 | ppl    25.48\n",
      "| epoch  12 |   400/  938 batches | lr 0.00057 | ms/batch 79.62 | loss  3.08 | ppl    21.78\n",
      "| epoch  12 |   600/  938 batches | lr 0.00057 | ms/batch 80.14 | loss  3.37 | ppl    29.18\n",
      "| epoch  12 |   800/  938 batches | lr 0.00057 | ms/batch 80.27 | loss  3.51 | ppl    33.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 79.46s | valid loss 10.56 | valid ppl 38482.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/  938 batches | lr 0.00054 | ms/batch 80.66 | loss  3.43 | ppl    30.87\n",
      "| epoch  13 |   400/  938 batches | lr 0.00054 | ms/batch 80.16 | loss  2.71 | ppl    15.00\n",
      "| epoch  13 |   600/  938 batches | lr 0.00054 | ms/batch 80.24 | loss  2.75 | ppl    15.68\n",
      "| epoch  13 |   800/  938 batches | lr 0.00054 | ms/batch 79.98 | loss  2.54 | ppl    12.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 79.75s | valid loss  6.76 | valid ppl   864.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/  938 batches | lr 0.00051 | ms/batch 80.53 | loss  2.32 | ppl    10.16\n",
      "| epoch  14 |   400/  938 batches | lr 0.00051 | ms/batch 80.23 | loss  1.93 | ppl     6.91\n",
      "| epoch  14 |   600/  938 batches | lr 0.00051 | ms/batch 80.23 | loss  3.03 | ppl    20.77\n",
      "| epoch  14 |   800/  938 batches | lr 0.00051 | ms/batch 79.95 | loss  2.36 | ppl    10.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 79.78s | valid loss  5.33 | valid ppl   206.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/  938 batches | lr 0.00049 | ms/batch 80.32 | loss  1.94 | ppl     6.98\n",
      "| epoch  15 |   400/  938 batches | lr 0.00049 | ms/batch 79.92 | loss  1.79 | ppl     5.99\n",
      "| epoch  15 |   600/  938 batches | lr 0.00049 | ms/batch 79.86 | loss  2.80 | ppl    16.47\n",
      "| epoch  15 |   800/  938 batches | lr 0.00049 | ms/batch 80.17 | loss  2.34 | ppl    10.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 79.48s | valid loss  6.36 | valid ppl   576.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/  938 batches | lr 0.00046 | ms/batch 79.47 | loss  2.13 | ppl     8.44\n",
      "| epoch  16 |   400/  938 batches | lr 0.00046 | ms/batch 79.19 | loss  2.68 | ppl    14.54\n",
      "| epoch  16 |   600/  938 batches | lr 0.00046 | ms/batch 79.36 | loss  2.36 | ppl    10.61\n",
      "| epoch  16 |   800/  938 batches | lr 0.00046 | ms/batch 79.09 | loss  1.57 | ppl     4.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 78.81s | valid loss  7.01 | valid ppl  1110.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/  938 batches | lr 0.00044 | ms/batch 79.46 | loss  2.41 | ppl    11.11\n",
      "| epoch  17 |   400/  938 batches | lr 0.00044 | ms/batch 78.84 | loss  1.43 | ppl     4.17\n",
      "| epoch  17 |   600/  938 batches | lr 0.00044 | ms/batch 79.01 | loss  1.60 | ppl     4.95\n",
      "| epoch  17 |   800/  938 batches | lr 0.00044 | ms/batch 79.04 | loss  1.72 | ppl     5.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 78.67s | valid loss  6.05 | valid ppl   424.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/  938 batches | lr 0.00042 | ms/batch 79.48 | loss  1.88 | ppl     6.55\n",
      "| epoch  18 |   400/  938 batches | lr 0.00042 | ms/batch 78.96 | loss  1.40 | ppl     4.06\n",
      "| epoch  18 |   600/  938 batches | lr 0.00042 | ms/batch 79.20 | loss  1.38 | ppl     3.99\n",
      "| epoch  18 |   800/  938 batches | lr 0.00042 | ms/batch 80.38 | loss  1.98 | ppl     7.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 79.18s | valid loss  8.21 | valid ppl  3659.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/  938 batches | lr 0.00040 | ms/batch 80.55 | loss  1.14 | ppl     3.14\n",
      "| epoch  19 |   400/  938 batches | lr 0.00040 | ms/batch 80.04 | loss  1.59 | ppl     4.89\n",
      "| epoch  19 |   600/  938 batches | lr 0.00040 | ms/batch 80.27 | loss  1.42 | ppl     4.12\n",
      "| epoch  19 |   800/  938 batches | lr 0.00040 | ms/batch 80.20 | loss  1.80 | ppl     6.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 79.74s | valid loss  8.53 | valid ppl  5089.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/  938 batches | lr 0.00038 | ms/batch 80.38 | loss  1.09 | ppl     2.99\n",
      "| epoch  20 |   400/  938 batches | lr 0.00038 | ms/batch 79.51 | loss  1.31 | ppl     3.71\n",
      "| epoch  20 |   600/  938 batches | lr 0.00038 | ms/batch 79.61 | loss  1.89 | ppl     6.63\n",
      "| epoch  20 |   800/  938 batches | lr 0.00038 | ms/batch 79.51 | loss  1.41 | ppl     4.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 79.23s | valid loss  8.07 | valid ppl  3187.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/  938 batches | lr 0.00036 | ms/batch 79.95 | loss  1.08 | ppl     2.94\n",
      "| epoch  21 |   400/  938 batches | lr 0.00036 | ms/batch 79.77 | loss  1.00 | ppl     2.71\n",
      "| epoch  21 |   600/  938 batches | lr 0.00036 | ms/batch 80.28 | loss  1.98 | ppl     7.28\n",
      "| epoch  21 |   800/  938 batches | lr 0.00036 | ms/batch 80.25 | loss  1.02 | ppl     2.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 79.61s | valid loss  5.97 | valid ppl   391.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/  938 batches | lr 0.00034 | ms/batch 80.77 | loss  0.99 | ppl     2.68\n",
      "| epoch  22 |   400/  938 batches | lr 0.00034 | ms/batch 79.08 | loss  1.54 | ppl     4.66\n",
      "| epoch  22 |   600/  938 batches | lr 0.00034 | ms/batch 78.65 | loss  0.93 | ppl     2.53\n",
      "| epoch  22 |   800/  938 batches | lr 0.00034 | ms/batch 78.59 | loss  0.86 | ppl     2.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 78.77s | valid loss  7.75 | valid ppl  2319.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   200/  938 batches | lr 0.00032 | ms/batch 79.31 | loss  1.51 | ppl     4.53\n",
      "| epoch  23 |   400/  938 batches | lr 0.00032 | ms/batch 78.60 | loss  1.04 | ppl     2.84\n",
      "| epoch  23 |   600/  938 batches | lr 0.00032 | ms/batch 78.89 | loss  1.26 | ppl     3.54\n",
      "| epoch  23 |   800/  938 batches | lr 0.00032 | ms/batch 78.66 | loss  1.26 | ppl     3.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 78.42s | valid loss  7.66 | valid ppl  2120.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/  938 batches | lr 0.00031 | ms/batch 78.99 | loss  0.54 | ppl     1.71\n",
      "| epoch  24 |   400/  938 batches | lr 0.00031 | ms/batch 78.51 | loss  0.74 | ppl     2.09\n",
      "| epoch  24 |   600/  938 batches | lr 0.00031 | ms/batch 78.50 | loss  0.74 | ppl     2.10\n",
      "| epoch  24 |   800/  938 batches | lr 0.00031 | ms/batch 78.48 | loss  0.73 | ppl     2.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 78.25s | valid loss 10.75 | valid ppl 46663.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   200/  938 batches | lr 0.00029 | ms/batch 78.97 | loss  0.76 | ppl     2.15\n",
      "| epoch  25 |   400/  938 batches | lr 0.00029 | ms/batch 78.44 | loss  0.67 | ppl     1.96\n",
      "| epoch  25 |   600/  938 batches | lr 0.00029 | ms/batch 78.61 | loss  1.12 | ppl     3.08\n",
      "| epoch  25 |   800/  938 batches | lr 0.00029 | ms/batch 78.83 | loss  1.29 | ppl     3.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 78.30s | valid loss  7.69 | valid ppl  2196.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   200/  938 batches | lr 0.00028 | ms/batch 78.88 | loss  0.82 | ppl     2.26\n",
      "| epoch  26 |   400/  938 batches | lr 0.00028 | ms/batch 78.53 | loss  0.62 | ppl     1.85\n",
      "| epoch  26 |   600/  938 batches | lr 0.00028 | ms/batch 78.46 | loss  0.36 | ppl     1.43\n",
      "| epoch  26 |   800/  938 batches | lr 0.00028 | ms/batch 78.47 | loss  0.68 | ppl     1.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 78.20s | valid loss  7.03 | valid ppl  1132.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   200/  938 batches | lr 0.00026 | ms/batch 79.04 | loss  0.89 | ppl     2.43\n",
      "| epoch  27 |   400/  938 batches | lr 0.00026 | ms/batch 78.61 | loss  0.80 | ppl     2.23\n",
      "| epoch  27 |   600/  938 batches | lr 0.00026 | ms/batch 78.50 | loss  0.37 | ppl     1.44\n",
      "| epoch  27 |   800/  938 batches | lr 0.00026 | ms/batch 78.64 | loss  0.94 | ppl     2.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 78.28s | valid loss  7.40 | valid ppl  1642.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   200/  938 batches | lr 0.00025 | ms/batch 79.05 | loss  0.73 | ppl     2.08\n",
      "| epoch  28 |   400/  938 batches | lr 0.00025 | ms/batch 78.41 | loss  0.34 | ppl     1.40\n",
      "| epoch  28 |   600/  938 batches | lr 0.00025 | ms/batch 78.45 | loss  0.66 | ppl     1.94\n",
      "| epoch  28 |   800/  938 batches | lr 0.00025 | ms/batch 78.58 | loss  0.62 | ppl     1.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 78.21s | valid loss  7.74 | valid ppl  2304.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   200/  938 batches | lr 0.00024 | ms/batch 78.99 | loss  0.85 | ppl     2.33\n",
      "| epoch  29 |   400/  938 batches | lr 0.00024 | ms/batch 78.49 | loss  0.57 | ppl     1.78\n",
      "| epoch  29 |   600/  938 batches | lr 0.00024 | ms/batch 78.43 | loss  0.39 | ppl     1.47\n",
      "| epoch  29 |   800/  938 batches | lr 0.00024 | ms/batch 78.48 | loss  0.45 | ppl     1.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 78.20s | valid loss  8.43 | valid ppl  4571.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   200/  938 batches | lr 0.00023 | ms/batch 78.92 | loss  0.86 | ppl     2.36\n",
      "| epoch  30 |   400/  938 batches | lr 0.00023 | ms/batch 78.49 | loss  0.22 | ppl     1.25\n",
      "| epoch  30 |   600/  938 batches | lr 0.00023 | ms/batch 78.31 | loss  0.24 | ppl     1.28\n",
      "| epoch  30 |   800/  938 batches | lr 0.00023 | ms/batch 78.38 | loss  0.79 | ppl     2.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 78.15s | valid loss  9.63 | valid ppl 15202.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.10 | test ppl   163.66\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "model = train_model(model, epochs, train_loader, test_loader, seq_len, digits_per_batch, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /tmp/i273233/i273233/cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoXTimes(\n",
      "  (model): Smartpool(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=28, out_features=2048, bias=True)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): GELU()\n",
      "      (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "      (4): Dropout(p=0.1, inplace=False)\n",
      "      (5): GELU()\n",
      "      (6): Linear(in_features=2048, out_features=1, bias=True)\n",
      "      (7): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=28, out_features=1028, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=1028, out_features=1028, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=1028, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19_bn', pretrained=False)\n",
    "#print(model)\n",
    "\"\"\"\n",
    "features = model.features\n",
    "features[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "features[6] = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "features[13] = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "features[26] = nn.MaxPool2d(kernel_size=2, stride=2, padding=(1,0))\n",
    "features[39] = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "features[52] = nn.MaxPool2d(kernel_size=(2,4), stride=(2,4), padding=0)\n",
    "\"\"\"\n",
    "classifier = nn.Sequential(nn.Linear(28, 1028),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(p=0.5, inplace=False),\n",
    "                            nn.Linear(in_features=1028, out_features=1028, bias=True),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(p=0.5, inplace=False),\n",
    "                            nn.Linear(in_features=1028, out_features=10, bias=True))\n",
    "\n",
    "model = DoXTimes(Smartpool(divider, 0.3), classifier)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/  938 batches | lr 0.00100 | ms/batch 113.60 | loss 74.83 | ppl 314418569566806511125523442171904.00\n",
      "| epoch   1 |   400/  938 batches | lr 0.00100 | ms/batch 112.95 | loss 51.22 | ppl 17542916054070148988928.00\n",
      "| epoch   1 |   600/  938 batches | lr 0.00100 | ms/batch 112.95 | loss 47.82 | ppl 587121920694627336192.00\n",
      "| epoch   1 |   800/  938 batches | lr 0.00100 | ms/batch 112.97 | loss 43.84 | ppl 10947473565823875072.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 114.49s | valid loss 75.67 | valid ppl 732282433965533799782835688570880.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/  938 batches | lr 0.00095 | ms/batch 113.37 | loss 40.80 | ppl 521566607571205824.00\n",
      "| epoch   2 |   400/  938 batches | lr 0.00095 | ms/batch 112.78 | loss 40.20 | ppl 286509426412913856.00\n",
      "| epoch   2 |   600/  938 batches | lr 0.00095 | ms/batch 112.81 | loss 39.76 | ppl 184238610063415200.00\n",
      "| epoch   2 |   800/  938 batches | lr 0.00095 | ms/batch 112.83 | loss 39.41 | ppl 130909507842458880.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 114.35s | valid loss 67.71 | valid ppl 255053244813505842418078973952.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/  938 batches | lr 0.00090 | ms/batch 113.41 | loss 38.07 | ppl 34071986138439120.00\n",
      "| epoch   3 |   400/  938 batches | lr 0.00090 | ms/batch 112.83 | loss 37.33 | ppl 16269339740978960.00\n",
      "| epoch   3 |   600/  938 batches | lr 0.00090 | ms/batch 112.84 | loss 37.43 | ppl 18076471757699312.00\n",
      "| epoch   3 |   800/  938 batches | lr 0.00090 | ms/batch 112.86 | loss 36.65 | ppl 8274581104414468.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 114.37s | valid loss 63.00 | valid ppl 2290566934529377082613432320.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/  938 batches | lr 0.00086 | ms/batch 113.40 | loss 35.27 | ppl 2086551369922178.75\n",
      "| epoch   4 |   400/  938 batches | lr 0.00086 | ms/batch 112.82 | loss 35.73 | ppl 3304702072803038.50\n",
      "| epoch   4 |   600/  938 batches | lr 0.00086 | ms/batch 112.82 | loss 35.77 | ppl 3420561663172913.50\n",
      "| epoch   4 |   800/  938 batches | lr 0.00086 | ms/batch 112.83 | loss 35.45 | ppl 2495645189669637.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 114.36s | valid loss 62.36 | valid ppl 1205415523227815297272512512.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/  938 batches | lr 0.00081 | ms/batch 113.42 | loss 34.34 | ppl 820292481550673.50\n",
      "| epoch   5 |   400/  938 batches | lr 0.00081 | ms/batch 112.85 | loss 34.23 | ppl 732273020785848.75\n",
      "| epoch   5 |   600/  938 batches | lr 0.00081 | ms/batch 112.86 | loss 34.54 | ppl 1002863467801456.75\n",
      "| epoch   5 |   800/  938 batches | lr 0.00081 | ms/batch 112.85 | loss 35.14 | ppl 1817680726234739.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 114.38s | valid loss 61.63 | valid ppl 583600985485159941732827136.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/  938 batches | lr 0.00077 | ms/batch 113.40 | loss 33.29 | ppl 286192660602086.00\n",
      "| epoch   6 |   400/  938 batches | lr 0.00077 | ms/batch 112.83 | loss 33.66 | ppl 415726721985207.88\n",
      "| epoch   6 |   600/  938 batches | lr 0.00077 | ms/batch 112.82 | loss 33.67 | ppl 418649384929086.38\n",
      "| epoch   6 |   800/  938 batches | lr 0.00077 | ms/batch 112.83 | loss 32.51 | ppl 131158650593127.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 114.36s | valid loss 56.77 | valid ppl 4525937871303877014323200.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/  938 batches | lr 0.00074 | ms/batch 113.43 | loss 32.94 | ppl 201508516174388.91\n",
      "| epoch   7 |   400/  938 batches | lr 0.00074 | ms/batch 112.87 | loss 31.49 | ppl 47545383477373.16\n",
      "| epoch   7 |   600/  938 batches | lr 0.00074 | ms/batch 112.84 | loss 32.69 | ppl 157413590945287.50\n",
      "| epoch   7 |   800/  938 batches | lr 0.00074 | ms/batch 112.84 | loss 32.51 | ppl 131861561189905.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 114.38s | valid loss 56.87 | valid ppl 4982727177218629807636480.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/  938 batches | lr 0.00070 | ms/batch 113.46 | loss 31.73 | ppl 60332594458553.28\n",
      "| epoch   8 |   400/  938 batches | lr 0.00070 | ms/batch 112.86 | loss 32.49 | ppl 128472309234013.52\n",
      "| epoch   8 |   600/  938 batches | lr 0.00070 | ms/batch 112.88 | loss 31.01 | ppl 29409265382884.34\n",
      "| epoch   8 |   800/  938 batches | lr 0.00070 | ms/batch 112.85 | loss 31.83 | ppl 66585594277782.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 114.40s | valid loss 56.65 | valid ppl 4009619388784906023731200.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/  938 batches | lr 0.00066 | ms/batch 113.51 | loss 31.68 | ppl 57409916182911.80\n",
      "| epoch   9 |   400/  938 batches | lr 0.00066 | ms/batch 113.09 | loss 31.18 | ppl 34753523790106.08\n",
      "| epoch   9 |   600/  938 batches | lr 0.00066 | ms/batch 112.93 | loss 31.73 | ppl 60016463363667.70\n",
      "| epoch   9 |   800/  938 batches | lr 0.00066 | ms/batch 112.94 | loss 30.73 | ppl 22235714870164.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 114.50s | valid loss 57.88 | valid ppl 13770445438528419780362240.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/  938 batches | lr 0.00063 | ms/batch 113.49 | loss 30.68 | ppl 21152131126596.74\n",
      "| epoch  10 |   400/  938 batches | lr 0.00063 | ms/batch 112.89 | loss 32.10 | ppl 87601954871974.98\n",
      "| epoch  10 |   600/  938 batches | lr 0.00063 | ms/batch 112.88 | loss 30.03 | ppl 10965460340190.12\n",
      "| epoch  10 |   800/  938 batches | lr 0.00063 | ms/batch 112.95 | loss 31.30 | ppl 39096537542586.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 114.44s | valid loss 54.74 | valid ppl 595722699651901680517120.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/  938 batches | lr 0.00060 | ms/batch 113.41 | loss 30.46 | ppl 16914950362819.04\n",
      "| epoch  11 |   400/  938 batches | lr 0.00060 | ms/batch 112.83 | loss 30.02 | ppl 10928255345074.82\n",
      "| epoch  11 |   600/  938 batches | lr 0.00060 | ms/batch 112.83 | loss 30.16 | ppl 12521059511778.81\n",
      "| epoch  11 |   800/  938 batches | lr 0.00060 | ms/batch 112.82 | loss 31.34 | ppl 40889533116373.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 114.35s | valid loss 55.66 | valid ppl 1493703322367475381698560.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/  938 batches | lr 0.00057 | ms/batch 113.40 | loss 30.15 | ppl 12450055964131.49\n",
      "| epoch  12 |   400/  938 batches | lr 0.00057 | ms/batch 112.82 | loss 30.88 | ppl 25696265132826.79\n",
      "| epoch  12 |   600/  938 batches | lr 0.00057 | ms/batch 112.82 | loss 30.92 | ppl 26749864038688.54\n",
      "| epoch  12 |   800/  938 batches | lr 0.00057 | ms/batch 112.81 | loss 29.12 | ppl 4415103779846.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 114.34s | valid loss 56.72 | valid ppl 4315504760172124353593344.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/  938 batches | lr 0.00054 | ms/batch 113.41 | loss 29.55 | ppl 6804121336999.07\n",
      "| epoch  13 |   400/  938 batches | lr 0.00054 | ms/batch 112.83 | loss 29.71 | ppl 8035719408456.79\n",
      "| epoch  13 |   600/  938 batches | lr 0.00054 | ms/batch 112.80 | loss 29.64 | ppl 7469176857027.27\n",
      "| epoch  13 |   800/  938 batches | lr 0.00054 | ms/batch 112.81 | loss 30.47 | ppl 17018323984050.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 114.36s | valid loss 56.55 | valid ppl 3625753142215184658464768.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/  938 batches | lr 0.00051 | ms/batch 113.32 | loss 29.84 | ppl 9088748304155.28\n",
      "| epoch  14 |   400/  938 batches | lr 0.00051 | ms/batch 112.77 | loss 28.43 | ppl 2233517830409.62\n",
      "| epoch  14 |   600/  938 batches | lr 0.00051 | ms/batch 112.74 | loss 30.63 | ppl 20163984159490.62\n",
      "| epoch  14 |   800/  938 batches | lr 0.00051 | ms/batch 112.75 | loss 29.13 | ppl 4492588544412.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 114.28s | valid loss 53.89 | valid ppl 254836615455871751684096.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/  938 batches | lr 0.00049 | ms/batch 113.43 | loss 28.35 | ppl 2046757357080.65\n",
      "| epoch  15 |   400/  938 batches | lr 0.00049 | ms/batch 112.85 | loss 28.95 | ppl 3744126081653.75\n",
      "| epoch  15 |   600/  938 batches | lr 0.00049 | ms/batch 112.83 | loss 29.69 | ppl 7811785439671.34\n",
      "| epoch  15 |   800/  938 batches | lr 0.00049 | ms/batch 112.83 | loss 29.12 | ppl 4420329051294.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 114.37s | valid loss 53.31 | valid ppl 142469629705516566446080.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/  938 batches | lr 0.00046 | ms/batch 113.41 | loss 28.30 | ppl 1948101658453.65\n",
      "| epoch  16 |   400/  938 batches | lr 0.00046 | ms/batch 112.84 | loss 28.90 | ppl 3569812087676.81\n",
      "| epoch  16 |   600/  938 batches | lr 0.00046 | ms/batch 112.83 | loss 28.77 | ppl 3114052975769.12\n",
      "| epoch  16 |   800/  938 batches | lr 0.00046 | ms/batch 112.82 | loss 29.26 | ppl 5104309921457.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 114.37s | valid loss 52.94 | valid ppl 98285705841032942321664.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/  938 batches | lr 0.00044 | ms/batch 113.48 | loss 28.15 | ppl 1674753980640.44\n",
      "| epoch  17 |   400/  938 batches | lr 0.00044 | ms/batch 112.87 | loss 29.03 | ppl 4065275174028.31\n",
      "| epoch  17 |   600/  938 batches | lr 0.00044 | ms/batch 112.86 | loss 28.17 | ppl 1713931797476.44\n",
      "| epoch  17 |   800/  938 batches | lr 0.00044 | ms/batch 112.85 | loss 28.17 | ppl 1706379357623.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 114.40s | valid loss 53.33 | valid ppl 145551920512642108620800.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/  938 batches | lr 0.00042 | ms/batch 113.43 | loss 28.44 | ppl 2249571884955.65\n",
      "| epoch  18 |   400/  938 batches | lr 0.00042 | ms/batch 112.84 | loss 28.39 | ppl 2130969450292.76\n",
      "| epoch  18 |   600/  938 batches | lr 0.00042 | ms/batch 112.86 | loss 28.09 | ppl 1580992371011.18\n",
      "| epoch  18 |   800/  938 batches | lr 0.00042 | ms/batch 112.85 | loss 28.68 | ppl 2864158628725.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 114.38s | valid loss 53.63 | valid ppl 194719510455033891127296.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/  938 batches | lr 0.00040 | ms/batch 113.36 | loss 27.81 | ppl 1190070828785.93\n",
      "| epoch  19 |   400/  938 batches | lr 0.00040 | ms/batch 112.86 | loss 27.93 | ppl 1350966120008.24\n",
      "| epoch  19 |   600/  938 batches | lr 0.00040 | ms/batch 112.84 | loss 27.71 | ppl 1087063653015.00\n",
      "| epoch  19 |   800/  938 batches | lr 0.00040 | ms/batch 112.79 | loss 28.05 | ppl 1525412847628.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 114.34s | valid loss 54.26 | valid ppl 365604088240333379338240.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/  938 batches | lr 0.00038 | ms/batch 113.42 | loss 27.48 | ppl 859911148277.06\n",
      "| epoch  20 |   400/  938 batches | lr 0.00038 | ms/batch 112.86 | loss 27.90 | ppl 1305889944712.59\n",
      "| epoch  20 |   600/  938 batches | lr 0.00038 | ms/batch 112.85 | loss 28.77 | ppl 3132494076294.32\n",
      "| epoch  20 |   800/  938 batches | lr 0.00038 | ms/batch 112.83 | loss 27.80 | ppl 1180985005914.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 114.37s | valid loss 52.07 | valid ppl 41013299937395125256192.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/  938 batches | lr 0.00036 | ms/batch 113.44 | loss 27.82 | ppl 1208530071958.82\n",
      "| epoch  21 |   400/  938 batches | lr 0.00036 | ms/batch 112.89 | loss 27.75 | ppl 1131077384925.44\n",
      "| epoch  21 |   600/  938 batches | lr 0.00036 | ms/batch 112.85 | loss 26.86 | ppl 461635181988.82\n",
      "| epoch  21 |   800/  938 batches | lr 0.00036 | ms/batch 112.87 | loss 27.30 | ppl 717701462782.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 114.39s | valid loss 51.51 | valid ppl 23527568812133660491776.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/  938 batches | lr 0.00034 | ms/batch 113.46 | loss 26.82 | ppl 445938966503.31\n",
      "| epoch  22 |   400/  938 batches | lr 0.00034 | ms/batch 112.83 | loss 26.96 | ppl 509978809434.42\n",
      "| epoch  22 |   600/  938 batches | lr 0.00034 | ms/batch 112.86 | loss 27.51 | ppl 887049801411.00\n",
      "| epoch  22 |   800/  938 batches | lr 0.00034 | ms/batch 112.83 | loss 26.66 | ppl 379654498289.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 114.38s | valid loss 51.16 | valid ppl 16571484966881178157056.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   200/  938 batches | lr 0.00032 | ms/batch 113.38 | loss 26.68 | ppl 388166664528.89\n",
      "| epoch  23 |   400/  938 batches | lr 0.00032 | ms/batch 112.81 | loss 27.27 | ppl 697012478654.47\n",
      "| epoch  23 |   600/  938 batches | lr 0.00032 | ms/batch 112.80 | loss 26.62 | ppl 364508050678.07\n",
      "| epoch  23 |   800/  938 batches | lr 0.00032 | ms/batch 112.80 | loss 27.39 | ppl 783764881346.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 114.33s | valid loss 51.84 | valid ppl 32738048919776905920512.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/  938 batches | lr 0.00031 | ms/batch 113.41 | loss 26.16 | ppl 228718102167.69\n",
      "| epoch  24 |   400/  938 batches | lr 0.00031 | ms/batch 112.84 | loss 25.82 | ppl 163166146994.09\n",
      "| epoch  24 |   600/  938 batches | lr 0.00031 | ms/batch 112.82 | loss 26.90 | ppl 481906484142.08\n",
      "| epoch  24 |   800/  938 batches | lr 0.00031 | ms/batch 112.86 | loss 28.10 | ppl 1598516384691.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 114.36s | valid loss 51.21 | valid ppl 17310643678827639734272.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   200/  938 batches | lr 0.00029 | ms/batch 113.39 | loss 26.84 | ppl 454953147701.63\n",
      "| epoch  25 |   400/  938 batches | lr 0.00029 | ms/batch 112.82 | loss 27.33 | ppl 742730771357.58\n",
      "| epoch  25 |   600/  938 batches | lr 0.00029 | ms/batch 112.81 | loss 26.76 | ppl 419626808604.72\n",
      "| epoch  25 |   800/  938 batches | lr 0.00029 | ms/batch 112.87 | loss 26.26 | ppl 254851616132.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 114.35s | valid loss 52.52 | valid ppl 64218560722890267820032.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   200/  938 batches | lr 0.00028 | ms/batch 113.38 | loss 26.26 | ppl 253761328423.10\n",
      "| epoch  26 |   400/  938 batches | lr 0.00028 | ms/batch 112.79 | loss 27.44 | ppl 825403061890.00\n",
      "| epoch  26 |   600/  938 batches | lr 0.00028 | ms/batch 112.78 | loss 26.53 | ppl 331360454647.85\n",
      "| epoch  26 |   800/  938 batches | lr 0.00028 | ms/batch 112.81 | loss 26.82 | ppl 442361311865.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 114.32s | valid loss 51.30 | valid ppl 18990047699613251534848.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   200/  938 batches | lr 0.00026 | ms/batch 113.38 | loss 26.75 | ppl 414216950802.04\n",
      "| epoch  27 |   400/  938 batches | lr 0.00026 | ms/batch 112.83 | loss 26.30 | ppl 264199145846.30\n",
      "| epoch  27 |   600/  938 batches | lr 0.00026 | ms/batch 112.81 | loss 26.78 | ppl 428433301681.58\n",
      "| epoch  27 |   800/  938 batches | lr 0.00026 | ms/batch 112.83 | loss 26.35 | ppl 278752145806.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 114.34s | valid loss 50.17 | valid ppl 6161874054626217558016.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   200/  938 batches | lr 0.00025 | ms/batch 113.41 | loss 26.23 | ppl 245652356735.90\n",
      "| epoch  28 |   400/  938 batches | lr 0.00025 | ms/batch 112.81 | loss 26.29 | ppl 261908373989.06\n",
      "| epoch  28 |   600/  938 batches | lr 0.00025 | ms/batch 112.81 | loss 26.06 | ppl 208624681133.11\n",
      "| epoch  28 |   800/  938 batches | lr 0.00025 | ms/batch 112.82 | loss 26.09 | ppl 214530458451.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 114.34s | valid loss 49.85 | valid ppl 4476688445780811841536.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   200/  938 batches | lr 0.00024 | ms/batch 113.39 | loss 25.21 | ppl 88673165737.10\n",
      "| epoch  29 |   400/  938 batches | lr 0.00024 | ms/batch 112.82 | loss 26.29 | ppl 261755366393.80\n",
      "| epoch  29 |   600/  938 batches | lr 0.00024 | ms/batch 112.79 | loss 25.82 | ppl 163107031575.51\n",
      "| epoch  29 |   800/  938 batches | lr 0.00024 | ms/batch 112.81 | loss 27.43 | ppl 818801818827.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 114.34s | valid loss 49.83 | valid ppl 4368817811318009495552.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   200/  938 batches | lr 0.00023 | ms/batch 113.41 | loss 25.07 | ppl 76907149261.69\n",
      "| epoch  30 |   400/  938 batches | lr 0.00023 | ms/batch 112.79 | loss 26.88 | ppl 470048654716.75\n",
      "| epoch  30 |   600/  938 batches | lr 0.00023 | ms/batch 112.82 | loss 25.95 | ppl 185670677587.52\n",
      "| epoch  30 |   800/  938 batches | lr 0.00023 | ms/batch 112.82 | loss 25.95 | ppl 186316744054.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 114.34s | valid loss 50.91 | valid ppl 12817150279368461778944.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss 50.85 | test ppl 12073127675578481115136.00\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "model = train_model(model, epochs, train_loader, test_loader, seq_len, digits_per_batch, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvMNIST(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): AvgPool2d(kernel_size=2, stride=2, padding=(1, 0))\n",
      "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (45): ReLU(inplace=True)\n",
      "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (48): ReLU(inplace=True)\n",
      "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (51): ReLU(inplace=True)\n",
      "    (52): AvgPool2d(kernel_size=(2, 4), stride=(2, 4), padding=0)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1028, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=1028, out_features=1028, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=1028, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Smartpooling toy task.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
